[
["index.html", "Python for Data Science Welcome", " Python for Data Science J. Hathaway Welcome This is the port of the website for “R for Data Science” into Python. I am keeping Garrett Grolemund and Hadley Wickham’s writing and examples as much as possible while demonstrating Python instead of R. This book will teach you how to do data science with Python: You’ll learn how to get your data into Python, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data. This website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. The book is written in RMarkdown with bookdown. The orignial authors and contributors can be found here "],
["introduction.html", "1 Introduction 1.1 What you will learn 1.2 How this book is organised 1.3 What you won’t learn 1.4 Prerequisites 1.5 Running Python code 1.6 Getting help and learning more 1.7 Datasets access 1.8 Acknowledgements 1.9 Colophon", " 1 Introduction Data science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge. The goal of “Python for Data Science” is to help you learn some of the tools in Python that will allow you to begin your data science journey. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges, using the best parts of Python. 1.1 What you will learn Data science is a huge field, and there’s no way you can master it by reading a single book. The goal of this book is to give you a foundation in the essential tools. Our model of the tools needed in a typical data science project looks something like this: First you must import your data into Python. This typically means that you take data stored in a file, database, or web API, and load it into a data frame in Python. If you can’t get your data into Python, you can’t do data science on it! Once you’ve imported your data, it is a good idea to tidy it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions. Once you have tidy data, a common first step is to transform it. Transformation includes narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called wrangling, because getting your data in a form that’s natural to work with often feels like a fight! Once you have tidy data with the variables you need, there are two main engines of knowledge generation: visualisation and modelling. These have complementary strengths and weaknesses so any real analysis will iterate between them many times. Visualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don’t scale particularly well because they require a human to interpret them. Models are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. The last step of data science is communication, an absolutely critical part of any data analysis project. It doesn’t matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others. Surrounding all these tools is programming. Programming is a cross-cutting tool that you use in every part of the project. You don’t need to be an expert programmer to be a data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease. You’ll use these tools in every data science project, but for most projects they’re not enough. There’s a rough 80-20 rule at play; you can tackle about 80% of every project using the tools that you’ll learn in this book, but you’ll need other tools to tackle the remaining 20%. Throughout this book we’ll point you to resources where you can learn more. 1.2 How this book is organised The previous description of the tools of data science is organised roughly according to the order in which you use them in an analysis (although of course you’ll iterate through them multiple times). In our experience, however, this is not the best way to learn them: Starting with data ingest and tidying is sub-optimal because 80% of the time it’s routine and boring, and the other 20% of the time it’s weird and frustrating. That’s a bad place to start learning a new subject! Instead, we’ll start with visualisation and transformation of data that’s already been imported and tidied. That way, when you ingest and tidy your own data, your motivation will stay high because you know the pain is worth it. Some topics are best explained with other tools. For example, we believe that it’s easier to understand how models work if you already know about visualisation, tidy data, and programming. Programming tools are not necessarily interesting in their own right, but do allow you to tackle considerably more challenging problems. We’ll give you a selection of programming tools in the middle of the book, and then you’ll see how they can combine with the data science tools to tackle interesting modelling problems. Within each chapter, we try and stick to a similar pattern: start with some motivating examples so you can see the bigger picture, and then dive into the details. Each section of the book is paired with exercises to help you practice what you’ve learned. While it’s tempting to skip the exercises, there’s no better way to learn than practicing on real problems. 1.3 What you won’t learn There are some important topics that this book doesn’t cover. We believe it’s important to stay ruthlessly focused on the essentials so you can get up and running as quickly as possible. That means this book can’t cover every important topic. 1.3.1 Big data This book proudly focuses on small, in-memory datasets. This is the right place to start because you can’t tackle big data unless you have experience with small data. The tools you learn in this book will easily handle hundreds of megabytes of data, and with a little care you can typically use them to work with 1-2 Gb of data. If you’re routinely working with larger data (10-100 Gb, say), you should learn more about data.table. This book doesn’t teach data.table because it has a very concise interface which makes it harder to learn since it offers fewer linguistic cues. But if you’re working with large data, the performance payoff is worth the extra effort required to learn it. If your data is bigger than this, carefully consider if your big data problem might actually be a small data problem in disguise. While the complete data might be big, often the data needed to answer a specific question is small. You might be able to find a subset, subsample, or summary that fits in memory and still allows you to answer the question that you’re interested in. The challenge here is finding the right small data, which often requires a lot of iteration. Another possibility is that your big data problem is actually a large number of small data problems. Each individual problem might fit in memory, but you have millions of them. For example, you might want to fit a model to each person in your dataset. That would be trivial if you had just 10 or 100 people, but instead you have a million. Fortunately each problem is independent of the others (a setup that is sometimes called embarrassingly parallel), so you just need a system (like Hadoop or Spark) that allows you to send different datasets to different computers for processing. Once you’ve figured out how to answer the question for a single subset using the tools described in this book, you learn new tools like sparklyr, rhipe, and ddr to solve it for the full dataset. 1.3.2 R, Julia, and friends In this book, you won’t learn anything about R, Julia, or any other programming language useful for data science. This isn’t because we think these tools are bad. They’re not! And in practice, most data science teams use a mix of languages, often at least R and Python. However, we strongly believe that it’s best to master one tool at a time. You will get better faster if you dive deep, rather than spreading yourself thinly over many topics. This doesn’t mean you should only know one thing, just that you’ll generally learn faster if you stick to one thing at a time. You should strive to learn new things throughout your career, but make sure your understanding is solid before you move on to the next interesting thing. We think Python is a great place to start your data science journey because it is an environment designed from the ground up to support data science. Python is not just a programming language, but it is also an interactive environment for doing data science. To support interaction, Python is a much more flexible language than many of its peers. 1.3.3 Non-rectangular data This book focuses exclusively on rectangular data: collections of values that are each associated with a variable and an observation. There are lots of datasets that do not naturally fit in this paradigm: including images, sounds, trees, and text. But rectangular data frames are extremely common in science and industry, and we believe that they are a great place to start your data science journey. 1.3.4 Hypothesis confirmation It’s possible to divide data analysis into two camps: hypothesis generation and hypothesis confirmation (sometimes called confirmatory analysis). The focus of this book is unabashedly on hypothesis generation, or data exploration. Here you’ll look deeply at the data and, in combination with your subject knowledge, generate many interesting hypotheses to help explain why the data behaves the way it does. You evaluate the hypotheses informally, using your scepticism to challenge the data in multiple ways. The complement of hypothesis generation is hypothesis confirmation. Hypothesis confirmation is hard for two reasons: You need a precise mathematical model in order to generate falsifiable predictions. This often requires considerable statistical sophistication. You can only use an observation once to confirm a hypothesis. As soon as you use it more than once you’re back to doing exploratory analysis. This means to do hypothesis confirmation you need to “preregister” (write out in advance) your analysis plan, and not deviate from it even when you have seen the data. We’ll talk a little about some strategies you can use to make this easier in modelling. It’s common to think about modelling as a tool for hypothesis confirmation, and visualisation as a tool for hypothesis generation. But that’s a false dichotomy: models are often used for exploration, and with a little care you can use visualisation for confirmation. The key difference is how often do you look at each observation: if you look only once, it’s confirmation; if you look more than once, it’s exploration. 1.4 Prerequisites We’ve made a few assumptions about what you already know in order to get the most out of this book. You should be generally numerically literate, and it’s helpful if you have some programming experience already. There are four things you need to run the code in this book: Python, VS Code, and a handful of other packages. Packages are the fundamental units of reproducible Python code. They include reusable functions, the documentation that describes how to use them, and sample data. 1.4.1 Python To download Python, go to python.org and download Python for your OS. A new major version of Python is released every few years, and there are 5-12 minor releases each year. 1.4.2 Visual Studio Code (VS Code) Visual Studio Code is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages (such as C++, C#, Java, Python, PHP, Go) and runtimes (such as .NET and Unity). Begin your journey with VS Code with these introductory videos. Microsoft provides a great overview of VS Code in their docs. When you use VS code, you’ll have four key regions in the interface: VS Code comes with a simple and intuitive layout that maximizes the space provided for the editor while leaving ample room to browse and access the full context of your folder or project. The UI is divided into five areas: Editor - The main area to edit your files. You can open as many editors as you like side by side vertically and horizontally. Side Bar - Contains different views like the Explorer to assist you while working on your project. Status Bar - Information about the opened project and the files you edit. Activity Bar - Located on the far left-hand side, this lets you switch between views and gives you additional context-specific indicators, like the number of outgoing changes when Git is enabled. Panels - You can display different panels below the editor region for output or debug information, errors and warnings, or an integrated terminal. Panel can also be moved to the right for more vertical space. Each time you start VS Code, it opens up in the same state it was in when you last closed it. The folder, layout, and opened files are preserved. Microsoft provides a great tutorial to get you through the following sections if you would like to follow their guide. 1.4.2.1 VS Code Python Extension Install the Python extension for VS Code from the Visual Studio Marketplace. For additional details on installing extensions, see Extension Marketplace. The Python extension is named Python and it’s published by Microsoft. You can follow more of their tutorial at code.visualstudio.com. The third video and fourth video in this Python for Beginners set of videos done by Microsoft can also guide you through the Python Extension. 1.4.2.2 VS Code Interactive Python Window An open-source project called Jupyter is the standard method for interactive Python use for data science or scientific computing. However, there are some issues with its use in a development environment. VS Code has provided a way for us to have the best of Python and Jupyter Notebooks with their Python Interactive Window. You will need to install the jupyter python package using pip or pip3 for the interactive Python window to work. See the following section for guidance on using pip. Using the VS Code functionality, you will work with a standard .py file instead of the .ipynb extension typically used with jupyter notebooks. The Python extension in VS Code will recognize # %% as a cell or chunk of python code and add notebook options to ‘Run Cell’ as well as other actions. You can see the code example bellow with the image of the view in VS Code as an example. Microsoft’s documentation goes into more detail (https://code.visualstudio.com/docs/python/jupyter-support-py). To make the interactive window use more functional you can ctrl + , or cmd + , on a mac to open the settings. From there you can search ‘Send Selection to Interactive Window’ and make sure the box is checked. Now you will be able to use shift + return to send a selected chunk of code or an entire cell. # %% msg = &quot;Hello World&quot; print(msg) # %% msg = &quot;Hello again&quot; print(msg) 1.4.3 Package Management (pip) You can install a standard set of data science python packages using pip. However, there are some complications using pip on computers with multiple versions of Python. pip: If your path environment is correct, then a standard pip install [package] will work. This is how most packages direct users to install Python packages. pip3: If your OS has Python 2 and Python 3 installed, you may need to use pip3 install [package]. Force Python version: You can run the pip related to a specific Python installation by using python -m pip install [package]. Some may need to provide the path to their Python installation if your Python path environment is not understood. A few cautions about package management with pip. Never run sudo pip install. If you don’t have root permissions or the OS package manager doesn’t have the package you need, use pip install --user. If you want multiple versions of the same library to coexist, to do Python development, or to isolate dependencies for any other reason, use virtual environments. Generally, you will want to update pip before installing packages - python -m pip install --user --upgrade pip setuptools wheel Conda, poetry, and pipenv are three other options for package management. However, we will focus on using pip. 1.4.3.1 pip package installation examples If we wanted to install the numpy, pandas, xlrd, matplolib, and seaborn packages, we would use pip. Depending on your OS configuration, one of the following should work. Everything in your path is clean and you are an admin on your computer pip install numpy pandas xlrd matplotlib seaborn Everything in your path is clean and you want to install package for the user pip install --user numpy pandas xlrd matplotlib seaborn You have multiple Python versions installed you want to install package for the user without a need to understand which pip maps to which Python python -m pip install --user numpy pandas xlrd matplotlib seaborn 1.4.4 The Data Science Packages You’ll also need to install some Python packages. A Python package is a collection of functions, data, and documentation that extends the capabilities of base Python. Using packages is key to the successful use of Python for data science. The majority of the packages that you will learn in this book are related to the so-called tidyverse packages in R. There are attempts to port they tidyverse package process into Python. We are not showing the tools that recreate the tidyverse in Python but those that current Data Scientists use to do equivelent work in Python. You will notice that pandas is the primary tool with a few packages that come with base Python. Pandas user guide will be referenced heavily as we progress. R Tidyverse Package Python Package dplyr pandas tidyr pandas tibble pandas stringr string and re forcats pandas categorical data readr pandas io tools readxl xlrd and openpyxl ggplot2 seaborn, altair, plotnine purrr built in map function base R stats package statsmodels tidymodels scikit-learn R tensorflow tensorflow R keras keras rmarkdown jupyter Notice that the visualization space in Python does not have a force like ggplot2. Chris Moffitt provided an efficient visualization tools diagram to help Python users with this decision. The following packages will give us a broad data science toolset in Python. pip install numpy pandas xlrd matplotlib pip install seaborn plotnine altair vega_datasets pip install statsmodels scikit-learn pip install jupyter On your computer, type that above of code in the command line. The Python package manager pip will download the packages from PyPi and install them on to your computer. If you have problems installing, make sure that you are connected to the internet. You will not be able to use the functions and objects in a package until you load it with import. It is common in Python for each package to have a standard abbreviated name. For example, numpy is imported as ‘np’, and pandas is imported as ‘pd’ in the code chunk below. import numpy as np import pandas as pd import string import re import matplotlib import matplotlib.pyplot as plt import seaborn as sns from plotnine import * import altair as alt There are many other excellent packages that are not included here. 1.5 Running Python code The previous section showed you a couple of examples of running Python code. Code in the book looks like this: 1 + 2 #&gt; 3 #&gt; 3 If you run the same code in interactive python with VS Code, it will look like this: [1] 1 + 2 3 The Python Interactive window can be used as a standalone console with arbitrary code (with or without code cells). To use the window as a console, open it with the Python: Show Python Interactive window command from the Command Palette. You can then type in code, using Enter to go to a new line and Shift+Enter to run the code. There are two main differences. In your interactive window, you type after the [#]; we don’t show the line number in the book. In the book, output is commented out with #&gt;; in your console it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and into the interactive window. Throughout the book we use a consistent set of conventions to refer to code: Functions are in a code font and followed by parentheses, like sum(), or mean(). Other R objects (like data or function arguments) are in a code font, without parentheses, like flights or x. 1.6 Getting help and learning more This book is not an island; there is no single resource that will allow you to master Python for data science. As you start to apply the techniques described in this book to your own data you will soon find questions that we do not answer. This section describes a few tips on how to get help, and to help you keep learning. If you get stuck, start with Google. Typically adding “python” to a query is enough to restrict it to relevant results: if the search isn’t useful, it often means that there aren’t any Python-specific results available. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it! Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = \"en\") and re-run the code; you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Start by spending a little time searching for an existing answer, including [python] to restrict your search to questions and answers that use Python. If you don’t find anything useful, prepare a minimal reproducible example or reprex. A good reprex makes it easier for other people to help you, and often you’ll figure out the problem yourself in the course of making it. There are three things you need to include to make your example reproducible: required packages, data, and code. Packages should be imported at the top of the script, so it’s easy to see which ones the example needs. This is a good time to check that you’re using the latest version of each package; it’s possible you’ve discovered a bug that’s been fixed since you installed the package. The easiest way to include data in a question is to create a minimal example to that recreates it. Try and find the smallest subset of your data that still reveals the problem. Spend a little bit of time ensuring that your code is easy for others to read: Make sure you’ve used spaces and your variable names are concise, yet informative. Use comments to indicate where your problem lies. Do your best to remove everything that is not related to the problem. The shorter your code is, the easier it is to understand, and the easier it is to fix. Finish by checking that you have actually made a reproducible example by starting a fresh Python session to run your script in. You should also spend some time preparing yourself to solve problems before they occur. Investing a little time in learning Python each day will pay off handsomely in the long run. One way is to follow what Wes McKinney, Garrett, and everyone else at RStudio are doing on the Pandas blog or the ossdata blog. This is where they post announcements about new packages and new IDE features. You might also want to follow Wes McKinney (@wesmckinn) on Twitter, or follow @code to keep up with new features in VS Code. To keep up with the data science community more broadly, we recommend reading https://planet.scipy.org/#. If you’re an active Twitter user, follow the #datascience hashtag. 1.7 Datasets access The data used in R for Data Science is generally within the R packages themselves. Many of the Python data science packages also come with datasets upon import. However, we will use the original datasets presented in R for Data Science. We have a data4python4ds GitHub data repository that contains all the datasets in varied file formats and all examples will use Pandas to read the data from GitHub. 1.8 Acknowledgements The text of this book is largely the product of Hadley and Garrett. J. Hathaway has ported the code and descriptions for using VS Code. You can see the original acknowledgements here. 1.9 Colophon An online version of this book is available at https://byuidatascience.github.io/python4ds/. It will continue to evolve. The source of the book is available at https://github.com/byuidatascience/python4ds. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with: sessioninfo::session_info() #&gt; ─ Session info ─────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 3.6.3 (2020-02-29) #&gt; os macOS Catalina 10.15.4 #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz America/Boise #&gt; date 2020-05-13 #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────── #&gt; package * version date lib source #&gt; assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) #&gt; backports 1.1.6 2020-04-05 [1] CRAN (R 3.6.3) #&gt; bookdown 0.18 2020-03-05 [1] CRAN (R 3.6.0) #&gt; broom 0.5.6 2020-04-20 [1] CRAN (R 3.6.2) #&gt; cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.6.0) #&gt; cli 2.0.2 2020-02-28 [1] CRAN (R 3.6.0) #&gt; codetools 0.2-16 2018-12-24 [1] CRAN (R 3.6.3) #&gt; colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.6.0) #&gt; crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) #&gt; DBI 1.0.0 2018-05-02 [1] CRAN (R 3.6.0) #&gt; dbplyr 1.4.2 2019-06-17 [1] CRAN (R 3.6.0) #&gt; digest 0.6.25 2020-02-23 [1] CRAN (R 3.6.0) #&gt; dplyr * 0.8.5 2020-03-07 [1] CRAN (R 3.6.0) #&gt; ellipsis 0.3.0 2019-09-20 [1] CRAN (R 3.6.0) #&gt; evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.0) #&gt; fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.0) #&gt; forcats * 0.5.0 2020-03-01 [1] CRAN (R 3.6.1) #&gt; fs 1.4.1 2020-04-04 [1] CRAN (R 3.6.3) #&gt; generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.0) #&gt; ggplot2 * 3.3.0 2020-03-05 [1] CRAN (R 3.6.1) #&gt; glue 1.4.0 2020-04-03 [1] CRAN (R 3.6.3) #&gt; gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.0) #&gt; haven 2.2.0 2019-11-08 [1] CRAN (R 3.6.0) #&gt; hms 0.5.3 2020-01-08 [1] CRAN (R 3.6.0) #&gt; htmltools 0.4.0 2019-10-04 [1] CRAN (R 3.6.0) #&gt; httr 1.4.1 2019-08-05 [1] CRAN (R 3.6.0) #&gt; jsonlite 1.6.1 2020-02-02 [1] CRAN (R 3.6.0) #&gt; knitr 1.28 2020-02-06 [1] CRAN (R 3.6.0) #&gt; lattice 0.20-38 2018-11-04 [1] CRAN (R 3.6.3) #&gt; lifecycle 0.2.0 2020-03-06 [1] CRAN (R 3.6.0) #&gt; lubridate 1.7.4 2018-04-11 [1] CRAN (R 3.6.0) #&gt; magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) #&gt; modelr 0.1.5 2019-08-08 [1] CRAN (R 3.6.0) #&gt; munsell 0.5.0 2018-06-12 [1] CRAN (R 3.6.0) #&gt; nlme 3.1-144 2020-02-06 [1] CRAN (R 3.6.3) #&gt; pillar 1.4.4 2020-05-05 [1] CRAN (R 3.6.2) #&gt; pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 3.6.0) #&gt; purrr * 0.3.4 2020-04-17 [1] CRAN (R 3.6.2) #&gt; R6 2.4.1 2019-11-12 [1] CRAN (R 3.6.1) #&gt; Rcpp 1.0.4.6 2020-04-09 [1] CRAN (R 3.6.3) #&gt; readr * 1.3.1 2018-12-21 [1] CRAN (R 3.6.0) #&gt; readxl 1.3.1 2019-03-13 [1] CRAN (R 3.6.0) #&gt; reprex 0.3.0 2019-05-16 [1] CRAN (R 3.6.0) #&gt; reticulate 1.14 2019-12-17 [1] CRAN (R 3.6.1) #&gt; rlang 0.4.6 2020-05-02 [1] CRAN (R 3.6.2) #&gt; rmarkdown 2.1.3 2020-05-07 [1] Github (rstudio/rmarkdown@d7e1bda) #&gt; rstudioapi 0.11 2020-02-07 [1] CRAN (R 3.6.0) #&gt; rvest 0.3.5 2019-11-08 [1] CRAN (R 3.6.0) #&gt; scales 1.1.0 2019-11-18 [1] CRAN (R 3.6.0) #&gt; sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.0) #&gt; stringi 1.4.6 2020-02-17 [1] CRAN (R 3.6.0) #&gt; stringr * 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) #&gt; tibble * 3.0.1 2020-04-20 [1] CRAN (R 3.6.2) #&gt; tidyr * 1.0.3 2020-05-07 [1] CRAN (R 3.6.2) #&gt; tidyselect 1.0.0 2020-01-27 [1] CRAN (R 3.6.0) #&gt; tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 3.6.0) #&gt; vctrs 0.2.4 2020-03-10 [1] CRAN (R 3.6.0) #&gt; withr 2.2.0 2020-04-20 [1] CRAN (R 3.6.2) #&gt; xfun 0.13 2020-04-13 [1] CRAN (R 3.6.2) #&gt; xml2 1.3.2 2020-04-23 [1] CRAN (R 3.6.2) #&gt; yaml 2.2.1 2020-02-01 [1] CRAN (R 3.6.0) #&gt; #&gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library "],
["explore-intro.html", "2 Introduction", " 2 Introduction The goal of the first part of this book is to get you up to speed with the basic tools of data exploration as quickly as possible. Data exploration is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again. The goal of data exploration is to generate many promising leads that you can later explore in more depth. In this part of the book you will learn some useful tools that have an immediate payoff: Visualisation is a fun place to start with Python programming, because the payoff is so clear: you get to make elegant and informative plots that help you understand data. In data visualisation you’ll dive into visualisation, learning the basic structure of a pandas plot, and powerful techniques for turning data into plots. Visualisation alone is typically not enough, so in data transformation you’ll learn the key verbs that allow you to select important variables, filter out key observations, create new variables, and compute summaries. Finally, in exploratory data analysis, you’ll combine visualisation and transformation with your curiosity and scepticism to ask and answer interesting questions about data. Modelling is an important part of the exploratory process, but you don’t have the skills to effectively learn or apply it yet. We’ll come back to it in modelling, once you’re better equipped with more data wrangling and programming tools. Nestled among these three chapters that teach you the tools of exploration are three chapters that focus on your Python workflow. In workflow: basics, workflow: scripts, and workflow: projects you’ll learn good practices for writing and organising your R code. These will set you up for success in the long run, as they’ll give you the tools to stay organised when you tackle real projects. "],
["data-visualisation.html", "3 Data visualisation 3.1 Introduction 3.2 First steps 3.3 Aesthetic mappings 3.4 Common problems 3.5 Facets 3.6 Geometric objects 3.7 Statistical transformations 3.8 Position adjustments 3.9 Coordinate systems (maps) 3.10 The layered grammar of graphics 3.11 Altair’s grammar of graphics", " 3 Data visualisation 3.1 Introduction “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey This chapter will teach you how to visualise your data using Altair. Python has several systems for making graphs, but altiar is one of the most elegant and versatile. Altair implements the declarative visualization much like the grammar of graphics, a coherent system for describing and building graphs. With altair, you can do more faster by learning one system and applying it in many places. If you’d like to learn more about Altair before you start, I’d recommend reading “Altair: Interactive Statistical Visualizations for Python”, https://joss.theoj.org/papers/10.21105/joss.01057.pdf. We should note that we are building this book using R with the package bookdown. Rendering Altair graphics using a python chunk is not straight forward but is not important for our use in VS Code. In VS Code the example chunks will render in the interactive Python viewer automatically. The following R code chunks show how we are rendering the Altair graphics in this book. Thanks to ijlyttle for his GitHub Gist. # ```{R, echo=FALSE} # vegawidget::as_vegaspec(py$chart$to_json()) # ``` # For Python examples that show chart.save() #```{r, message = FALSE, echo=FALSE} #knitr::include_graphics(&quot;screenshots/chartp_chartleft.png&quot;) #``` 3.1.1 Prerequisites This chapter focusses on Altair. Language has been shifted using the material from Altair’s materials. To access the datasets, help pages, and functions that we will use in this chapter, load the Python data science tools by running this code: import pandas as pd import altair as alt If you run this code and get the error message “No module named ‘altair’” or “No module named ‘pandas’”, you’ll need to first install them. python -m pip install pandas altair You only need to install a package once, but you need to reload it every time you start a new session. 3.1.2 Altair data management When specifying data in Altair, we can use pandas DataFrame objects or other Altair options. According to the Altair documentation, the use of a pandas DataFrame will prompt Altair to store the entire data set in JSON format in the chart object. You should be carefully creating Altair specs with all the data in the chart object for use in HTML or Jupyter Notebooks. If you try to plot a data set with more than 5000 rows, Altair will return a maxRowsError. In this book, we will save the Altair chart as a ‘.png’ file to avoid dealing with large stored data in our ‘.html’ files. We have elected to use the Local Filesystem approach proposed by Altair. They do note that the filesystem approach may not work on some cloud-based Jupyter notebook services. alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) 3.2 First steps Let’s use our first graph to answer a question: Do cars with big engines use more fuel than cars with small engines? You probably already have an answer, but try to make your answer precise. What does the relationship between engine size and fuel efficiency look like? Is it positive? Negative? Linear? Nonlinear? 3.2.1 The mpg data frame You can test your answer with the mpg data frame found in ggplot2 (aka ggplot2::mpg). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). The ‘mpg’ data contains observations collected by the US Environmental Protection Agency on 38 models of car. We will identify the ‘mpg’ data using mpg for the remainder of this introduction. mpg = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv&quot;) Among the variables in mpg are: displ, a car’s engine size, in litres. hwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. To learn more about mpg, read informat at data4python4ds. 3.2.2 Creating an Altair plot To plot mpg, run this code to put displ on the x-axis and hwy on the y-axis: chart = (alt.Chart(mpg). encode( x=&#39;displ&#39;, y=&#39;hwy&#39;). mark_circle() ) The plot shows a negative relationship between engine size (displ) and fuel efficiency (hwy). In other words, cars with big engines use more fuel. Does this confirm or refute your hypothesis about fuel efficiency and engine size? With Altair, you begin a plot with the function Chart(). Chart() creates a Chart object that you can add layers to. The only argument of Chart() is the dataset to use in the graph. So Chart(mpg) creates an Chart object upon which we can marks. You complete your graph by adding one or more marks to Chart(). The attribute mark_point() adds a layer of points to your plot, which creates a scatterplot. Altair comes with many mark methods that each add a different type of layer to a plot. You’ll learn a whole bunch of them throughout this chapter. Each mark method in Altair has an encode() attribute. This defines how variables in your dataset are encoded to visual properties. The encode() method is always paired with x and y arguments to specify which variables to map to the x and y axes. Altair looks for the encoded variables in the data argument, in this case, mpg. For pandas dataframes, Altair automatically determines the appropriate data type for the mapped column. 3.2.3 A graphing template Let’s turn this code into a reusable template for making graphs with ggplot2. To make a graph, replace the bracketed sections in the code below with a dataset, a geom function, or a collection of mappings. (alt.Chart(&lt;DATA&gt;). &lt;mark_*().&gt; encode(&lt;ENCODINGS&gt;)) The rest of this chapter will show you how to complete and extend this template to make different types of graphs. We will begin with the &lt;ENCODINGS&gt; component. 3.2.4 Exercises Run Chart(mpg).mark_points(). What do you see? How many rows are in mpg? How many columns? What does the drv variable describe? Make a scatterplot of hwy vs cyl. What happens if you make a scatterplot of class vs drv? Why is the plot not useful? 3.3 Aesthetic mappings “The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey In the plot below, one group of points (highlighted in red) seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars? Let’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular). You can add a third variable, like class, to a two dimensional scatterplot by mapping it to an encoding. An encoding is a visual property of the objects in your plot. Encodings include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its encoded properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe encoded properties. Here we change the levels of a point’s size, shape, and color to make the point small, triangular, or blue: You can convey information about your data by mapping the encodings in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. chart = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color = &quot;class&quot; ) ) (We don’t prefer British English, like Hadley, so don’t use colour instead of color.) To map an encoding to a variable, associate the name of the encoding to the name of the variable inside encode(). Altair will automatically assign a unique level of the encoding (here a unique color) to each unique value of the variable, a process known as scaling. Altair will also add a legend that explains which levels correspond to which values. The colors reveal that many of the unusual points are two-seater cars. These cars don’t seem like hybrids, and are, in fact, sports cars! Sports cars have large engines like SUVs and pickup trucks, but small bodies like midsize and compact cars, which improves their gas mileage. In hindsight, these cars were unlikely to be hybrids since they have large engines. In the above example, we mapped class to the color encoding, but we could have mapped class to the size encoding in the same way. In this case, the exact size of each point would reveal its class affiliation. Mapping an unordered variable (class) to an ordered aesthetic (size) is not a good idea. chart = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, size = &quot;class&quot; ) ) Or we could have mapped class to the opacity encoding, which controls the transparency of the points, or to the shape encoding, which controls the shape of the points. # First chart1 = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, opacity = &quot;class&quot; ) ) # Second chart2 = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, shape = &quot;class&quot; ) ) chart1.save(&quot;screenshots/altair_opacity.png&quot;) #&gt; WARN Channel opacity should not be used with an unsorted discrete field. chart2.save(&quot;screenshots/altair_shape.png&quot;) #&gt; WARN shape dropped as it is incompatible with &quot;circle&quot;. Altair will only use 8 shapes for one chart. Charting more than 8 shapes is not recommended as the shapes simply recycle. For each encoding, you use encode() to associate the name of the encoding with a variable to display. The encode() function gathers together each of the encoded mappings used by a layer and passes them to the layer’s mapping argument. The syntax highlights a useful insight about x and y: the x and y locations of a point are themselves encodings, visual properties that you can map to variables to display information about the data. Once you map an encoding, Altair takes care of the rest. It selects a reasonable scale to use with the encoding, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, Altair does not create a legend, but it creates an axis line with tick marks and a label. The axis line acts as a legend; it explains the mapping between locations and values. You can also configure the encoding properties of your mark manually. For example, we can make all of the points in our plot blue: chart = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color = alt.value(&quot;blue&quot;) ) ) Here, the color doesn’t convey information about a variable, but only changes the appearance of the plot. To set an encoding manually, use alt.value() by name as an argument of your encode() function; i.e. the value goes inside of alt.value(). You’ll need to pick a level that makes sense for that encoding: The name of a color as a character string. The size of a point in pixels. The shape of a point as a character string. Note that only a limited set of mark properties can be bound to encodings, so for some (e.g. fillOpacity, strokeOpacity, etc.) the encoding approach using alt.value() is not available. Encoding settings will always override local or global configuration settings. There are other methods for manually encoding properties as explained in the Altair documentation 3.3.1 Exercises Which variables in mpg are categorical? Which variables are continuous? How can you see this information when you run mpg? (Hint mpg.dtypes) Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables? What happens if you map the same variable to multiple encodings? What does the stroke encoding do? What shapes does it work with? (Hint: use mark_point()) 3.4 Common problems As you start to run Python code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing Python code for months, and every day I still write code that doesn’t work! Start by carefully comparing the code that you’re running to the code in the book. Python is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \". One common problem when creating Altair graphics as shown in this book, is to put the () in the wrong place: the ( comes before the alt.chart() command and the ) has to come at the end of the command. For example the code below works in Python. alt.Chart(mpg).mark_circle().encode(x = &quot;displ&quot;, y = &quot;hwy&quot;) However, the complexity of the more details graphics necessicates placing the code on multiple lines. When using multiple lines we need the enclosing (). Make sure you haven’t accidentally excluded a ( or ) like this (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;) or placed the () incorrectly like this (chart = alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;) ) If you’re still stuck, try the help. You can get help about any Altair function from their website - https://altair-viz.github.io/, or hovering over the function name in VS Code. If that doesn’t help, carefully read the error message. Sometimes the answer will be buried there! But when you’re new to Python, the answer might be in the error message but you don’t yet know how to understand it. Another great tool is Google: try googling the error message, as it’s likely someone else has had the same problem, and has gotten help online. 3.5 Facets One way to add additional variables is with encodings. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data. To facet your plot by a single variable, use facet(). The first argument of facet() is . The variable that you pass to facet_wrap() should be discrete. chart_f = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). facet( facet = &quot;class&quot;, columns = 4 ) ) chart_f.save(&quot;screenshots/altair_facet_1.png&quot;) To facet your plot on the combination of two variables, The first argument of facet() is also column and the second is row. This time the formula should contain two variable names. chart_f2 = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). facet( column = &quot;drv&quot;, row = &quot;cyl&quot; ) ) chart_f2.save(&quot;screenshots/altair_facet_2.png&quot;) #&gt; WARN row encoding should be discrete (ordinal / nominal / binned). If you prefer to not facet in the rows or columns dimension, simply remove that facet argument. You can read more about compound charts in the Altair documentation. 3.5.1 Exercises What happens if you facet on a continuous variable? What do the empty cells in plot with facet(column = \"drv\", row = \"cyl\") mean? How do they relate to this plot? (alt.Chart(mpg). mark_circle(). encode( x = &quot;drv&quot;, y = &quot;cyl&quot;) ) What plots does the following code make? What does . do? (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;). facet(column = &quot;drv&quot;) ) (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;). facet(row = &quot;cyl&quot;) ) Take the first faceted plot in this section: What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset? When using facet() you should usually put the variable with more unique levels in the columns. Why? 3.6 Geometric objects How are these two plots similar? chartp = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chartf = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chartp.save(&quot;screenshots/altair_basic_points.png&quot;) chartf.save(&quot;screenshots/altair_smooth_line.png&quot;) Both plots contain the same x variable, the same y variable, and both describe the same data. But the plots are not identical. Each plot uses a different visual object to represent the data. In Altair syntax, we say that they use different marks. A mark is the geometrical object that a plot uses to represent data. People often describe plots by the type of mark that the plot uses. For example, bar charts use bar marks, line charts use line marks, boxplots use boxplot marks, and so on. Scatterplots break the trend; they use the point mark. As we see above, you can use different marks to plot the same data. The first plot uses the point mark, and the second plot uses the line mark, a smooth line fitted to the data is calculated using a transformation. To change the mark in your plot, change the mark function that you add to Chart(). Every mark function in Altair has encode arguments. However, not every encoding works with every mark. You could set the shape of a point, but you couldn’t set the “shape” of a line. On the other hand, you could set the type of line. mark_line() will draw a different line, with a different strokeDash, for each unique value of the variable that you map to strokeDash. chartl = (alt.Chart(mpg). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, strokeDash = &quot;drv&quot; ) ) chartl.save(&quot;screenshots/altair_dashed_lines.png&quot;) Here mark_line() separates the cars into three lines based on their drv value, which describes a car’s drivetrain. One line describes all of the points with a 4 value, one line describes all of the points with an f value, and one line describes all of the points with an r value. Here, 4 stands for four-wheel drive, f for front-wheel drive, and r for rear-wheel drive. If this sounds strange, we can make it more clear by overlaying the lines on top of the raw data and then coloring everything according to drv. Notice that this plot contains two marks in the same graph! If this makes you excited, buckle up. We will learn how to place multiple marks on the same chart very soon. Altair provides about 15 marks. The best way to get a comprehensive overview is the Altair marks page, which you can find at https://altair-viz.github.io/user_guide/marks.html. Many marks, like mark_line(), use a single mark object to display multiple rows of data. For these marks, you can set the detail encoding to a categorical variable to draw multiple objects. Altair will draw a separate object for each unique value of the detail variable. In practice, Altair will automatically group the data for these marks whenever you map an encoding to a discrete variable (as in the strokeDash example). It is convenient to rely on this feature because the detail encoding by itself does not add a legend or distinguishing features to the marks. chartleft = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chartmiddle = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, detail = &quot;drv&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) chartright = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color=alt.Color(&quot;drv&quot;, legend=None) ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) chartleft.save(&quot;screenshots/altair_chartleft.png&quot;) chartmiddle.save(&quot;screenshots/altair_chartmiddle.png&quot;) chartright.save(&quot;screenshots/altair_chartright.png&quot;) To display multiple marks in the same plot, you can used layered charts as shown in the example below that uses the chartleft object from the above code chunk: chartp = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ). mark_circle() ) chart = chartp + chartleft chart.save(&quot;screenshots/altair_chartcombine.png&quot;) This, however, introduces some duplication in our code. Imagine if you wanted to change the y-axis to display cty instead of hwy. You’d need to change the variable in two places, and you might forget to update one. You can avoid this type of repetition by passing a set of encodings to a base alt.Chart(). Altair will treat these encodings as global encodings that apply to each mark layer in the layered chart. In other words, this code will produce the same plot as the previous code: base =(alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart = base.mark_circle() + base.transform_loess(&quot;displ&quot;, &quot;hwy&quot;).mark_line() chart.save(&quot;screenshots/altair_combine_clean.png&quot;) If you place encodings in an encode function, Altair will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the base encodings for that layer only. This makes it possible to display different aesthetics in different layers. Alatair automatically chooses useful plot settings and chart configurations to allow you to think about data instead of the programming mechanics of the chart. You can review their guidance on customizing visualizations to see the varied ways to change the look of your graphic. base =(alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart = base.encode(color = &quot;drv&quot;).mark_circle() + base.transform_loess(&quot;displ&quot;, &quot;hwy&quot;).mark_line() chart.save(&quot;screenshots/altair_combine_clean_color.png&quot;) You can use the same idea to specify different data for each layer. Here, our smooth line displays just a subset of the mpg dataset, the subcompact cars. The local data argument in geom_smooth() overrides the global data argument in ggplot() for that layer only. #column name of class does not work nicely with Altair filter. base = (alt.Chart(mpg.rename(columns = {&quot;class&quot;: &quot;class1&quot;})). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart_smooth_sub = (base. transform_filter( alt.datum.class1 == &quot;subcompact&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chart = base.encode(color = &quot;class1&quot;).mark_circle() + chart_smooth_sub chart.save(&quot;screenshots/altair_combine_clean_color_filter.png&quot;) (You’ll learn how pandas filter works in the chapter on data transformations. To keep the same base chart, filtering is done with Altair in this example: for now, just know that this command selects only the subcompact cars.) 3.6.1 Exercises What geom would you use to draw a line chart? A boxplot? A histogram? An area chart? What does legend=None in alt.Color() do? What happens if you remove it? Why do you think I used it earlier in the chapter? Recreate the Python code necessary to generate the following graphs. 3.7 Statistical transformations Next, let’s take a look at a bar chart. Bar charts seem simple, but they are interesting because they reveal something subtle about plots. Consider a basic bar chart, as drawn with mark_bar(). The following chart displays the total number of diamonds in the diamonds dataset, grouped by cut. The diamonds dataset comes in ggplot2 R package and can be used in Python using the following Python command. Note that we also need to use pandas to format a few of the columns as ordered categorical to have the diamonds DataFrame act like it does in R. diamonds = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot;) diamonds[&#39;cut&#39;] = pd.Categorical(diamonds.cut, ordered = True, categories = [&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot; ]) diamonds[&#39;color&#39;] = pd.Categorical(diamonds.color, ordered = True, categories = [&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;]) diamonds[&#39;clarity&#39;] = pd.Categorical(diamonds.clarity, ordered = True, categories = [&quot;I1&quot;, &quot;SI2&quot;, &quot;SI1&quot;, &quot;VS2&quot;, &quot;VS1&quot;, &quot;VVS2&quot;, &quot;VVS1&quot;, &quot;IF&quot;]) It contains information about ~54,000 diamonds, including the price, carat, color, clarity, and cut of each diamond. The chart shows that more diamonds are available with high quality cuts than with low quality cuts. chart = (alt.Chart(diamonds). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count():Q&quot;) ). mark_bar(). properties(width = 400) ) chart.save(&quot;screenshots/altair_diamond_bar.png&quot;) On the x-axis, the chart displays cut, a variable from diamonds. On the y-axis, it displays count, but count is not a variable in diamonds! Many graphs, like scatterplots, plot the raw values of your dataset. Other graphs, like bar charts, calculate new values to plot: bar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin. smoothers fit a model to your data and then plot predictions from the model. boxplots compute a robust summary of the distribution and then display a specially formatted box. The algorithm used to calculate new values for a graph is called a transform, short for transformation. The figure below describes how this process works with mark_bar(). You must explicitely define the transformation a mark uses through transformations using alt.Y() or alt.X() function. For example, mark_bar() requires the y encoding alt.Y(\"count():Q\"). A histogram is created using mark_bar() with transformations on both the x and y axes. The bin argument accepts a boolean or an alt.Bin() function where the argument maxbins can be used - bin=alt.Bin(maxbins=100). chart = (alt.Chart(diamonds). encode( x =alt.X(&quot;price&quot;, bin=True), y =alt.Y(&quot;count()&quot;) ). mark_bar() ) chart.save(&quot;screenshots/altair_histogram.png&quot;) For more complicated transformations Altair provides transform functions. We saw one of these transforms previously when we used mark_line() to describe each drive type. If you are working with pandas DataFrames then you may want to do these transformations using pandas. Altair’s transformations can be used with DataFrames as well as JSON files or URL pointers to CSV files. chartright = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color=alt.Color(&quot;drv&quot;, legend=None) ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) Finally, mark_boxplot() is available which does the statistical transformations for you after you specify the encodings for the x and y axes. chart = (alt.Chart(diamonds). encode( y =&quot;price&quot;, x =&quot;cut&quot; ). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_boxplot.png&quot;) 3.8 Position adjustments There’s one more piece of magic associated with bar charts. You can colour a bar chart using either the stroke aesthetic, or, more usefully, color: chart_left = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), stroke = &quot;cut&quot; ). properties(width = 200) ) chart_right = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), color = &quot;cut&quot; ). properties(width = 200) ) chart_left.save(&quot;screenshots/altair_bar_linecolor.png&quot;) chart_right.save(&quot;screenshots/altair_bar_fillcolor.png&quot;) Note what happens if you map the color encoding to another variable, like clarity: the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity. chart = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), color = &quot;clarity&quot; ). properties(width = 200) ) chart.save(&quot;screenshots/stacked_barchart.png&quot;) The stacking is performed automatically by mark_bar(). If you don’t want a stacked bar chart, you can use use the stack argument in alt.Y() one of three other options: \"identity\", \"dodge\" or \"fill\". position = \"identity\" will place each object exactly where it falls in the context of the graph. This is not very useful for bars, because it overlaps them. To see that overlapping we either need to make the bars slightly transparent by setting alpha to a small value, or completely transparent by setting fill = NA. chart_left = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=None), color = &quot;clarity&quot;, opacity = alt.value(1/5) ). properties(width = 200) ) chart_right = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=None), stroke = &quot;clarity&quot;, color = alt.value(&quot;none&quot;) ). properties(width = 200) ) chart_left.save(&quot;screenshots/altair_nostack_opacity.png&quot;) chart_right.save(&quot;screenshots/altair_nostack_lines.png&quot;) position = \"fill\" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups. chart = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=&#39;normalize&#39;), color = &quot;clarity&quot; ). properties(width = 200) ) chart.save(&quot;screenshots/altair_normalize_bar.png&quot;) knitr::include_graphics(&quot;screenshots/altair_normalize_bar.png&quot;) Placing overlapping objects directly beside one another is done by using the column encoding. This makes it easier to compare individual values with a common baseline. chart = (alt.Chart(diamonds). mark_bar(). encode( x=&#39;clarity&#39;, y=alt.Y(&#39;count()&#39;), color=&#39;clarity&#39;, column=&#39;cut&#39; ) ) chart.save(&quot;screenshots/altair_position_dodge.png&quot;) Altair does not have a simple way to add random jitter to points using an encoding or simple argument to alt.X() or alt.Y(). Altair can create a jittered point plot, also called a stripplot. However, it is not as straight forward. We should note that Altair also does not provide piechart or donut chart marks as well. 3.9 Coordinate systems (maps) Coordinate systems are generally on an x and y axis in Altair. This coordinate system is the Cartesian coordinate system, where the x and y positions act independently to determine the location of each point. Maps are on a Cartesian coordinate system, but their behavior is different as they require a transformation or projection of the globe to an x and y Cartesian plane. Visualizing data on maps can get complicated quickly. Altair has a some mapping marks available for charts. The data formats for plotting with maps also get more involved. You can use the gpdvega package to help with spatial data in Altair. Many Python users us geopandas or Shapely to handle spatial data in Python. PennState has a nice list of spatial tools for Python. 3.10 The layered grammar of graphics In the previous sections, you learned much more than how to make scatterplots, bar charts, and boxplots. You learned a foundation that you can use to make any type of plot with ggplot2. To see this, let’s add position adjustments, stats, coordinate systems, and faceting to our code template: (alt.Chart(&lt;DATA&gt;). encoding( ).facet( column = &lt;FACET VARIABLE&gt; ). &lt;TRANFORM_FUNCTION&gt;(). &lt;MARK_FUNCTION&gt;(). properties( width = , height = ) ) Altair’s general template takes four main parameters - alt.Chart(), mark_*(), encode(), and facet(). In practice, only need to supply the first three parameters to make a chart because Altair will provide useful defaults for everything except the data, the encodings, and the mark function. The parameters in the template compose the grammar of graphics, a formal system for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a mark, a set of encodings, a transformation, a position adjustment, and a faceting scheme. To see how this works, consider how you could build a basic plot from scratch: you could start with a dataset and then transform it into the information that you want to display (with a stat). Next, you could choose a geometric object to represent each observation in the transformed data. You could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic. You’d then select a coordinate system to place the geoms into. You’d use the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables. At that point, you would have a complete graph, but you could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting). You could also extend the plot by adding one or more additional layers, where each additional layer uses a dataset, a geom, a set of mappings, a stat, and a position adjustment. You could use this method to build any plot that you imagine. In other words, you can use the code template that you’ve learned in this chapter to build hundreds of thousands of unique plots. 3.11 Altair’s grammar of graphics Plotnine is a Python implementation of ggplot2 in R’s grammar of graphics using matplotlib as the plotting backend. Altair’s implementation of the grammar of graphics, much like ggplot2 or plotnine at a high level. However, it uses the Vega-Lite grammar of graphics constructs and plotting backend. Below are some useful links that will help you dig deeper into the Altair implementation of the grammar of graphics. Altair mark guidance Altair encoding guidance Altair facet guidance Altair tranformations guidance Altair chart customization Altair sorting barcharts Altair theme editing Altair Encoding data types "],
["workflow-basics.html", "4 Workflow: basics 4.1 Coding basics 4.2 What’s in a name? 4.3 Calling functions 4.4 Exercises", " 4 Workflow: basics You now have some experience running Python code. I didn’t give you many details, but you’ve obviously figured out the basics, or you would’ve thrown this book away in frustration! Frustration is natural when you start programming in Python, because it is such a stickler for punctuation, and even one character out of place will cause it to complain. But while you should expect to be a little frustrated, take comfort in that it’s both typical and temporary: it happens to everyone, and the only way to get over it is to keep trying. Before we go any further, let’s make sure you’ve got a solid foundation in running Python code, and that you know about some of the most helpful VS Code features. 4.1 Coding basics Let’s review some basics we’ve so far omitted in the interests of getting you plotting as quickly as possible. You can use Python as a calculator: 1 / 200 * 30 (59 + 73 + 2) / 3 You can create new objects with =: x = 3 * 4 All Python statements where you create objects, assignment statements, have the same form: object_name = value When reading that code say “object name gets value” in your head. You will make lots of assignments in data science Python programming. It is also good code formatting practice to wrap = with spaces. Code is miserable to read on a good day, so giveyoureyesabreak and use spaces. 4.2 What’s in a name? Object names must start with a letter, and can only contain letters, numbers, and _. You cannot use . like R. You want your object names to be descriptive, so you’ll need a convention for multiple words. Python coding conventions recommend snake_case where you separate lowercase words with _. i_use_snake_case otherPeopleUseCamelCase some.people.use.periods And_aFew.People_RENOUNCEconvention We’ll come back to code style later, in [functions]. You can inspect an object by typing its name: x #&gt; 12 Make another assignment: this_is_a_really_long_name = 2.5 To inspect this object, try out VS Codes completion facility: type “this_”, pause, add characters until you have a unique prefix, then press shift + return. Make yet another assignment: python_rocks = 2 ^ 3 Let’s try to inspect it: python_rock #&gt; --------------------------------------------------------------------------- #&gt; NameError Traceback (most recent call last) #&gt; ~.../python4ds_practice.py in #&gt; ----&gt; 1 python_rock #&gt; NameError: name &#39;python_rock&#39; is not defined Python_rocks #&gt; --------------------------------------------------------------------------- #&gt; NameError Traceback (most recent call last) #&gt; ~.../python4ds_practice.py in #&gt; ----&gt; 1 Python_rocks #&gt; NameError: name &#39;Python_rocks&#39; is not defined There’s an implied contract between you and Python: it will do the tedious computation for you, but in return, you must be completely precise in your instructions. Typos matter. Case matters. 4.3 Calling functions Python does not have a large collection of built-in mathematical and statistical functions. You will need to use pandas, numpy, scikit-learn, and statsmodels to get the suite of functions for working and modeling with data. import pandas as pd import numpy as np import sklearn as sk import statsmodels.api as sm Functions are called like this: &lt;AS PACKAGE NAME&gt;.function_name(arg1 = val1, arg2 = val2, ...) Let’s try using np.arange() which returns regular arangement of numbers and, while we’re at it, learn more helpful features of intellisense in VS code. Type np.ar and pause. A popup shows you possible completions. Specify np.arange() by typing more (a “ange”) to disambiguate, or by using ↑/↓ arrows to select. If you hover over np.arange or type np.arange() a floating tooltip pops up, reminding you of the function’s arguments and purpose. If you want more help, can scroll through the arguments tool tip with your mouse. VS Code will add matching opening (() and closing ()) parentheses for you. Type the arguments 1, 10 and hit return. np.arange(1,10) #&gt; array([1, 2, 3, 4, 5, 6, 7, 8, 9]) Type this code and notice you get similar assistance with the paired quotation marks: x = &quot;hello world&quot; Quotation marks and parentheses must always come in a pair. VS Code does its best to help you, but it’s still possible to mess up and end up with a mismatch. Now look at your Python interactive environment in VS Code in the top toolbar by selecting the icon circled in red : Here you can see all of the objects that you’ve created. 4.4 Exercises Why does this code not work? my_variable &lt;- 10 #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_variable&#39; is not defined #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; my_varıable #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_varıable&#39; is not defined #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Look carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.) Navigate to Visual Studio Code keyboard shortcuts by going to the menu under File &gt; Preferences &gt; Keyboard Shortcuts. (Code &gt; Preferences &gt; Keyboard Shortcuts on macOS). What do you see? You can read more about keybindings on the VS Code website. "],
["transform.html", "5 Data transformation 5.1 Introduction 5.2 Filter rows with query() 5.3 Arrange or sort rows with sort_values() 5.4 Select columns with filter() or loc[] 5.5 Add new variables with assign() 5.6 Grouped summaries or aggregations with agg() 5.7 Grouped transforms (and filters)", " 5 Data transformation 5.1 Introduction Visualization is an important tool for insight generation, but it is rare that you get the data in exactly the right form you need. Often you’ll need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. You’ll learn how to do all that (and more!) in this chapter, which will teach you how to transform your data using the pandas package and a new dataset on flights departing New York City in 2013. 5.1.1 Prerequisites In this chapter we’re going to focus on how to use the pandas package, the foundational package for data science in Python. We’ll illustrate the key ideas using data from the nycflights13 R package, and use Altair to help us understand the data. We will also need two additional Python packages to help us with mathematical and statistical functions - NumPy and SciPy. Notice the from ____ import ____ follows the SciPy guidance to import functions from submodule spaces. Now we will call functions using the SciPy package with the stats.&lt;FUNCTION&gt; structure. import pandas as pd import altair as alt import numpy as np from scipy import stats flights_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/flights/flights.csv&quot; flights = pd.read_csv(flights_url) flights[&#39;time_hour&#39;] = pd.to_datetime(flights.time_hour, format = &quot;%Y-%m-%d %H:%M:%S&quot;) 5.1.2 nycflights13 To explore the basic data manipulation verbs of pandas, we’ll use flights. This data frame contains all 336,776 flights that departed from New York City in 2013. The data comes from the US Bureau of Transportation Statistics, and is documented here. #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 2013 9 30 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 2013 9 30 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 2013 9 30 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 2013 9 30 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] You might notice that this data frame does not print in its entirety as other data frames you might have seen in the past: it only shows the first few and last few rows with only the columns that fit on one screen. (To see the whole dataset, you can open the variable view in your interactive Python window and double click on the flights object which will open the dataset in the VS Code data viewer). Using flights.dtypes will show you the variables types for each column. These describe the type of each variable: #&gt; year int64 #&gt; month int64 #&gt; day int64 #&gt; dep_time float64 #&gt; sched_dep_time int64 #&gt; dep_delay float64 #&gt; arr_time float64 #&gt; sched_arr_time int64 #&gt; arr_delay float64 #&gt; carrier object #&gt; flight int64 #&gt; tailnum object #&gt; origin object #&gt; dest object #&gt; air_time float64 #&gt; distance int64 #&gt; hour int64 #&gt; minute int64 #&gt; time_hour datetime64[ns, UTC] #&gt; dtype: object int64 stands for integers. float64 stands for doubles, or real numbers. object stands for character vectors, or strings. datetime64 stands for date-times (a date + a time) and dates. You can read more about pandas datetime tools There are three other common types of variables that aren’t used in this dataset but you’ll encounter later in the book: bool stands for logical, vectors that contain only True or False. category stands for factors, which pandas uses to represent categorical variables with fixed possible values. 5.1.3 pandas data manipulation basics In this chapter you are going to learn five key pandas functions or object methods. Object methods are things the objects can perform. For example, pandas data frames know how to tell you their shape, the pandas object knows how to concatenate two data frames together. The way we tell an object we want it to do something is with the ‘dot operator’. We will refer to these object operators as functions or methods. Below are the five methods that allow you to solve the vast majority of your data manipulation challenges: Pick observations by their values (query()). Reorder the rows (sort()). Pick variables by their names (select()). Create new variables with functions of existing variables (assign()). Collapse many values down to a single summary (groupby()). The pandas package can handle all of the same functionality of dplyr in R. You can read pandas mapping guide and this towards data science article to get more details on the following brief table. |R dplyr function | Python pandas function| |_________________|_______________________| |filter() | query() | |arrange() | sort_values() | |select() | filter() or loc[]| |rename () | rename() | |mutate() | assign() (see note)| |group_by () | groupby() | |summarise() | agg() | Note: The dpylr::mutate() function works similar to assign() in pandas on data frames. But you cannot use assign() on grouped data frame in pandas like you would use dplyr::mutate() on a grouped object. In that case you would use transform() and even then the functionality is not quite the same. The groupby() changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These functions provide the verbs for a language of data manipulation. All verbs work similarly: The first argument is a pandas dataFrame. The subsequent methods describe what to do with the data frame. The result is a new data frame. Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let’s dive in and see how these verbs work. 5.2 Filter rows with query() query() allows you to subset observations based on their values. The first argument specifies the rows to be selected. This argument can be label names or a boolean series. The second argument specifies the columns to be selected. The bolean filter on the rows is our focus. For example, we can select all flights on January 1st with: flights.query(&#39;month == 1 &amp; day == 1&#39;) #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; .. ... ... ... ... ... ... ... #&gt; 837 2013 1 1 ... 23 59 2013-01-02 04:00:00+00:00 #&gt; 838 2013 1 1 ... 16 30 2013-01-01 21:00:00+00:00 #&gt; 839 2013 1 1 ... 19 35 2013-01-02 00:00:00+00:00 #&gt; 840 2013 1 1 ... 15 0 2013-01-01 20:00:00+00:00 #&gt; 841 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; #&gt; [842 rows x 19 columns] The previous expression is equivalent to flights[(flights.month == 1) &amp; (flights.day == 1)] When you run that line of code, pandas executes the filtering operation and returns a new data frame. pandas functions usually don’t modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, =: jan1 = flights.query(&#39;month == 1 &amp; day == 1&#39;) Interactive Python either prints out the results, or saves them to a variable. 5.2.1 Comparisons To use filtering effectively, you have to know how to select the observations that you want using the comparison operators. Python provides the standard suite: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). When you’re starting out with Python, the easiest mistake to make is to use = instead of == when testing for equality. When this happens you’ll get an error: flights.query(&#39;month = 1&#39;) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: cannot assign without a target object #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/frame.py&quot;, line 3231, in query #&gt; res = self.eval(expr, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/frame.py&quot;, line 3346, in eval #&gt; return _eval(expr, inplace=inplace, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/eval.py&quot;, line 332, in eval #&gt; parsed_expr = Expr(expr, engine=engine, parser=parser, env=env) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 764, in __init__ #&gt; self.terms = self.parse() #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 781, in parse #&gt; return self._visitor.visit(self.expr) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 375, in visit #&gt; return visitor(node, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 381, in visit_Module #&gt; return self.visit(expr, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 375, in visit #&gt; return visitor(node, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 585, in visit_Assign #&gt; raise ValueError(&quot;cannot assign without a target object&quot;) There’s another common problem you might encounter when using ==: floating point numbers. The following result might surprise you! np.sqrt(2) ** 2 == 2 #&gt; False 1 / 49 * 49 == 1 #&gt; False Computers use finite precision arithmetic (they obviously can’t store an infinite number of digits!) so remember that every number you see is an approximation. Instead of relying on ==, use np.isclose(): np.isclose(np.sqrt(2) ** 2, 2) #&gt; True np.isclose(1 / 49 * 49, 1) #&gt; True 5.2.2 Logical operators Multiple arguments to query() are combined with “and”: every expression must be true in order for a row to be included in the output. For other types of combinations, you’ll need to use Boolean operators yourself: &amp; is “and”, | is “or”, and ! is “not”. Figure 5.1 shows the complete set of Boolean operations. Figure 5.1: Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. The following code finds all flights that departed in November or December: flights.query(&#39;month == 11 | month == 12&#39;) The order of operations doesn’t work like English. You can’t write filter(flights, month == (11 | 12)), which you might literally translate into “finds all flights that departed in November or December”. Instead it finds all months that equal 11 | 12, an expression that evaluates to True. In a numeric context (like here), True becomes one, so this finds all flights in January, not November or December. This is quite confusing! A useful short-hand for this problem is x in y. This will select every row where x is one of the values in y. We could use it to rewrite the code above: nov_dec = flights.query(&#39;month in [11, 12]&#39;) Sometimes you can simplify complicated subsetting by remembering De Morgan’s law: !(x &amp; y) is the same as !x | !y, and !(x | y) is the same as !x &amp; !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters: flights.query(&#39;arr_delay &gt; 120 | dep_delay &gt; 120&#39;) flights.query(&#39;arr_delay &lt;= 120 | dep_delay &lt;= 120&#39;) Whenever you start using complicated, multipart expressions in query(), consider making them explicit variables instead. That makes it much easier to check your work. You’ll learn how to create new variables shortly. 5.2.3 Missing values One important feature of pandas in Python that can make comparison tricky are missing values, or NAs (“not availables”). NA represents an unknown value so missing values are “contagious”: almost any operation involving an unknown value will also be unknown. np.nan + 10 #&gt; nan np.nan / 2 #&gt; nan The most confusing result are the comparisons. They always return a False. The logic for this result is explained on stackoverflow. The pandas missing data guide is a helpful read. np.nan &gt; 5 #&gt; False 10 == np.nan #&gt; False np.nan == np.nan #&gt; False It’s easiest to understand why this is true with a bit more context: # Let x be Mary&#39;s age. We don&#39;t know how old she is. x = np.nan # Let y be John&#39;s age. We don&#39;t know how old he is. y = np.nan # Are John and Mary the same age? x == y # Illogical comparisons are False. #&gt; False The Python development team did decide to provide functionality to find np.nan objects in your code by allowing np.nan != np.nan to return True. Once again you can read the rationale for this decision. Python now has isnan() functions to make this comparison more straight forward in your code. Pandas uses the nan structure in Python to identify NA or ‘missing’ values. If you want to determine if a value is missing, use pd.isna(): pd.isna(x) #&gt; True query() only includes rows where the condition is TRUE; it excludes both FALSE and NA values. df = pd.DataFrame({&#39;x&#39;: [1, np.nan, 3]}) df.query(&#39;x &gt; 1&#39;) #&gt; x #&gt; 2 3.0 If you want to preserve missing values, ask for them explicitly using the trick mentioned in the previous paragraph or by using pd.isna() with the symbolic reference @ in your condition: df.query(&#39;x != x | x &gt; 1&#39;) #&gt; x #&gt; 1 NaN #&gt; 2 3.0 df.query(&#39;@pd.isna(x) | x &gt; 1&#39;) #&gt; x #&gt; 1 NaN #&gt; 2 3.0 5.2.4 Exercises Find all flights that Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United, American, or Delta Departed in summer (July, August, and September) Arrived more than two hours late, but didn’t leave late Were delayed by at least an hour, but made up over 30 minutes in flight Departed between midnight and 6am (inclusive) How many flights have a missing dep_time? What other variables are missing? What might these rows represent? 5.3 Arrange or sort rows with sort_values() sort() works similarly to query() except that instead of selecting rows, it changes their order. It takes a data frame and a column name or a list of column names to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns: flights.sort_values(by = [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 111291 2013 12 31 ... 7 5 2013-12-31 12:00:00+00:00 #&gt; 111292 2013 12 31 ... 8 25 2013-12-31 13:00:00+00:00 #&gt; 111293 2013 12 31 ... 16 15 2013-12-31 21:00:00+00:00 #&gt; 111294 2013 12 31 ... 6 0 2013-12-31 11:00:00+00:00 #&gt; 111295 2013 12 31 ... 8 30 2013-12-31 13:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] Use the argument ascending = False to re-order by a column in descending order: flights.sort_values(by = [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;], ascending = False) #&gt; year month day ... hour minute time_hour #&gt; 110520 2013 12 31 ... 23 59 2014-01-01 04:00:00+00:00 #&gt; 110521 2013 12 31 ... 23 59 2014-01-01 04:00:00+00:00 #&gt; 110522 2013 12 31 ... 22 45 2014-01-01 03:00:00+00:00 #&gt; 110523 2013 12 31 ... 5 0 2013-12-31 10:00:00+00:00 #&gt; 110524 2013 12 31 ... 5 15 2013-12-31 10:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 837 2013 1 1 ... 23 59 2013-01-02 04:00:00+00:00 #&gt; 838 2013 1 1 ... 16 30 2013-01-01 21:00:00+00:00 #&gt; 839 2013 1 1 ... 19 35 2013-01-02 00:00:00+00:00 #&gt; 840 2013 1 1 ... 15 0 2013-01-01 20:00:00+00:00 #&gt; 841 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] Missing values are always sorted at the end: df = pd.DataFrame({&#39;x&#39;: [5, 2, np.nan]}) df.sort_values(&#39;x&#39;) #&gt; x #&gt; 1 2.0 #&gt; 0 5.0 #&gt; 2 NaN df.sort_values(&#39;x&#39;, ascending = False) #&gt; x #&gt; 0 5.0 #&gt; 1 2.0 #&gt; 2 NaN 5.3.1 Exercises How could you use sort() to sort all missing values to the start? (Hint: use isna()). Sort flights to find the most delayed flights. Find the flights that left earliest. Sort flights to find the fastest (highest speed) flights. Which flights travelled the farthest? Which travelled the shortest? 5.4 Select columns with filter() or loc[] It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. fliter() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. filter() is not terribly useful with the flights data because we only have 19 variables, but you can still get the general idea: # Select columns by name flights.filter([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) # Select all columns except year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] flights.drop(columns = [&#39;year&#39;, &#39;day&#39;]) #&gt; month dep_time sched_dep_time ... hour minute time_hour #&gt; 0 1 517.0 515 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 1 533.0 529 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 1 542.0 540 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 1 544.0 545 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 1 554.0 600 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 9 NaN 1455 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 9 NaN 2200 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 9 NaN 1210 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 9 NaN 1159 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 9 NaN 840 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 17 columns] loc[] functions in a similar fashion. # Select columns by name flights.loc[:, [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]] # Select all columns between year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] flights.loc[:, &#39;year&#39;:&#39;day&#39;] # Select all columns except year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] There are a number of helper regular expressions you can use within filter(): flights.filter(regex = '^sch'): matches column names that begin with “sch”. flights.filter(regex = \"time$\"): matches names that end with “time”. flights.filter(regex = \"_dep_\"): matches names that contain “dep”. flights.filter(regex = '(.)\\\\1'): selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in [strings]. See pandas filter documentation for more details. Use rename() to rename a column or multiple columns. flights.rename(columns = {&#39;year&#39;: &#39;YEAR&#39;, &#39;month&#39;:&#39;MONTH&#39;}) #&gt; YEAR MONTH day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 2013 9 30 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 2013 9 30 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 2013 9 30 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 2013 9 30 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] 5.4.1 Exercises Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. What happens if you include the name of a variable multiple times in a filter() call? Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default? flights.filter(regex = &quot;TIME&quot;) 5.5 Add new variables with assign() Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. That’s the job of assign(). assign() always adds new columns at the end of your dataset so we’ll start by creating a narrower dataset so we can see the new variables. flights_sml = (flights. filter(regex = &quot;^year$|^month$|^day$|delay$|^distance$|^air_time$&quot;) ) flights_sml.assign( gain = lambda x: x.dep_delay - x.arr_delay, speed = lambda x: x.distance / x.air_time * 60 ).head() #&gt; year month day dep_delay arr_delay air_time distance gain speed #&gt; 0 2013 1 1 2.0 11.0 227.0 1400 -9.0 370.044053 #&gt; 1 2013 1 1 4.0 20.0 227.0 1416 -16.0 374.273128 #&gt; 2 2013 1 1 2.0 33.0 160.0 1089 -31.0 408.375000 #&gt; 3 2013 1 1 -1.0 -18.0 183.0 1576 17.0 516.721311 #&gt; 4 2013 1 1 -6.0 -25.0 116.0 762 19.0 394.137931 Note that you can refer to columns that you’ve just created: flights_sml.assign( gain = lambda x: x.dep_delay - x.arr_delay, hours = lambda x: x.air_time / 60, gain_per_hour = lambda x: x.gain / x.hours ).head() #&gt; year month day dep_delay ... distance gain hours gain_per_hour #&gt; 0 2013 1 1 2.0 ... 1400 -9.0 3.783333 -2.378855 #&gt; 1 2013 1 1 4.0 ... 1416 -16.0 3.783333 -4.229075 #&gt; 2 2013 1 1 2.0 ... 1089 -31.0 2.666667 -11.625000 #&gt; 3 2013 1 1 -1.0 ... 1576 17.0 3.050000 5.573770 #&gt; 4 2013 1 1 -6.0 ... 762 19.0 1.933333 9.827586 #&gt; #&gt; [5 rows x 10 columns] 5.5.1 Useful creation functions There are many functions for creating new variables that you can use with assign(). The key property is that the function must be vectorised: it must take a vector of values as input, return a vector with the same number of values as output. Some arithmetic operators are available in Python without the need for any additional packages. However, many arithmetic functions like mean() and std() are accessed through importing additional packages. Python comes with a math and statistics package. However, we recommend the NumPy package for accessing the suite of mathematical functions needed. You would import NumPy with import numpy as np. There’s no way to list every possible function that you might use, but here’s a selection of functions that are frequently useful: Arithmetic operators: +, -, *, /, ^. These are all vectorised, using the so called “recycling rules”. If one parameter is shorter than the other, it will be automatically extended to be the same length. This is most useful when one of the arguments is a single number: air_time / 60, hours * 60 + minute, etc. Arithmetic operators are also useful in conjunction with the aggregate functions you’ll learn about later. For example, x / np.sum(x) calculates the proportion of a total, and y - np.mean(y) computes the difference from the mean. Modular arithmetic: // (integer division) and % (remainder), where x == y * (x // y) + (x % y). Modular arithmetic is a handy tool because it allows you to break integers up into pieces. For example, in the flights dataset, you can compute hour and minute from dep_time with: (flights. filter([&#39;dep_time&#39;]). assign( hour = lambda x: x.dep_time // 100, minute = lambda x: x.dep_time % 100) ) #&gt; dep_time hour minute #&gt; 0 517.0 5.0 17.0 #&gt; 1 533.0 5.0 33.0 #&gt; 2 542.0 5.0 42.0 #&gt; 3 544.0 5.0 44.0 #&gt; 4 554.0 5.0 54.0 #&gt; ... ... ... ... #&gt; 336771 NaN NaN NaN #&gt; 336772 NaN NaN NaN #&gt; 336773 NaN NaN NaN #&gt; 336774 NaN NaN NaN #&gt; 336775 NaN NaN NaN #&gt; #&gt; [336776 rows x 3 columns] Logs: np.log(), np.log2(), np.log10(). Logarithms are an incredibly useful transformation for dealing with data that ranges across multiple orders of magnitude. They also convert multiplicative relationships to additive, a feature we’ll come back to in modelling. All else being equal, I recommend using np.log2() because it’s easy to interpret: a difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving. Offsets: shift(1) and shift(-1) allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - x.shift(1)) or find when values change (x != x.shift(1)). They are most useful in conjunction with groupby(), which you’ll learn about shortly. x = pd.Series(np.arange(1,10)) x.shift(1) #&gt; 0 NaN #&gt; 1 1.0 #&gt; 2 2.0 #&gt; 3 3.0 #&gt; 4 4.0 #&gt; 5 5.0 #&gt; 6 6.0 #&gt; 7 7.0 #&gt; 8 8.0 #&gt; dtype: float64 x.shift(-1) #&gt; 0 2.0 #&gt; 1 3.0 #&gt; 2 4.0 #&gt; 3 5.0 #&gt; 4 6.0 #&gt; 5 7.0 #&gt; 6 8.0 #&gt; 7 9.0 #&gt; 8 NaN #&gt; dtype: float64 Cumulative and rolling aggregates: pandas provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(). If you need rolling aggregates (i.e. a sum computed over a rolling window), try the rolling() in the pandas package. x #&gt; 0 1 #&gt; 1 2 #&gt; 2 3 #&gt; 3 4 #&gt; 4 5 #&gt; 5 6 #&gt; 6 7 #&gt; 7 8 #&gt; 8 9 #&gt; dtype: int64 x.cumsum() #&gt; 0 1 #&gt; 1 3 #&gt; 2 6 #&gt; 3 10 #&gt; 4 15 #&gt; 5 21 #&gt; 6 28 #&gt; 7 36 #&gt; 8 45 #&gt; dtype: int64 x.rolling(2).mean() #&gt; 0 NaN #&gt; 1 1.5 #&gt; 2 2.5 #&gt; 3 3.5 #&gt; 4 4.5 #&gt; 5 5.5 #&gt; 6 6.5 #&gt; 7 7.5 #&gt; 8 8.5 #&gt; dtype: float64 Logical comparisons, &lt;, &lt;=, &gt;, &gt;=, !=, and ==, which you learned about earlier. If you’re doing a complex sequence of logical operations it’s often a good idea to store the interim values in new variables so you can check that each step is working as expected. Ranking: there are a number of ranking functions, but you should start with min_rank(). It does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small ranks; use desc(x) to give the largest values the smallest ranks. y = pd.Series([1, 2, 2, np.nan, 3, 4]) y.rank(method = &#39;min&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 2.0 #&gt; 3 NaN #&gt; 4 4.0 #&gt; 5 5.0 #&gt; dtype: float64 y.rank(ascending=False, method = &#39;min&#39;) #&gt; 0 5.0 #&gt; 1 3.0 #&gt; 2 3.0 #&gt; 3 NaN #&gt; 4 2.0 #&gt; 5 1.0 #&gt; dtype: float64 If method = 'min'`` doesn't do what you need, look at the variantsmethod = ‘first’,method = ‘dense’,method = ‘percent’,pct = True`. See the rank help page for more details. y.rank(method = &#39;first&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 3.0 #&gt; 3 NaN #&gt; 4 4.0 #&gt; 5 5.0 #&gt; dtype: float64 y.rank(method = &#39;dense&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 2.0 #&gt; 3 NaN #&gt; 4 3.0 #&gt; 5 4.0 #&gt; dtype: float64 y.rank(pct = True) #&gt; 0 0.2 #&gt; 1 0.5 #&gt; 2 0.5 #&gt; 3 NaN #&gt; 4 0.8 #&gt; 5 1.0 #&gt; dtype: float64 5.5.2 Exercises Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for method = 'min'. What trigonometric functions does NumPy provide? 5.6 Grouped summaries or aggregations with agg() The last key verb is agg(). It collapses a data frame to a single row: flights.agg({&#39;dep_delay&#39;: np.mean}) #&gt; dep_delay 12.63907 #&gt; dtype: float64 (Pandas aggregate functions ignores the np.nan values like na.rm = TRUE in R.) agg() is not terribly useful unless we pair it with groupby(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the pandas functions on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied similiar code to a data frame grouped by date, we get the average delay per date. Note that with the groupby() function we used tuple to identify the column (first entry) and the function to apply on the column (second entry). This is called named aggregation in pandas: by_day = flights.groupby([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) by_day.agg(delay = (&#39;dep_delay&#39;, np.mean)).reset_index() #&gt; year month day delay #&gt; 0 2013 1 1 11.548926 #&gt; 1 2013 1 2 13.858824 #&gt; 2 2013 1 3 10.987832 #&gt; 3 2013 1 4 8.951595 #&gt; 4 2013 1 5 5.732218 #&gt; .. ... ... ... ... #&gt; 360 2013 12 27 10.937630 #&gt; 361 2013 12 28 7.981550 #&gt; 362 2013 12 29 22.309551 #&gt; 363 2013 12 30 10.698113 #&gt; 364 2013 12 31 6.996053 #&gt; #&gt; [365 rows x 4 columns] Note the use of reset_index() to remove pandas creation of a MultiIndex. You can read more about the use of grouby in pandas with their Group By: split-apply-combine user Guid documentation Together groupby() and agg() provide one of the tools that you’ll use most commonly when working with pandas: grouped summaries. But before we go any further with this, we need to introduce a structure for pandas code when doing data science work. We structure our code much like ‘the pipe’, %&gt;% in the tidyverse packages from R-Studio. 5.6.1 Combining multiple operations Imagine that we want to explore the relationship between the distance and average delay for each location. Using what you know about pandas, you might write code like this: by_dest = flights.groupby(&#39;dest&#39;) delay = by_dest.agg( count = (&#39;distance&#39;, &#39;size&#39;), dist = (&#39;distance&#39;, np.mean), delay = (&#39;arr_delay&#39;, np.mean) ) delay = delay.query(&#39;count &gt; 20 &amp; dest != &quot;HNL&quot;&#39;) # It looks like delays increase with distance up to ~750 miles # and then decrease. Maybe as flights get longer there&#39;s more # ability to make up delays in the air? chart_base = (alt.Chart(delay). encode( x = &#39;dist&#39;, y = &#39;delay&#39; )) chart = chart_base.mark_point() + chart_base.transform_loess(&#39;dist&#39;, &#39;delay&#39;).mark_line() There are three steps to prepare this data: Group flights by destination. Summarise to compute distance, average delay, and number of flights. Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport. This code is a little frustrating to write because we have to give each intermediate data frame a name, even though we don’t care about it. Naming things is hard, so this slows down our analysis. There’s another way to tackle the same problem without the additional objects: delays = (flights. groupby(&#39;dest&#39;). agg( count = (&#39;distance&#39;, &#39;size&#39;), dist = (&#39;distance&#39;, np.mean), delay = (&#39;arr_delay&#39;, np.mean) ). query(&#39;count &gt; 20 &amp; dest != &quot;HNL&quot;&#39;)) This focuses on the transformations, not what’s being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarise, then filter. As suggested by this reading, a good way to pronounce . when reading pandas code is “then”. You can use the () with . to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. We’ll use this format frequently from now on because it considerably improves the readability of complex pandas code. 5.6.2 Missing values You may have wondered about the np.nan values we put into our pandas data frame above. Pandas just started an experimental options (version 1.0) for pd.NA but it is not standard as in the R language. You can read the full details about missing data in pandas. Pandas’ and NumPy’s handling of missing values defaults to the opposite functionality of R and the Tidyverse. Here are three key defaults when using Pandas. When summing data, NA (missing) values will be treated as zero. If the data are all NA, the result will be 0. Cumulative methods ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include missing values, use skipna=False. All the .groupby() methods exclude missing values in their calculations as described in the pandas groupby documentation. In our case, where missing values represent cancelled flights, we could also tackle the problem by first removing the cancelled flights. We’ll save this dataset so we can reuse it in the next few examples. not_cancelled = flights.dropna(subset = [&#39;dep_delay&#39;, &#39;arr_delay&#39;]) 5.6.3 Counts Whenever you do any aggregation, it’s always a good idea to include either a count (size()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data. For example, let’s look at the planes (identified by their tail number) that have the highest average delays: delays = not_cancelled.groupby(&#39;tailnum&#39;).agg( delay = (&quot;arr_delay&quot;, np.mean) ) chart = (alt.Chart(delays). transform_density( density = &#39;delay&#39;, as_ = [&#39;delay&#39;, &#39;density&#39;], bandwidth=10 ). encode( x = &#39;delay:Q&#39;, y = &#39;density:Q&#39; ). mark_line() ) Wow, there are some planes that have an average delay of 5 hours (300 minutes)! The story is actually a little more nuanced. We can get more insight if we draw a scatterplot of number of flights vs. average delay: delays = not_cancelled.groupby(&#39;tailnum&#39;).agg( delay = (&quot;arr_delay&quot;, np.mean), n = (&#39;arr_delay&#39;, &#39;size&#39;) ) chart = (alt.Chart(delays). encode( x = &#39;n&#39;, y = &#39;delay&#39; ). mark_point( filled = True, opacity = 1/10) ) Not surprisingly, there is much greater variation in the average delay when there are few flights. The shape of this plot is very characteristic: whenever you plot a mean (or other summary) vs. group size, you’ll see that the variation decreases as the sample size increases. When looking at this sort of plot, it’s often useful to filter out the groups with the smallest numbers of observations, so you can see more of the pattern and less of the extreme variation in the smallest groups. This is what the following code does, as well as showing you a handy pattern for simple data frame manipulations only needed for a chart. chart = (alt.Chart(delays.query(&quot;n &gt; 25&quot;)). encode( x = &#39;n&#39;, y = &#39;delay&#39; ). mark_point( filled = True, opacity = 1/10) ) chart.save(&quot;screenshots/altair_delays.png&quot;) There’s another common variation of this type of pattern. Let’s look at how the average performance of batters in baseball is related to the number of times they’re at bat. Here I use data from the Lahman package to compute the batting average (number of hits / number of attempts) of every major league baseball player. When I plot the skill of the batter (measured by the batting average, ba) against the number of opportunities to hit the ball (measured by at bat, ab), you see two patterns: As above, the variation in our aggregate decreases as we get more data points. There’s a positive correlation between skill (ba) and opportunities to hit the ball (ab). This is because teams control who gets to play, and obviously they’ll pick their best players. # settings for Altair to handle large data alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) batting_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/batting/batting.csv&quot; batting = pd.read_csv(batting_url) batters = (batting. groupby(&#39;playerID&#39;). agg( ab = (&quot;AB&quot;, &quot;sum&quot;), h = (&quot;H&quot;, &quot;sum&quot;) ). assign( ba = lambda x: x.h/x.ab ) ) chart = (alt.Chart(batters.query(&#39;ab &gt; 100&#39;)). encode( x = &#39;ab&#39;, y = &#39;ba&#39; ). mark_point() ) chart.save(&quot;screenshots/altair_batters.png&quot;) This also has important implications for ranking. If you naively sort on desc(ba), the people with the best batting averages are clearly lucky, not skilled: batters.sort_values(&#39;ba&#39;, ascending = False).head(10) #&gt; ab h ba #&gt; playerID #&gt; egeco01 1 1 1.0 #&gt; simspe01 1 1 1.0 #&gt; paciojo01 3 3 1.0 #&gt; bruneju01 1 1 1.0 #&gt; liddeda01 1 1 1.0 #&gt; garcimi02 1 1 1.0 #&gt; meehabi01 1 1 1.0 #&gt; rodried01 1 1 1.0 #&gt; hopkimi01 2 2 1.0 #&gt; gallaja01 1 1 1.0 You can find a good explanation of this problem at http://varianceexplained.org/r/empirical_bayes_baseball/ and http://www.evanmiller.org/how-not-to-sort-by-average-rating.html. 5.6.4 Useful summary functions Just using means, counts, and sum can get you a long way, but NumPy, SciPy, and Pandas provide many other useful summary functions (remember we are using the SciPy stats submodule): Measures of location: we’ve used np.mean(), but np.median() is also useful. The mean is the sum divided by the length; the median is a value where 50% of x is above it, and 50% is below it. It’s sometimes useful to combine aggregation with logical subsetting. We haven’t talked about this sort of subsetting yet, but you’ll learn more about it in subsetting. (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]). agg( avg_delay1 = (&#39;arr_delay&#39;, np.mean), avg_delay2 = (&#39;arr_delay&#39;, lambda x: np.mean(x[x &gt; 0])) ) ) #&gt; avg_delay1 avg_delay2 #&gt; year month day #&gt; 2013 1 1 12.651023 32.481562 #&gt; 2 12.692888 32.029907 #&gt; 3 5.733333 27.660870 #&gt; 4 -1.932819 28.309764 #&gt; 5 -1.525802 22.558824 #&gt; ... ... ... #&gt; 12 27 -0.148803 29.046832 #&gt; 28 -3.259533 25.607692 #&gt; 29 18.763825 47.256356 #&gt; 30 10.057712 31.243802 #&gt; 31 6.212121 24.455959 #&gt; #&gt; [365 rows x 2 columns] Measures of spread: np.sd(), stats.iqr(), stats.median_absolute_deviation(). The root mean squared deviation, or standard deviation np.sd(), is the standard measure of spread. The interquartile range stats.iqr() and median absolute deviation stats.median_absolute_deviation() are robust equivalents that may be more useful if you have outliers. # Why is distance to some destinations more variable than to others? (not_cancelled. groupby([&#39;dest&#39;]). agg(distance_sd = (&#39;distance&#39;, np.std)). sort_values(&#39;distance_sd&#39;, ascending = False) ) #&gt; distance_sd #&gt; dest #&gt; EGE 10.542765 #&gt; SAN 10.350094 #&gt; SFO 10.216017 #&gt; HNL 10.004197 #&gt; SEA 9.977993 #&gt; ... ... #&gt; BZN 0.000000 #&gt; BUR 0.000000 #&gt; PSE 0.000000 #&gt; ABQ 0.000000 #&gt; LEX NaN #&gt; #&gt; [104 rows x 1 columns] Measures of rank: np.min(), np.quantile(), np.max(). Quantiles are a generalisation of the median. For example, np.quantile(x, 0.25) will find a value of x that is greater than 25% of the values, and less than the remaining 75%. # When do the first and last flights leave each day? (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]). agg( first = (&#39;dep_time&#39;, np.min), last = (&#39;dep_time&#39;, np.max) ) ) #&gt; first last #&gt; year month day #&gt; 2013 1 1 517.0 2356.0 #&gt; 2 42.0 2354.0 #&gt; 3 32.0 2349.0 #&gt; 4 25.0 2358.0 #&gt; 5 14.0 2357.0 #&gt; ... ... ... #&gt; 12 27 2.0 2351.0 #&gt; 28 7.0 2358.0 #&gt; 29 3.0 2400.0 #&gt; 30 1.0 2356.0 #&gt; 31 13.0 2356.0 #&gt; #&gt; [365 rows x 2 columns] Measures of position: first(), nth(), last(). These work similarly to x[1], x[2], and x[size(x)] but let you set a default value if that position does not exist (i.e. you’re trying to get the 3rd element from a group that only has two elements). For example, we can find the first and last departure for each day: # using first and last (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( first_dep = (&#39;dep_time&#39;, &#39;first&#39;), last_dep = (&#39;dep_time&#39;, &#39;last&#39;) ) ) #&gt; first_dep last_dep #&gt; year month day #&gt; 2013 1 1 517.0 2356.0 #&gt; 2 42.0 2354.0 #&gt; 3 32.0 2349.0 #&gt; 4 25.0 2358.0 #&gt; 5 14.0 2357.0 #&gt; ... ... ... #&gt; 12 27 2.0 2351.0 #&gt; 28 7.0 2358.0 #&gt; 29 3.0 2400.0 #&gt; 30 1.0 2356.0 #&gt; 31 13.0 2356.0 #&gt; #&gt; [365 rows x 2 columns] # using position (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( first_dep = (&#39;dep_time&#39;, lambda x: list(x)[1]), last_dep = (&#39;dep_time&#39;, lambda x: list(x)[-1]) ) ) #&gt; first_dep last_dep #&gt; year month day #&gt; 2013 1 1 533.0 2356.0 #&gt; 2 126.0 2354.0 #&gt; 3 50.0 2349.0 #&gt; 4 106.0 2358.0 #&gt; 5 37.0 2357.0 #&gt; ... ... ... #&gt; 12 27 8.0 2351.0 #&gt; 28 56.0 2358.0 #&gt; 29 39.0 2400.0 #&gt; 30 23.0 2356.0 #&gt; 31 18.0 2356.0 #&gt; #&gt; [365 rows x 2 columns] Counts: You’ve seen size(), which takes no arguments, and returns the size of the current group. To count the number of non-missing values, use isnull().sum(). To count the number of unique (distinct) values, use nunique(). # Which destinations have the most carriers? (flights. groupby(&#39;dest&#39;). agg( carriers_unique = (&#39;carrier&#39;, &#39;nunique&#39;), carriers_count = (&#39;carrier&#39;, &#39;size&#39;), missing_time = (&#39;dep_time&#39;, lambda x: x.isnull().sum()) ) ) #&gt; carriers_unique carriers_count missing_time #&gt; dest #&gt; ABQ 1 254 0.0 #&gt; ACK 1 265 0.0 #&gt; ALB 1 439 20.0 #&gt; ANC 1 8 0.0 #&gt; ATL 7 17215 317.0 #&gt; ... ... ... ... #&gt; TPA 7 7466 59.0 #&gt; TUL 1 315 16.0 #&gt; TVC 2 101 5.0 #&gt; TYS 2 631 52.0 #&gt; XNA 2 1036 25.0 #&gt; #&gt; [105 rows x 3 columns] Counts are useful and pandas provides a simple helper if all you want is a count: not_cancelled[&#39;dest&#39;].value_counts() #&gt; ATL 16837 #&gt; ORD 16566 #&gt; LAX 16026 #&gt; BOS 15022 #&gt; MCO 13967 #&gt; ... #&gt; HDN 14 #&gt; MTJ 14 #&gt; SBN 10 #&gt; ANC 8 #&gt; LEX 1 #&gt; Name: dest, Length: 104, dtype: int64 Counts and proportions of logical values: sum(x &gt; 10), mean(y == 0). When used with numeric functions, TRUE is converted to 1 and FALSE to 0. This makes sum() and mean() very useful: sum(x) gives the number of TRUEs in x, and mean(x) gives the proportion. # How many flights left before 5am? (these usually indicate delayed # flights from the previous day) (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( n_early = (&#39;dep_time&#39;, lambda x: np.sum(x &lt; 500)) ) ) # What proportion of flights are delayed by more than an hour? #&gt; n_early #&gt; year month day #&gt; 2013 1 1 0.0 #&gt; 2 3.0 #&gt; 3 4.0 #&gt; 4 3.0 #&gt; 5 3.0 #&gt; ... ... #&gt; 12 27 7.0 #&gt; 28 2.0 #&gt; 29 3.0 #&gt; 30 6.0 #&gt; 31 4.0 #&gt; #&gt; [365 rows x 1 columns] (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( hour_prop = (&#39;arr_delay&#39;, lambda x: np.sum(x &gt; 60)) ) ) #&gt; hour_prop #&gt; year month day #&gt; 2013 1 1 60.0 #&gt; 2 79.0 #&gt; 3 51.0 #&gt; 4 36.0 #&gt; 5 25.0 #&gt; ... ... #&gt; 12 27 51.0 #&gt; 28 31.0 #&gt; 29 129.0 #&gt; 30 69.0 #&gt; 31 33.0 #&gt; #&gt; [365 rows x 1 columns] 5.6.5 Grouping by multiple variables Be careful when progressively rolling up summaries: it’s OK for sums and counts, but you need to think about weighting means and variances, and it’s not possible to do it exactly for rank-based statistics like the median. In other words, the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median. 5.6.6 Ungrouping (reseting the index) If you need to remove grouping and MultiIndex use reset.index(). This is a rough equivalent to ungroup() in R but it is not the same thing. Notice the column names are no longer in multiple levels. dat = (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( hour_prop = (&#39;arr_delay&#39;, lambda x: np.sum(x &gt; 60)) ) ) dat.head() #&gt; hour_prop #&gt; year month day #&gt; 2013 1 1 60.0 #&gt; 2 79.0 #&gt; 3 51.0 #&gt; 4 36.0 #&gt; 5 25.0 dat.reset_index().head() #&gt; year month day hour_prop #&gt; 0 2013 1 1 60.0 #&gt; 1 2013 1 2 79.0 #&gt; 2 2013 1 3 51.0 #&gt; 3 2013 1 4 36.0 #&gt; 4 2013 1 5 25.0 5.6.7 Exercises Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios: A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time. A flight is always 10 minutes late. A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time. 99% of the time a flight is on time. 1% of the time it’s 2 hours late. Which is more important: arrival delay or departure delay? Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column? Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay? Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights.groupby(['carrier', 'dest']).agg(n = ('dep_time', 'size'))) 5.7 Grouped transforms (and filters) Grouping is most useful in conjunction with agg(), but you can also do convenient operations with transform(). This is a difference in pandas as compared to dplyr. Once you create a .groupby() object you cannot use assign() and the best equivalent is transform(). Following pandas groupby guide on ‘split-apply-combine’, we would assign our transfomred variables to our data frame and then perform filters on the full data frame. Find the worst members of each group: flights_sml[&#39;ranks&#39;] = (flights_sml. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). arr_delay.rank(ascending = False)) #&gt; /usr/local/bin/python3:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy flights_sml.query(&#39;ranks &lt; 10&#39;).drop(columns = &#39;ranks&#39;) #&gt; year month day dep_delay arr_delay air_time distance #&gt; 151 2013 1 1 853.0 851.0 41.0 184 #&gt; 649 2013 1 1 290.0 338.0 213.0 1134 #&gt; 673 2013 1 1 260.0 263.0 46.0 266 #&gt; 729 2013 1 1 157.0 174.0 60.0 213 #&gt; 746 2013 1 1 216.0 222.0 121.0 708 #&gt; ... ... ... ... ... ... ... ... #&gt; 336579 2013 9 30 158.0 121.0 95.0 765 #&gt; 336668 2013 9 30 182.0 174.0 95.0 708 #&gt; 336724 2013 9 30 158.0 136.0 91.0 746 #&gt; 336757 2013 9 30 194.0 194.0 50.0 301 #&gt; 336763 2013 9 30 154.0 130.0 123.0 944 #&gt; #&gt; [3306 rows x 7 columns] Find all groups bigger than a threshold: popular_dests = flights popular_dests[&#39;n&#39;] = popular_dests.groupby(&#39;dest&#39;).arr_delay.transform(&#39;size&#39;) popular_dests = flights.query(&#39;n &gt; 365&#39;).drop(columns = &#39;n&#39;) popular_dests #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 2013 9 30 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 2013 9 30 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 2013 9 30 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 2013 9 30 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [332577 rows x 19 columns] Standardise to compute per group metrics: (popular_dests. query(&#39;arr_delay &gt; 0&#39;). assign(prop_delay = lambda x: x.arr_delay / x.groupby(&#39;dest&#39;).arr_delay.transform(&#39;sum&#39;)). filter([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;, &#39;dest&#39;, &#39;arr_delay&#39;, &#39;prop_delay&#39;]) ) #&gt; year month day dest arr_delay prop_delay #&gt; 0 2013 1 1 IAH 11.0 0.000111 #&gt; 1 2013 1 1 IAH 20.0 0.000201 #&gt; 2 2013 1 1 MIA 33.0 0.000235 #&gt; 5 2013 1 1 ORD 12.0 0.000042 #&gt; 6 2013 1 1 FLL 19.0 0.000094 #&gt; ... ... ... ... ... ... ... #&gt; 336759 2013 9 30 BNA 7.0 0.000057 #&gt; 336760 2013 9 30 STL 57.0 0.000717 #&gt; 336762 2013 9 30 SFO 42.0 0.000204 #&gt; 336763 2013 9 30 MCO 130.0 0.000631 #&gt; 336768 2013 9 30 BOS 1.0 0.000005 #&gt; #&gt; [131106 rows x 6 columns] 5.7.1 Exercises Which plane (tailnum) has the worst on-time record? What time of day should you fly if you want to avoid delays as much as possible? For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination. Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Explore how the delay of a flight is related to the delay of the immediately preceding flight. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air? Find all destinations that are flown by at least two carriers. Use that information to rank the carriers. For each plane, count the number of flights before the first delay of greater than 1 hour. "],
["workflow-scripts.html", "6 Workflow: scripts 6.1 Running code", " 6 Workflow: scripts So far you’ve been using the console to run code. That’s a great place to start, but you’ll find it gets cramped pretty quickly as you create more complex ggplot2 graphics and dplyr pipes. To give yourself more room to work, it’s a great idea to use the script editor. Open it up either by clicking the File menu, and selecting New File or using the keyboard shortcut Cmd/Ctrl + N. Now you’ll see one pane with a tab named ‘Untitled-1’. After saving the file as *.py you can start a new coding cell by typing # %% in your script which will prompt VS Code to give you an interactive Python framework. You will see options to ‘Run Cell’, ‘Run Below’, and ‘Debug cell’ just above the text you typed. Clicking the ‘Run Cell’ will open the Python Interactive console in a side panel. : The script editor is a great place to put code you care about. Keep experimenting in the console, but once you have written code that works and does what you want, put it in the script editor. VS Code does not automatically save the contents of the editor by default. However, you can turn on autosave in the settings and when you quit VS Code it will save and automatically load it when you re-open. Nevertheless, it’s a good idea to save your scripts regularly and to back them up. 6.1 Running code The script editor is also a great place to build up complex Altair charts or long sequences of pandas manipulations. The key to using the script editor effectively is to memorise one of the most important keyboard shortcuts: shift + Enter. This executes the current cell from your Python script in the console. I recommend that you always start your script with the packages that you need. That way, if you share your code with others, they can easily see what packages they need to install. Note, however, that you should never include os.chdir() in a script that you share. It’s very antisocial to change settings on someone else’s computer! When working through future chapters, I highly recommend starting in the editor and practicing your keyboard shortcuts. Over time, sending code to the console in this way will become so natural that you won’t even think about it. "],
["exploratory-data-analysis.html", "7 Exploratory Data Analysis 7.1 Introduction 7.2 Questions 7.3 Variation 7.4 Missing values 7.5 Covariation 7.6 Patterns and models 7.7 Altair calls 7.8 Learning more", " 7 Exploratory Data Analysis 7.1 Introduction This chapter will show you how to use visualisation and transformation to explore your data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short. EDA is an iterative cycle. You: Generate questions about your data. Search for answers by visualising, transforming, and modelling your data. Use what you learn to refine your questions and/or generate new questions. EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others. EDA is an important part of any data analysis, even if the questions are handed to you on a platter, because you always need to investigate the quality of your data. Data cleaning is just one application of EDA: you ask questions about whether your data meets your expectations or not. To do data cleaning, you’ll need to deploy all the tools of EDA: visualisation, transformation, and modelling. 7.1.1 Prerequisites In this chapter we’ll combine what you’ve learned about pandas and Altair to interactively ask questions, answer them with data, and then ask new questions. import pandas as pd import altair as alt import numpy as np from scipy import stats alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) Then make sure you have the diamonds data loaded. diamonds = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot;) diamonds[&#39;cut&#39;] = pd.Categorical(diamonds.cut, ordered = True, categories = [&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot; ]) diamonds[&#39;color&#39;] = pd.Categorical(diamonds.color, ordered = True, categories = [&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;]) diamonds[&#39;clarity&#39;] = pd.Categorical(diamonds.clarity, ordered = True, categories = [&quot;I1&quot;, &quot;SI2&quot;, &quot;SI1&quot;, &quot;VS2&quot;, &quot;VS1&quot;, &quot;VVS2&quot;, &quot;VVS1&quot;, &quot;IF&quot;]) 7.2 Questions “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make. EDA is fundamentally a creative process. And like most creative processes, the key to asking quality questions is to generate a large quantity of questions. It is difficult to ask revealing questions at the start of your analysis because you do not know what insights are contained in your dataset. On the other hand, each new question that you ask will expose you to a new aspect of your data and increase your chance of making a discovery. You can quickly drill down into the most interesting parts of your data—and develop a set of thought-provoking questions—if you follow up each question with a new question based on what you find. There is no rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data. You can loosely word these questions as: What type of variation occurs within my variables? What type of covariation occurs between my variables? The rest of this chapter will look at these two questions. I’ll explain what variation and covariation are, and I’ll show you several ways to answer each question. To make the discussion easier, let’s define some terms: A variable is a quantity, quality, or property that you can measure. A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement. An observation is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable. I’ll sometimes refer to an observation as a data point. Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row. So far, all of the data that you’ve seen has been tidy. In real-life, most data isn’t tidy, so we’ll come back to these ideas again in [tidy data]. 7.3 Variation Variation is the tendency of the values of a variable to change from measurement to measurement. You can see variation easily in real life; if you measure any continuous variable twice, you will get two different results. This is true even if you measure quantities that are constant, like the speed of light. Each of your measurements will include a small amount of error that varies from measurement to measurement. Categorical variables can also vary if you measure across different subjects (e.g. the eye colors of different people), or different times (e.g. the energy levels of an electron at different moments). Every variable has its own pattern of variation, which can reveal interesting information. The best way to understand that pattern is to visualise the distribution of the variable’s values. 7.3.1 Visualising distributions How you visualise the distribution of a variable will depend on whether the variable is categorical or continuous. A variable is categorical if it can only take one of a small set of values. In R, categorical variables are usually saved as factors or character vectors. To examine the distribution of a categorical variable, use a bar chart: We want the height of the bars to display how many observations occurred with each x value. You can compute these values with size(): chart_dat = (diamonds. groupby(&#39;cut&#39;). agg(count = (&#39;carat&#39;, &#39;size&#39;)). reset_index()) chart = (alt.Chart(chart_dat). encode( x = &#39;cut&#39;, y = &#39;count&#39; ). mark_bar(). properties(width = 400) ) chart.save(&quot;screenshots/altair_diamonds_barchart.png&quot;) A variable is continuous if it can take any of an infinite set of ordered values. Numbers and date-times are two examples of continuous variables. To examine the distribution of a continuous variable, use a histogram: chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;carat&#39;, bin = alt.Bin(step = 0.5)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_hist_bin.png&quot;) A histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that almost 30,000 observations have a carat value between 0.25 and 0.75, which are the left and right edges of the bar. You can set the width of the intervals in a histogram with the alt.Bin() and the step argument, which is measured in the units of the x variable. You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. For example, here is how the graph above looks when we zoom into just the diamonds with a size of less than three carats and choose a smaller binwidth. smaller = diamonds.query(&#39;carat &lt; 3&#39;) chart = (alt.Chart(smaller). encode( x = alt.X(&#39;carat&#39;, bin = alt.Bin(step = 0.1)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_hist_smallbin.png&quot;) If you wish to see multiple histograms, Altair doesn’t have a straightforward way to overly histograms. We recommend using facet(). facet() performs the same calculation for each group within the facet varaible. chart = (alt.Chart(smaller). encode( x = alt.X(&#39;carat&#39;, bin = alt.Bin(step = 0.1)), y = &#39;count()&#39;, color = &#39;cut&#39;). mark_bar(). facet(facet = &#39;cut&#39;, columns = 2) ) chart.save(&quot;screenshots/altair_diamonds_facet_hist.png&quot;) There are a few challenges with this type of plot, which we will come back to in visualising a categorical and a continuous variable. Now that you can visualise variation, what should you look for in your plots? And what type of follow-up questions should you ask? I’ve put together a list below of the most useful types of information that you will find in your graphs, along with some follow-up questions for each type of information. The key to asking good follow-up questions will be to rely on your curiosity (What do you want to learn more about?) as well as your skepticism (How could this be misleading?). 7.3.2 Typical values In both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data. To turn this information into useful questions, look for anything unexpected: Which values are the most common? Why? Which values are rare? Why? Does that match your expectations? Can you see any unusual patterns? What might explain them? As an example, the histogram below suggests several interesting questions: Why are there more diamonds at whole carats and common fractions of carats? Why are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak? Why are there no diamonds bigger than 3 carats? chart = (alt.Chart(smaller). encode( x = alt.X(&#39;carat&#39;, bin = alt.Bin(step = 0.01)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_hist_smallestbin.png&quot;) Clusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask: How are the observations within each cluster similar to each other? How are the observations in separate clusters different from each other? How can you explain or describe the clusters? Why might the appearance of clusters be misleading? The histogram below shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between. faithful = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/faithful/faithful.csv&quot;) chart = (alt.Chart(faithful). encode( x = alt.X(&#39;eruptions&#39;, bin = alt.Bin(step = 0.25)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_faithful_hist.png&quot;) Many of the questions above will prompt you to explore a relationship between variables, for example, to see if the values of one variable can explain the behavior of another variable. We’ll get to that shortly. 7.3.3 Unusual values Outliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the y variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis. chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;y&#39;, bin = alt.Bin(step = 0.5)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_y_hist.png&quot;) There are so many observations in the common bins that the rare bins are so short that you can’t see them (although maybe if you stare intently at 0 you’ll spot something). To make it easy to see the unusual values, we need to zoom to small values of the y-axis with alt.Scale() and the argument clip set to True within mark_bar() (note that Altair has other axis options): chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;y&#39;, bin = alt.Bin(step = 0.5)), y = alt.Y(&#39;count()&#39;, scale = alt.Scale(domain = (0, 50)))). mark_bar(clip = True) ) chart.save(&quot;screenshots/altair_diamonds_y_hist_domain.png&quot;) (alt.Scale() can be used within alt.Y() and alt.X() for zooming on either axis. Altair also has a clamp argument that functions that work slightly differently: it stacks all the data right at the limits. But it does not work with mark_bar()) This allows us to see that there are three unusual values: 0, ~30, and ~60. We pluck them out with dplyr: unusual = (diamonds. query(&#39;y &lt; 3 | y &gt; 20&#39;). filter([&#39;price&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;]). sort_values(&#39;y&#39;) ) unusual #&gt; price x y z #&gt; 11963 5139 0.00 0.0 0.00 #&gt; 15951 6381 0.00 0.0 0.00 #&gt; 24520 12800 0.00 0.0 0.00 #&gt; 26243 15686 0.00 0.0 0.00 #&gt; 27429 18034 0.00 0.0 0.00 #&gt; 49556 2130 0.00 0.0 0.00 #&gt; 49557 2130 0.00 0.0 0.00 #&gt; 49189 2075 5.15 31.8 5.12 #&gt; 24067 12210 8.09 58.9 8.06 The y variable measures one of the three dimensions of these diamonds, in mm. We know that diamonds can’t have a width of 0mm, so these values must be incorrect. We might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars! It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up. 7.3.4 Exercises Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the step or the binwidth and make sure you try a wide range of values.) How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference? 7.4 Missing values If you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options. Drop the entire row with the strange values: diamonds2 = diamonds.query(&#39;3 &lt;= y &lt;= 20&#39;) I don’t recommend this option because just because one measurement is invalid, doesn’t mean all the measurements are. Additionally, if you have low quality data, by time that you’ve applied this approach to every variable you might find that you don’t have any data left! Instead, I recommend replacing the unusual values with missing values. The easiest way to do this is to use assign() to replace the variable with a modified copy. You can use the np.where() function to replace unusual values with np.nan: diamonds2 = diamonds.assign( y = lambda x: np.where((x.y &lt; 3) | (x.y &gt; 20), np.nan, x.y) ) np.where() has three arguments. The first argument condition should be a logical vector. The result will contain the value of the second argument, yes, when condition is True, and the value of the third argument, no, when it is False. Altair has two ways to handled missing values, np.nan. It’s not obvious where you should plot missing values, so Altair excludes them in the plot. If you would like to treat the missing values as 0 then you can use invalid = None. chart = (alt.Chart(diamonds2). encode( x = &#39;x&#39;, y = &#39;y&#39;). mark_point() ) chart.save(&quot;screenshots/altair_diamonds2_missing.png&quot;) To plot np.nan as 0 then set invalid = None: chart = (alt.Chart(diamonds2). encode( x = &#39;x&#39;, y = &#39;y&#39;). mark_circle(invalid = None) ) chart.save(&quot;screenshots/altair_diamonds2_zero.png&quot;) Other times you want to understand what makes observations with missing values different to observations with recorded values. For example, in flights, missing values in the dep_time variable indicate that the flight was cancelled. So you might want to compare the scheduled departure times for cancelled and non-cancelled times. You can do this by making a new variable with pd.isna(). flights_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/flights/flights.csv&quot; flights = pd.read_csv(flights_url) flights[&#39;time_hour&#39;] = pd.to_datetime(flights.time_hour, format = &quot;%Y-%m-%d %H:%M:%S&quot;) pdat = flights.assign( cancelled = lambda x: pd.isna(x.dep_time), sched_hour = lambda x: x.sched_dep_time // 100, sched_minute = lambda x: x.sched_dep_time % 100, sched_dep_time = lambda x: x.sched_hour + x.sched_minute / 60 ) chart = (alt.Chart(pdat). encode( x = alt.X(&#39;sched_dep_time&#39;, bin = alt.Bin(step = .25)), y = &#39;count()&#39;, color = &#39;cancelled&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_flights_scheduled.png&quot;) However this plot isn’t great because there are many more non-cancelled flights than cancelled flights. In the next section we’ll explore some techniques for improving this comparison. 7.4.1 Exercises What does invalid = None do in Altair? 7.5 Covariation If variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualise the relationship between two or more variables. How you do that should again depend on the type of variables involved. 7.5.1 A categorical and continuous variable It’s common to want to explore the distribution of a continuous variable broken down by a categorical variable, as in the previous frequency polygon. The default appearance of geom_freqpoly() is not that useful for that sort of comparison because the height is given by the count. That means if one of the groups is much smaller than the others, it’s hard to see the differences in shape. For example, let’s explore how the price of a diamond varies with its quality: chart = (alt.Chart(diamonds). encode( x= alt.X(&#39;price&#39;, bin = alt.Bin(step = 500)), y = &#39;count()&#39;, color = &#39;cut&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_price_cut.png&quot;) It’s hard to see the difference in distribution because the overall counts differ so much: chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;cut:O&#39;, scale=alt.Scale(domain=[&#39;Fair&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Premium&#39;, &#39;Ideal&#39;])), y = &#39;count()&#39;). mark_bar(). properties(width = 400)) chart.save(&quot;screenshots/altair_diamonds_price_cut_bar.png&quot;) To make the comparison easier we need to create a chart with density displayed on the y-axis, which is the count standardised so that the area under each curve is one. To do this we must use transform_density which is one of many transform_ functions in Altair. chart = (alt.Chart(diamonds). transform_density( density = &#39;price&#39;, bandwidth = 500, counts = True, steps = 500, as_ = [&#39;price&#39;, &#39;density&#39;], groupby = [&#39;cut&#39;]). encode( x = &#39;price&#39;, y = alt.Y(&#39;density:Q&#39;, stack = &#39;zero&#39;), color = alt.Color(&#39;cut:O&#39;, scale=alt.Scale(scheme=&#39;dark2&#39;, domain=[&#39;Fair&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Premium&#39;, &#39;Ideal&#39;]), )). mark_bar(opacity = .20) ) chart.save(&quot;screenshots/altair_diamonds_price_density.png&quot;) There’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price! But maybe that’s because frequency polygons are a little hard to interpret - there’s a lot going on in this plot. Another alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot. A boxplot is a type of visual shorthand for a distribution of values that is popular among statisticians. Each boxplot consists of: A box that stretches from the 25th percentile of the distribution to the 75th percentile, a distance known as the interquartile range (IQR). In the middle of the box is a line that displays the median, i.e. 50th percentile, of the distribution. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side. Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually. A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution. Let’s take a look at the distribution of price by cut using geom_boxplot(): chart = (alt.Chart(diamonds). encode( x = &#39;cut&#39;, y = &#39;price&#39;). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_boxplot_2.png&quot;) We see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot). It supports the counterintuitive finding that better quality diamonds are cheaper on average! In the exercises, you’ll be challenged to figure out why. cut is an ordered factor: fair is worse than good, which is worse than very good and so on. Many categorical variables don’t have such an intrinsic order, so you might want to reorder them to make a more informative display. One way to do that is within the alt.X() or alt.Y() functions with the sort argument that accepts a string of the encoding channel used for the sort (e.g. 'x' or 'y') with an optional minus prefix for descending order. Another options that will work with all Altair mark types is to use pandas to define the ordering of the categorical levels to use in alt.Scale() with the domain argument. For example, take the class variable in the mpg dataset. You might be interested to know how highway mileage varies across classes: mpg = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv&quot;) chart = (alt.Chart(mpg). encode( x = &#39;class&#39;, y = &#39;hwy&#39;). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_boxplot_3.png&quot;) To make the trend easier to see, we can reorder class based on the median value of hwy: index = list(mpg.groupby(&#39;class&#39;).median().sort_values(&#39;hwy&#39;).index) chart = (alt.Chart(mpg). encode( x = alt.X(&#39;class&#39;, scale = alt.Scale(domain = index)), y = &#39;hwy&#39;). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_boxplot_4.png&quot;) If you have long variable names, mark_boxplot() will work better if you flip it 90°. You can do that with changing the x and y encodings. chart = (alt.Chart(mpg). encode( y = alt.Y(&#39;class&#39;, scale = alt.Scale(domain = index)), x = &#39;hwy&#39;). mark_boxplot(size = 25). properties(height = 300) ) chart.save(&quot;screenshots/altair_boxplot_5.png&quot;) 7.5.1.1 Exercises Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive? 7.5.2 Two categorical variables To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination. One way to do that is to compute the count with pandas: chart_dat = (diamonds. groupby([&#39;color&#39;, &#39;cut&#39;]). size(). reset_index(name = &#39;n&#39;) ) chart_dat.head() #&gt; color cut n #&gt; 0 D Fair 163 #&gt; 1 D Good 662 #&gt; 2 D Very Good 1513 #&gt; 3 D Premium 1603 #&gt; 4 D Ideal 2834 Then visualise with geom_tile() and the fill aesthetic: chart = (alt.Chart(chart_dat). encode( x = &#39;color&#39;, y = &#39;cut&#39;, color = &#39;n&#39;, stroke = alt.value(&#39;grey&#39;)). mark_rect() ) chart.save(&quot;screenshots/altair_heatmap.png&quot;) If the categorical variables are unordered, you might want to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. 7.5.2.1 Exercises How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut? Use mark_rect() together with pandas to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it? 7.5.3 Two continuous variables You’ve already seen one great way to visualise the covariation between two continuous variables: draw a scatterplot with mark_point(). You can see covariation as a pattern in the points. For example, you can see an exponential relationship between the carat size and price of a diamond. chart = (alt.Chart(diamonds). encode( x = &#39;carat&#39;, y = &#39;price&#39;). mark_circle() ) chart.save(&quot;screenshots/altair_diamonds_scatter_eda.png&quot;) Scatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black (as above). You’ve already seen one way to fix the problem: using the opacity argument to add transparency. chart = (alt.Chart(diamonds). encode( x = &#39;carat&#39;, y = &#39;price&#39;). mark_circle(opacity = 1/100) ) chart.save(&quot;screenshots/altair_diamonds_scatter_eda_2.png&quot;) But using transparency can be challenging for very large datasets. Another solution is to use bins. Previously you used mark_histogram() to bin in one dimension. We will need to use pandas to bin in two directions. First create two new columns that bin price and carat. chart_dat = (smaller. assign( price_cut = lambda x: pd.cut(x.price, bins = np.arange(0, 20000, step = 1000), labels = np.arange(0, 19000, step = 1000)), carat_cut = lambda x: pd.cut(x.carat, bins = np.arange(0, 5.4, step = .2), labels = np.arange(0, 5.2, step = .2)) )) chart_dat.head() #&gt; carat cut color clarity depth ... x y z price_cut carat_cut #&gt; 0 0.23 Ideal E SI2 61.5 ... 3.95 3.98 2.43 0 0.2 #&gt; 1 0.21 Premium E SI1 59.8 ... 3.89 3.84 2.31 0 0.2 #&gt; 2 0.23 Good E VS1 56.9 ... 4.05 4.07 2.31 0 0.2 #&gt; 3 0.29 Premium I VS2 62.4 ... 4.20 4.23 2.63 0 0.2 #&gt; 4 0.31 Good J SI2 63.3 ... 4.34 4.35 2.75 0 0.2 #&gt; #&gt; [5 rows x 12 columns] Then create the binned counts and replace all zero counts with np.nan. chart_dat_binned = (chart_dat. groupby([&#39;carat_cut&#39;, &#39;price_cut&#39;]). size(). reset_index(name = &#39;n&#39;)) chart_dat_binned[&#39;n&#39;].replace(to_replace = 0, value = np.nan, inplace = True) chart_dat_binned.head() #&gt; carat_cut price_cut n #&gt; 0 0.0 0 12.0 #&gt; 1 0.0 1000 NaN #&gt; 2 0.0 2000 NaN #&gt; 3 0.0 3000 NaN #&gt; 4 0.0 4000 NaN mark_rect can then use the divided coordinate plane (2d bins) and then use a fill color to display how many points fall into each bin. Notice the use of `sort = ‘y’’ to reorient the y-axis. chart = (alt.Chart(chart_dat_binned). encode( x = &#39;carat_cut&#39;, y = alt.Y(&#39;price_cut&#39;, sort = &#39;-y&#39;), color = &#39;n:Q&#39;). mark_rect() ) chart.save(&quot;screenshots/altair_diamonds_scatter_binned.png&quot;) Another option is to bin one continuous variable so it acts like a categorical variable. Then you can use one of the techniques for visualising the combination of a categorical and a continuous variable that you learned about. For example, you could bin carat and then for each group, display a boxplot: chart = (alt.Chart(chart_dat). encode( x = &#39;carat_cut:O&#39;, y = &#39;price&#39;). mark_boxplot(). properties(width = 300) ) chart.save(&quot;screenshots/altair_diamonds_scatter_binned_boxplot.png&quot;) pd.cut(), as used above, divides x into bins of equal widths. By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summarises a different number of points. One approach is to display approximately the same number of points in each bin. That’s the job of pd.qcut(): chart_dat = smaller.assign(carat_cut = lambda x: pd.qcut(x.carat,10, labels = False)) chart = (alt.Chart(chart_dat). encode( x = &#39;carat_cut:O&#39;, y = &#39;price&#39;). mark_boxplot(). properties(width = 300) ) chart.save(&quot;screenshots/altair_diamonds_scatter_binned_boxplot_quantiles.png&quot;) 7.5.3.1 Exercises Visualise the distribution of carat, partitioned by price. How does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you? Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price. Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;x&#39;, scale = alt.Scale(domain = (4, 11))), y = alt.Y(&#39;y&#39;, scale = alt.Scale(domain = (4, 11)))). mark_circle(clip = True)) chart.save(&quot;screenshots/altair_diamonds_scatter_clip.png&quot;) Why is a scatterplot a better display than a binned plot for this case? What does clamp = True in alt.Scale() do due your chart? 7.6 Patterns and models Patterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself: Could this pattern be due to coincidence (i.e. random chance)? How can you describe the relationship implied by the pattern? How strong is the relationship implied by the pattern? What other variables might affect the relationship? Does the relationship change if you look at individual subgroups of the data? A scatterplot of Old Faithful eruption lengths versus the wait time between eruptions shows a pattern: longer wait times are associated with longer eruptions. The scatterplot also displays the two clusters that we noticed above. chart = (alt.Chart(faithful). encode( x = alt.X(&#39;eruptions&#39;, scale=alt.Scale(zero=False)), y = alt.Y(&#39;waiting&#39;, scale=alt.Scale(zero=False))). mark_circle() ) chart.save(&quot;screenshots/altair_faithful_scatter_clip.png&quot;) Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second. Models are a tool for extracting patterns out of data. For example, consider the diamonds data. It’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. It’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts price from carat and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed. import statsmodels.formula.api as smf mod = smf.ols(&#39;np.log(price) ~ np.log(carat)&#39;, data = diamonds).fit() diamonds2 = diamonds.assign(resid = np.exp(mod.resid)) chart = (alt.Chart(diamonds2). encode( x = &#39;carat&#39;, y = &#39;resid&#39;). mark_circle() ) chart.save(&quot;screenshots/altair_diamonds_model_scatter.png&quot;) Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive. chart = (alt.Chart(diamonds2). encode( x = &#39;cut&#39;, y = &#39;resid&#39;). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_diamonds_model_boxplot.png&quot;) You’ll learn how models, and the modelr package, work in the final part of the book, model. We’re saving modelling for later because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand. 7.7 Altair calls As we move on from these introductory chapters, we’ll transition to a more concise expression of Altair code. So far we’ve been very explicit, which is helpful when you are learning: (alt.Chart(faithful). encode( x = alt.X(&#39;eruptions&#39;), y = alt.Y(&#39;waiting&#39;)). mark_circle() ) In the remainder of the book, we won’t supply the x and y names if we are using alt.X() and alt.Y(). That saves typing, and, by reducing the amount of boilerplate, makes it easier to see what’s different between plots. That’s a really important programming concern that we’ll come back in [functions]. In addition, for less complicated charts, we will not use () with each element on it’s own line. Altair guidance also often places the mark_ before the encode(). Rewriting the previous plot more concisely yields: alt.Chart(faithful).encode( alt.X(&#39;eruptions&#39;), alt.Y(&#39;waiting&#39;)).mark_circle() Altair guidance also often places the mark_ before the encode() to allow the final element to be the encoding which can easily be placed on multiple lines without the need of () around the entire command. alt.Chart(faithful).mark_circle().encode( alt.X(&#39;eruptions&#39;), alt.Y(&#39;waiting&#39;)) 7.8 Learning more If you want to learn more about the mechanics of Altair, I’d highly recommend reveiwing the Altair website: https://altair-viz.github.io/getting_started/overview.html. Another useful resource is the Vega-Lite website. "],
["workflow-projects.html", "8 Workflow: projects 8.1 What is real? 8.2 Where does your analysis live? 8.3 Paths and directories 8.4 VS Code workspaces 8.5 Summary", " 8 Workflow: projects One day you will need to quit Python, go do something else and return to your analysis the next day. One day you will be working on multiple analyses simultaneously that all use Python and you want to keep them separate. One day you will need to bring data from the outside world into Python and send numerical results and figures from Python back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. what will you save as your lasting record of what happened? Where does your analysis “live”? 8.1 What is real? As a beginning Python user, it’s OK to consider your environment (i.e. the objects listed in the variables pane) “real”. However, in the long run, you’ll be much better off if you consider your Python scripts as “real”. With your Python scripts (and your data files), you can recreate the environment. It’s much harder to recreate your Python scripts from your environment! You’ll either have to retype a lot of code from memory (making mistakes all the way) or you’ll have to carefully mine your Python history. There’s nothing worse than discovering three months after the fact that you’ve only stored the results of an important calculation in your workspace, not the calculation itself in your code. 8.2 Where does your analysis live? VS Code’s interactive Python has a powerful notion of the working directory. This is where Python looks for files that you ask it to load, and where it will put any files that you ask it to save. The interactive console will default the working directory to the location of your .py script. And you can print this out in Python code by running os.getcwd(): import os os.getcwd() #&gt; &#39;/Users/hathawayj/git/byuidatascience/python4ds&#39; As a beginning Python user, it’s OK to let your home directory, documents directory, or any other weird directory on your computer be Python’s working directory. But you’re six chapters into this book, and you’re no longer a rank beginner. Very soon now you should evolve to organising your analytical projects into directories and, when working on a project, setting Python’s working directory to the associated directory. I do not recommend it, but you can also set the working directory from within Python: os.chdir(&quot;/path/to/my/CoolProject&quot;) But you should never do this because there’s a better way; a way that also puts you on the path to managing your Python data science work like an expert. 8.3 Paths and directories Paths and directories are a little complicated because there are two basic styles of paths: Mac/Linux and Windows. There are three chief ways in which they differ: The most important difference is how you separate the components of the path. Mac and Linux uses slashes (e.g. plots/diamonds.pdf) and Windows uses backslashes (e.g. plots\\diamonds.pdf). Python can work with either type (no matter what platform you’re currently using), but unfortunately, backslashes mean something special to Python, and to get a single backslash in the path, you need to type two backslashes! That makes life frustrating, so I recommend always using the Linux/Mac style with forward slashes. Absolute paths (i.e. paths that point to the same place regardless of your working directory) look different. In Windows they start with a drive letter (e.g. C:) or two backslashes (e.g. \\\\servername) and in Mac/Linux they start with a slash “/” (e.g. /users/hadley). You should never use absolute paths in your scripts, because they hinder sharing: no one else will have exactly the same directory configuration as you. The last minor difference is the place that ~ points to. ~ is a convenient shortcut to your home directory. Windows doesn’t really have the notion of a home directory, so it instead points to your documents directory. 8.4 VS Code workspaces Python experts keep all the files associated with a project together — input data, R scripts, analytical results, figures. This is such a wise and common practice that VS Code has built-in support for this via workspaces. Let’s make a workspace for you to use while you’re working through the rest of this book. Click File &gt; Open and select a newly created folder for your work. New Project, then: Name your folder python4ds and think carefully about which subdirectory you put the folder in. If you don’t store it somewhere sensible, it will be hard to find it in the future! Once this process is complete, you’ll get a new VS Code workspace just for this book. Under the Welcome screen select ‘New File’ and save the file, calling it “diamonds.py”. Upon saving the file as a Python file VS Code will make sure your workspace is setup to work with Python with a few prompts. Check that the “home” directory of your workspace is the current working directory: import os os.getcwd() #&gt; &#39;/Users/hathawayj/Downloads/python4ds&#39; Whenever you refer to a file with a relative path it will look for it here. Now enter the following commands in the script editor, and save the file, calling it “diamonds.py”. Next, run the complete script which will save a PNG, CSV, and JSON file into your project directory. Don’t worry about the details, you’ll learn them later in the book. import pandas as pd import altair as alt alt.data_transformers.enable(&#39;json&#39;) url_path = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot; diamonds = pd.read_csv(url_path) chart = (alt.Chart(diamonds). mark_circle(). encode( x = alt.X(&quot;carat&quot;, bin=True), y = alt.Y(&quot;price&quot;, bin=True), size = &quot;count()&quot; ) ) chart.save(&quot;diamonds.png&quot;) diamonds.to_csv(&quot;diamonds.csv&quot;) Inspect the folder associated with your project — notice the .vscode folder. Double-click that folder to see the default workspace settings for your project. You can read more about VS code workspaces to understand the other available options. In your favorite OS-specific way, search your computer for diamonds.png and you will find the PNG (no surprise) but also the script that created it (diamonds.py). This is huge win! One day you will want to remake a figure or just understand where it came from. If you rigorously save figures to files with Python code and never with the mouse or the clipboard, you will be able to reproduce old work with ease! 8.5 Summary In summary, VS Code workspaces give you a solid workflow that will serve you well in the future: Create an workspace for each data analysis project. Keep data files there; we’ll talk about loading them into Python in [data import]. Keep scripts there; edit them, run them in bits or as a whole. Save your outputs (plots and cleaned data) there. Only ever use relative paths, not absolute paths. Everything you need is in one place, and cleanly separated from all the other projects that you are working on. "],
["wrangle-intro.html", "9 Introduction", " 9 Introduction In this part of the book, you’ll learn about data wrangling, the art of getting your data into R in a useful form for visualisation and modelling. Data wrangling is very important: without it you can’t work with your own data! There are three main parts to data wrangling: This part of the book proceeds as follows: In [tibbles], you’ll learn about the variant of the data frame that we use in this book: the tibble. You’ll learn what makes them different from regular data frames, and how you can construct them “by hand”. In [data import], you’ll learn how to get your data from disk and into R. We’ll focus on plain-text rectangular formats, but will give you pointers to packages that help with other types of data. In [tidy data], you’ll learn about tidy data, a consistent way of storing your data that makes transformation, visualisation, and modelling easier. You’ll learn the underlying principles, and how to get your data into a tidy form. Data wrangling also encompasses data transformation, which you’ve already learned a little about. Now we’ll focus on new skills for three specific types of data you will frequently encounter in practice: [Relational data] will give you tools for working with multiple interrelated datasets. [Strings] will introduce regular expressions, a powerful tool for manipulating strings. [Factors] are how R stores categorical data. They are used when a variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string. [Dates and times] will give you the key tools for working with dates and date-times. "],
["dataframe.html", "10 DataFrame 10.1 Introduction 10.2 Creating tibbles 10.3 Printing 10.4 Exercises", " 10 DataFrame 10.1 Introduction Throughout this book we work with pandas _DataFrame__. Python is an old language, and the tools for data in Python that were useful 10 or 20 years ago now get in your way. Here we will describe the DataFrame data structure, which provides opinionated data frames that make working in with data easier. In most places, I’ll use the term data frame. If this chapter leaves you wanting to learn more about data frames, you might enjoy reading the documentation. 10.1.1 Prerequisites In this chapter we’ll explore the DataFrame, a foundational element of pandas. import pandas as pd import pandas as pd import altair as alt import numpy as np 10.2 Creating tibbles Almost all of the functions that you’ll use in this book produce data frames, as data frames are one of the unifying features of pandas. You can create a new data frames from individual vectors with pd.DataFrame(). pd.DataFrame() will automatically recycle inputs of length 1 but does not allow you to refer to variables that you just created. pd.DataFrame({ &#39;x&#39;: [1, 2, 3, 4, 5], &#39;y&#39;: 1} ).assign(z = lambda x: x.x**2 + x.y) #&gt; x y z #&gt; 0 1 1 2 #&gt; 1 2 1 5 #&gt; 2 3 1 10 #&gt; 3 4 1 17 #&gt; 4 5 1 26 Unlike R data frames, pandas data frames can have column names that are not valid R variable names, aka non-syntactic names. For example, they might not start with a letter, or they might contain unusual characters like a space. Notice the use of index as we are passing all scalar values: tb = pd.DataFrame({ &#39;:)&#39;: &#39;smile&#39;, &#39; &#39; : &#39;space&#39;, &#39;2000&#39;: &#39;number&#39;},index=[0]) tb #&gt; :) 2000 #&gt; 0 smile space number Another way to create a tibble is with np.arrray(). Sometimes np.array() makes it possible to lay out small amounts of data in easy to read form. pd.DataFrame(np.array( [[&quot;a&quot;, 2, 3.6], [&quot;b&quot;, 1, 8.5]]), columns = [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;]) #&gt; x y z #&gt; 0 a 2 3.6 #&gt; 1 b 1 8.5 10.3 Printing Data frames have a refined print method that shows only the first and last 5 rows, and all the columns that fit on screen. This makes it much easier to work with large data. tibble( a = lubridate::now() + runif(1e3) * 86400, b = lubridate::today() + runif(1e3) * 30, c = 1:1e3, d = runif(1e3), e = sample(letters, 1e3, replace = TRUE) ) #&gt; # A tibble: 1,000 x 5 #&gt; a b c d e #&gt; &lt;dttm&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2020-06-24 20:39:53 2020-07-01 1 0.368 n #&gt; 2 2020-06-25 14:45:03 2020-07-06 2 0.612 l #&gt; 3 2020-06-25 09:08:42 2020-07-16 3 0.415 p #&gt; 4 2020-06-24 22:29:59 2020-07-15 4 0.212 m #&gt; 5 2020-06-24 18:54:16 2020-07-12 5 0.733 i #&gt; 6 2020-06-25 05:55:13 2020-07-08 6 0.460 n #&gt; # … with 994 more rows Data frames are designed so that you don’t accidentally overwhelm your console when you print large data frames. But sometimes you need more output than the default display. There are a few options that can help. First, you can return the data frame using .head() on the data frame and control the number of rows (n) of the display. In the interactive Python viewer in VS Code you can scroll to see the other columns. flights.head(20) You can also control the default print behaviour by setting options: pd.set_option(\"display.max_rows\", 101): if more than 101 rows, print only n rows. pd.set_option('precision', 5) will set the number of decimals that are shown. You can see a complete list of options by looking at the pandas help. 10.3.1 Subsetting So far all the tools you’ve learned have worked with complete data frames. If you want to pull out a single variable, you need some new tools, [. [ can extract by name or position. df = pd.DataFrame({ &#39;x&#39;: np.random.uniform(size = 5), &#39;y&#39;: np.random.normal(size = 5)}) # Extract by name as pandas data frame df[[&quot;x&quot;]] # Extract by name as array #&gt; x #&gt; 0 0.325624 #&gt; 1 0.136056 #&gt; 2 0.444913 #&gt; 3 0.414994 #&gt; 4 0.350474 df[&quot;x&quot;] # Extract by position #&gt; 0 0.325624 #&gt; 1 0.136056 #&gt; 2 0.444913 #&gt; 3 0.414994 #&gt; 4 0.350474 #&gt; Name: x, dtype: float64 df.iloc[:, 1] #&gt; 0 -1.851640 #&gt; 1 -0.152508 #&gt; 2 -2.425987 #&gt; 3 -0.851618 #&gt; 4 0.108888 #&gt; Name: y, dtype: float64 df[df.columns[1]] #&gt; 0 -1.851640 #&gt; 1 -0.152508 #&gt; 2 -2.425987 #&gt; 3 -0.851618 #&gt; 4 0.108888 #&gt; Name: y, dtype: float64 10.4 Exercises If you have the name of a variable stored in an object, e.g. var &lt;- \"mpg\", how can you extract the reference variable from a tibble? Practice referring to non-syntactic names in the following data frame by: Extracting the variable called 1. Plotting a scatterplot of 1 vs 2. Creating a new column called 3 which is 2 divided by 1. Renaming the columns to one, two and three. annoying &lt;- tibble( `1` = 1:10, `2` = `1` * 2 + rnorm(length(`1`)) ) "]
]
