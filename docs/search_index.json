[
["index.html", "Python for Data Science Welcome", " Python for Data Science J. Hathaway Welcome This is the port of the website for “R for Data Science” into Python. I am keeping Garrett Grolemund and Hadley Wickham’s writing and examples as much as possible while demonstrating Python instead of R. This book will teach you how to do data science with Python: You’ll learn how to get your data into Python, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with R. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data. This website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. The book is written in RMarkdown with bookdown. The orignial authors and contributors can be found here "],
["introduction.html", "1 Introduction 1.1 What you will learn 1.2 How this book is organised 1.3 What you won’t learn 1.4 Prerequisites 1.5 Running Python code 1.6 Getting help and learning more 1.7 Datasets access 1.8 Acknowledgements 1.9 Colophon", " 1 Introduction Data science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge. The goal of “Python for Data Science” is to help you learn some of the tools in Python that will allow you to begin your data science journey. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges, using the best parts of Python. 1.1 What you will learn Data science is a huge field, and there’s no way you can master it by reading a single book. The goal of this book is to give you a foundation in the essential tools. Our model of the tools needed in a typical data science project looks something like this: First you must import your data into Python. This typically means that you take data stored in a file, database, or web API, and load it into a data frame in Python. If you can’t get your data into Python, you can’t do data science on it! Once you’ve imported your data, it is a good idea to tidy it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions. Once you have tidy data, a common first step is to transform it. Transformation includes narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called wrangling, because getting your data in a form that’s natural to work with often feels like a fight! Once you have tidy data with the variables you need, there are two main engines of knowledge generation: visualisation and modelling. These have complementary strengths and weaknesses so any real analysis will iterate between them many times. Visualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don’t scale particularly well because they require a human to interpret them. Models are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. The last step of data science is communication, an absolutely critical part of any data analysis project. It doesn’t matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others. Surrounding all these tools is programming. Programming is a cross-cutting tool that you use in every part of the project. You don’t need to be an expert programmer to be a data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease. You’ll use these tools in every data science project, but for most projects they’re not enough. There’s a rough 80-20 rule at play; you can tackle about 80% of every project using the tools that you’ll learn in this book, but you’ll need other tools to tackle the remaining 20%. Throughout this book we’ll point you to resources where you can learn more. 1.2 How this book is organised The previous description of the tools of data science is organised roughly according to the order in which you use them in an analysis (although of course you’ll iterate through them multiple times). In our experience, however, this is not the best way to learn them: Starting with data ingest and tidying is sub-optimal because 80% of the time it’s routine and boring, and the other 20% of the time it’s weird and frustrating. That’s a bad place to start learning a new subject! Instead, we’ll start with visualisation and transformation of data that’s already been imported and tidied. That way, when you ingest and tidy your own data, your motivation will stay high because you know the pain is worth it. Some topics are best explained with other tools. For example, we believe that it’s easier to understand how models work if you already know about visualisation, tidy data, and programming. Programming tools are not necessarily interesting in their own right, but do allow you to tackle considerably more challenging problems. We’ll give you a selection of programming tools in the middle of the book, and then you’ll see how they can combine with the data science tools to tackle interesting modelling problems. Within each chapter, we try and stick to a similar pattern: start with some motivating examples so you can see the bigger picture, and then dive into the details. Each section of the book is paired with exercises to help you practice what you’ve learned. While it’s tempting to skip the exercises, there’s no better way to learn than practicing on real problems. 1.3 What you won’t learn There are some important topics that this book doesn’t cover. We believe it’s important to stay ruthlessly focused on the essentials so you can get up and running as quickly as possible. That means this book can’t cover every important topic. 1.3.1 Big data This book proudly focuses on small, in-memory datasets. This is the right place to start because you can’t tackle big data unless you have experience with small data. The tools you learn in this book will easily handle hundreds of megabytes of data, and with a little care you can typically use them to work with 1-2 Gb of data. If you’re routinely working with larger data (10-100 Gb, say), you should learn more about data.table. This book doesn’t teach data.table because it has a very concise interface which makes it harder to learn since it offers fewer linguistic cues. But if you’re working with large data, the performance payoff is worth the extra effort required to learn it. If your data is bigger than this, carefully consider if your big data problem might actually be a small data problem in disguise. While the complete data might be big, often the data needed to answer a specific question is small. You might be able to find a subset, subsample, or summary that fits in memory and still allows you to answer the question that you’re interested in. The challenge here is finding the right small data, which often requires a lot of iteration. Another possibility is that your big data problem is actually a large number of small data problems. Each individual problem might fit in memory, but you have millions of them. For example, you might want to fit a model to each person in your dataset. That would be trivial if you had just 10 or 100 people, but instead you have a million. Fortunately each problem is independent of the others (a setup that is sometimes called embarrassingly parallel), so you just need a system (like Hadoop or Spark) that allows you to send different datasets to different computers for processing. Once you’ve figured out how to answer the question for a single subset using the tools described in this book, you learn new tools like sparklyr, rhipe, and ddr to solve it for the full dataset. 1.3.2 R, Julia, and friends In this book, you won’t learn anything about R, Julia, or any other programming language useful for data science. This isn’t because we think these tools are bad. They’re not! And in practice, most data science teams use a mix of languages, often at least R and Python. However, we strongly believe that it’s best to master one tool at a time. You will get better faster if you dive deep, rather than spreading yourself thinly over many topics. This doesn’t mean you should only know one thing, just that you’ll generally learn faster if you stick to one thing at a time. You should strive to learn new things throughout your career, but make sure your understanding is solid before you move on to the next interesting thing. We think Python is a great place to start your data science journey because it is an environment designed from the ground up to support data science. Python is not just a programming language, but it is also an interactive environment for doing data science. To support interaction, Python is a much more flexible language than many of its peers. 1.3.3 Non-rectangular data This book focuses exclusively on rectangular data: collections of values that are each associated with a variable and an observation. There are lots of datasets that do not naturally fit in this paradigm: including images, sounds, trees, and text. But rectangular data frames are extremely common in science and industry, and we believe that they are a great place to start your data science journey. 1.3.4 Hypothesis confirmation It’s possible to divide data analysis into two camps: hypothesis generation and hypothesis confirmation (sometimes called confirmatory analysis). The focus of this book is unabashedly on hypothesis generation, or data exploration. Here you’ll look deeply at the data and, in combination with your subject knowledge, generate many interesting hypotheses to help explain why the data behaves the way it does. You evaluate the hypotheses informally, using your scepticism to challenge the data in multiple ways. The complement of hypothesis generation is hypothesis confirmation. Hypothesis confirmation is hard for two reasons: You need a precise mathematical model in order to generate falsifiable predictions. This often requires considerable statistical sophistication. You can only use an observation once to confirm a hypothesis. As soon as you use it more than once you’re back to doing exploratory analysis. This means to do hypothesis confirmation you need to “preregister” (write out in advance) your analysis plan, and not deviate from it even when you have seen the data. We’ll talk a little about some strategies you can use to make this easier in modelling. It’s common to think about modelling as a tool for hypothesis confirmation, and visualisation as a tool for hypothesis generation. But that’s a false dichotomy: models are often used for exploration, and with a little care you can use visualisation for confirmation. The key difference is how often do you look at each observation: if you look only once, it’s confirmation; if you look more than once, it’s exploration. 1.4 Prerequisites We’ve made a few assumptions about what you already know in order to get the most out of this book. You should be generally numerically literate, and it’s helpful if you have some programming experience already. There are four things you need to run the code in this book: Python, VS Code, and a handful of other packages. Packages are the fundamental units of reproducible Python code. They include reusable functions, the documentation that describes how to use them, and sample data. 1.4.1 Python To download Python, go to python.org and download Python for your OS. A new major version of Python is released every few years, and there are 5-12 minor releases each year. 1.4.2 Visual Studio Code (VS Code) Visual Studio Code is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages (such as C++, C#, Java, Python, PHP, Go) and runtimes (such as .NET and Unity). Begin your journey with VS Code with these introductory videos. Microsoft provides a great overview of VS Code in their docs. When you use VS code, you’ll have four key regions in the interface: VS Code comes with a simple and intuitive layout that maximizes the space provided for the editor while leaving ample room to browse and access the full context of your folder or project. The UI is divided into five areas: Editor - The main area to edit your files. You can open as many editors as you like side by side vertically and horizontally. Side Bar - Contains different views like the Explorer to assist you while working on your project. Status Bar - Information about the opened project and the files you edit. Activity Bar - Located on the far left-hand side, this lets you switch between views and gives you additional context-specific indicators, like the number of outgoing changes when Git is enabled. Panels - You can display different panels below the editor region for output or debug information, errors and warnings, or an integrated terminal. Panel can also be moved to the right for more vertical space. Each time you start VS Code, it opens up in the same state it was in when you last closed it. The folder, layout, and opened files are preserved. 1.4.2.1 VS Code Python Extension Install the Python extension for VS Code from the Visual Studio Marketplace. For additional details on installing extensions, see Extension Marketplace. The Python extension is named Python and it’s published by Microsoft. You can follow more of their tutorial at code.visualstudio.com. The third video and fourth video in this Python for Beginners set of videos done by Microsoft can also guide you through the Python Extension. 1.4.2.2 VS Code Interactive Python Window An open-source project called Jupyter is the standard method for interactive Python use for data science or scientific computing. However, there are some issues with its use in a development environment. VS Code has provided a way for us to have the best of Python and Jupyter Notebooks with their Python Interactive Window. You will need to install the jupyter python package using pip or pip3 for the interactive Python window to work. See the following section for guidance on using pip. Using the VS Code functionality, you will work with a standard .py file instead of the .ipynb extension typically used with jupyter notebooks. The Python extension in VS Code will recognize # %% as a cell or chunk of python code and add notebook options to ‘Run Cell’ as well as other actions. You can see the code example bellow with the image of the view in VS Code as an example. Microsoft’s documentation goes into more detail (https://code.visualstudio.com/docs/python/jupyter-support-py). # %% msg = &quot;Hello World&quot; print(msg) # %% msg = &quot;Hello again&quot; print(msg) 1.4.3 Package Management (pip) You can install a standard set of data science python packages using pip. However, there are some complications using pip on computers with multiple versions of Python. pip: If your path environment is correct, then a standard pip install [package] will work. This is how most packages direct users to install Python packages. pip3: If your OS has Python 2 and Python 3 installed, you may need to use pip3 install [package]. Force Python version: You can run the pip related to a specific Python installation by using python -m pip install [package]. Some may need to provide the path to their Python installation if your Python path environment is not understood. A few cautions about package management with pip. Never run sudo pip install. If you don’t have root permissions or the OS package manager doesn’t have the package you need, use pip install --user. If you want multiple versions of the same library to coexist, to do Python development, or to isolate dependencies for any other reason, use virtual environments. Generally, you will want to update pip before installing packages - python -m pip install --user --upgrade pip setuptools wheel Conda, poetry, and pipenv are three other options for package management. However, we will focus on using pip. 1.4.3.1 pip package installation examples If we wanted to install the numpy, pandas, xlrd, matplolib, and seaborn packages, we would use pip. Depending on your OS configuration, one of the following should work. Everything in your path is clean and you are an admin on your computer pip install numpy pandas xlrd matplotlib seaborn Everything in your path is clean and you want to install package for the user pip install --user numpy pandas xlrd matplotlib seaborn You have multiple Python versions installed you want to install package for the user without a need to understand which pip maps to which Python python -m pip install --user numpy pandas xlrd matplotlib seaborn 1.4.4 The Data Science Packages You’ll also need to install some Python packages. A Python package is a collection of functions, data, and documentation that extends the capabilities of base Python. Using packages is key to the successful use of Python for data science. The majority of the packages that you will learn in this book are related to the so-called tidyverse packages in R. There are attempts to port they tidyverse package process into Python. We are not showing the tools that recreate the tidyverse in Python but those that current Data Scientists use to do equivelent work in Python. You will notice that pandas is the primary tool with a few packages that come with base Python. Pandas user guide will be referenced heavily as we progress. R Tidyverse Package Python Package dplyr pandas tidyr pandas tibble pandas stringr string and re forcats pandas categorical data readr pandas io tools readxl xlrd and openpyxl ggplot2 seaborn, altair, plotnine purrr built in map function base R stats package statsmodels tidymodels scikit-learn R tensorflow tensorflow R keras keras rmarkdown jupyter Notice that the visualization space in Python does not have a force like ggplot2. Chris Moffitt provided an efficient visualization tools diagram to help Python users with this decision. The following packages will give us a broad data science toolset in Python. pip install numpy pandas xlrd matplotlib pip install seaborn plotnine altair vega_datasets pip install statsmodels scikit-learn pip install jupyter On your computer, type that above of code in the command line. The Python package manager pip will download the packages from PyPi and install them on to your computer. If you have problems installing, make sure that you are connected to the internet. You will not be able to use the functions and objects in a package until you load it with import. It is common in Python for each package to have a standard abbreviated name. For example, numpy is imported as ‘np’, and pandas is imported as ‘pd’ in the code chunk below. import numpy as np import pandas as pd import string import re import matplotlib import matplotlib.pyplot as plt import seaborn as sns from plotnine import * import altair as alt There are many other excellent packages that are not included here. 1.5 Running Python code The previous section showed you a couple of examples of running Python code. Code in the book looks like this: 1 + 2 #&gt; 3 #&gt; 3 If you run the same code in interactive python with VS Code, it will look like this: [1] 1 + 2 3 The Python Interactive window can be used as a standalone console with arbitrary code (with or without code cells). To use the window as a console, open it with the Python: Show Python Interactive window command from the Command Palette. You can then type in code, using Enter to go to a new line and Shift+Enter to run the code. There are two main differences. In your interactive window, you type after the [#]; we don’t show the line number in the book. In the book, output is commented out with #&gt;; in your console it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and into the interactive window. Throughout the book we use a consistent set of conventions to refer to code: Functions are in a code font and followed by parentheses, like sum(), or mean(). Other R objects (like data or function arguments) are in a code font, without parentheses, like flights or x. 1.6 Getting help and learning more This book is not an island; there is no single resource that will allow you to master Python for data science. As you start to apply the techniques described in this book to your own data you will soon find questions that we do not answer. This section describes a few tips on how to get help, and to help you keep learning. If you get stuck, start with Google. Typically adding “python” to a query is enough to restrict it to relevant results: if the search isn’t useful, it often means that there aren’t any Python-specific results available. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it! Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = \"en\") and re-run the code; you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Start by spending a little time searching for an existing answer, including [python] to restrict your search to questions and answers that use Python. If you don’t find anything useful, prepare a minimal reproducible example or reprex. A good reprex makes it easier for other people to help you, and often you’ll figure out the problem yourself in the course of making it. There are three things you need to include to make your example reproducible: required packages, data, and code. Packages should be imported at the top of the script, so it’s easy to see which ones the example needs. This is a good time to check that you’re using the latest version of each package; it’s possible you’ve discovered a bug that’s been fixed since you installed the package. The easiest way to include data in a question is to create a minimal example to that recreates it. Try and find the smallest subset of your data that still reveals the problem. Spend a little bit of time ensuring that your code is easy for others to read: Make sure you’ve used spaces and your variable names are concise, yet informative. Use comments to indicate where your problem lies. Do your best to remove everything that is not related to the problem. The shorter your code is, the easier it is to understand, and the easier it is to fix. Finish by checking that you have actually made a reproducible example by starting a fresh Python session to run your script in. You should also spend some time preparing yourself to solve problems before they occur. Investing a little time in learning Python each day will pay off handsomely in the long run. One way is to follow what Wes McKinney, Garrett, and everyone else at RStudio are doing on the Pandas blog or the ossdata blog. This is where they post announcements about new packages and new IDE features. You might also want to follow Wes McKinney (@wesmckinn) on Twitter, or follow @code to keep up with new features in VS Code. To keep up with the data science community more broadly, we recommend reading https://planet.scipy.org/#. If you’re an active Twitter user, follow the #datascience hashtag. 1.7 Datasets access The data used in R for Data Science is generally within the R packages themselves. Many of the Python data science packages also come with datasets upon import. However, we will use the original datasets presented in R for Data Science. We have a data4python4ds GitHub data repository that contains all the datasets in varied file formats and all examples will use Pandas to read the data from GitHub. 1.8 Acknowledgements The text of this book is largely the product of Hadley and Garrett. J. Hathaway has ported the code and descriptions for using VS Code. You can see the original acknowledgements here. 1.9 Colophon An online version of this book is available at https://byuidatascience.github.io/python4ds/. It will continue to evolve. The source of the book is available at https://github.com/byuidatascience/python4ds. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with: sessioninfo::session_info() #&gt; ─ Session info ─────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 3.6.3 (2020-02-29) #&gt; os macOS Catalina 10.15.4 #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz America/Boise #&gt; date 2020-05-13 #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────── #&gt; package * version date lib source #&gt; assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) #&gt; backports 1.1.6 2020-04-05 [1] CRAN (R 3.6.3) #&gt; bookdown 0.18 2020-03-05 [1] CRAN (R 3.6.0) #&gt; broom 0.5.6 2020-04-20 [1] CRAN (R 3.6.2) #&gt; cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.6.0) #&gt; cli 2.0.2 2020-02-28 [1] CRAN (R 3.6.0) #&gt; codetools 0.2-16 2018-12-24 [1] CRAN (R 3.6.3) #&gt; colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.6.0) #&gt; crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) #&gt; DBI 1.0.0 2018-05-02 [1] CRAN (R 3.6.0) #&gt; dbplyr 1.4.2 2019-06-17 [1] CRAN (R 3.6.0) #&gt; digest 0.6.25 2020-02-23 [1] CRAN (R 3.6.0) #&gt; dplyr * 0.8.5 2020-03-07 [1] CRAN (R 3.6.0) #&gt; ellipsis 0.3.0 2019-09-20 [1] CRAN (R 3.6.0) #&gt; evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.0) #&gt; fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.0) #&gt; forcats * 0.5.0 2020-03-01 [1] CRAN (R 3.6.1) #&gt; fs 1.4.1 2020-04-04 [1] CRAN (R 3.6.3) #&gt; generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.0) #&gt; ggplot2 * 3.3.0 2020-03-05 [1] CRAN (R 3.6.1) #&gt; glue 1.4.0 2020-04-03 [1] CRAN (R 3.6.3) #&gt; gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.0) #&gt; haven 2.2.0 2019-11-08 [1] CRAN (R 3.6.0) #&gt; hms 0.5.3 2020-01-08 [1] CRAN (R 3.6.0) #&gt; htmltools 0.4.0 2019-10-04 [1] CRAN (R 3.6.0) #&gt; httr 1.4.1 2019-08-05 [1] CRAN (R 3.6.0) #&gt; jsonlite 1.6.1 2020-02-02 [1] CRAN (R 3.6.0) #&gt; knitr 1.28 2020-02-06 [1] CRAN (R 3.6.0) #&gt; lattice 0.20-38 2018-11-04 [1] CRAN (R 3.6.3) #&gt; lifecycle 0.2.0 2020-03-06 [1] CRAN (R 3.6.0) #&gt; lubridate 1.7.4 2018-04-11 [1] CRAN (R 3.6.0) #&gt; magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) #&gt; modelr 0.1.5 2019-08-08 [1] CRAN (R 3.6.0) #&gt; munsell 0.5.0 2018-06-12 [1] CRAN (R 3.6.0) #&gt; nlme 3.1-144 2020-02-06 [1] CRAN (R 3.6.3) #&gt; pillar 1.4.4 2020-05-05 [1] CRAN (R 3.6.2) #&gt; pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 3.6.0) #&gt; purrr * 0.3.4 2020-04-17 [1] CRAN (R 3.6.2) #&gt; R6 2.4.1 2019-11-12 [1] CRAN (R 3.6.1) #&gt; Rcpp 1.0.4.6 2020-04-09 [1] CRAN (R 3.6.3) #&gt; readr * 1.3.1 2018-12-21 [1] CRAN (R 3.6.0) #&gt; readxl 1.3.1 2019-03-13 [1] CRAN (R 3.6.0) #&gt; reprex 0.3.0 2019-05-16 [1] CRAN (R 3.6.0) #&gt; reticulate 1.14 2019-12-17 [1] CRAN (R 3.6.1) #&gt; rlang 0.4.6 2020-05-02 [1] CRAN (R 3.6.2) #&gt; rmarkdown 2.1.3 2020-05-07 [1] Github (rstudio/rmarkdown@d7e1bda) #&gt; rstudioapi 0.11 2020-02-07 [1] CRAN (R 3.6.0) #&gt; rvest 0.3.5 2019-11-08 [1] CRAN (R 3.6.0) #&gt; scales 1.1.0 2019-11-18 [1] CRAN (R 3.6.0) #&gt; sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.0) #&gt; stringi 1.4.6 2020-02-17 [1] CRAN (R 3.6.0) #&gt; stringr * 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) #&gt; tibble * 3.0.1 2020-04-20 [1] CRAN (R 3.6.2) #&gt; tidyr * 1.0.3 2020-05-07 [1] CRAN (R 3.6.2) #&gt; tidyselect 1.0.0 2020-01-27 [1] CRAN (R 3.6.0) #&gt; tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 3.6.0) #&gt; vctrs 0.2.4 2020-03-10 [1] CRAN (R 3.6.0) #&gt; withr 2.2.0 2020-04-20 [1] CRAN (R 3.6.2) #&gt; xfun 0.13 2020-04-13 [1] CRAN (R 3.6.2) #&gt; xml2 1.3.2 2020-04-23 [1] CRAN (R 3.6.2) #&gt; yaml 2.2.1 2020-02-01 [1] CRAN (R 3.6.0) #&gt; #&gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library "],
["explore-intro.html", "2 Introduction", " 2 Introduction The goal of the first part of this book is to get you up to speed with the basic tools of data exploration as quickly as possible. Data exploration is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again. The goal of data exploration is to generate many promising leads that you can later explore in more depth. In this part of the book you will learn some useful tools that have an immediate payoff: Visualisation is a fun place to start with Python programming, because the payoff is so clear: you get to make elegant and informative plots that help you understand data. In data visualisation you’ll dive into visualisation, learning the basic structure of a pandas plot, and powerful techniques for turning data into plots. Visualisation alone is typically not enough, so in data transformation you’ll learn the key verbs that allow you to select important variables, filter out key observations, create new variables, and compute summaries. Finally, in [exploratory data analysis], you’ll combine visualisation and transformation with your curiosity and scepticism to ask and answer interesting questions about data. Modelling is an important part of the exploratory process, but you don’t have the skills to effectively learn or apply it yet. We’ll come back to it in modelling, once you’re better equipped with more data wrangling and programming tools. Nestled among these three chapters that teach you the tools of exploration are three chapters that focus on your Python workflow. In workflow: basics, workflow: scripts, and workflow: projects you’ll learn good practices for writing and organising your R code. These will set you up for success in the long run, as they’ll give you the tools to stay organised when you tackle real projects. "],
["data-visualisation.html", "3 Data visualisation 3.1 Introduction 3.2 First steps 3.3 Aesthetic mappings 3.4 Common problems 3.5 Facets 3.6 Geometric objects 3.7 Statistical transformations 3.8 Position adjustments 3.9 Coordinate systems (maps) 3.10 The layered grammar of graphics 3.11 Altair’s grammar of graphics", " 3 Data visualisation 3.1 Introduction “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey This chapter will teach you how to visualise your data using Altair. Python has several systems for making graphs, but altiar is one of the most elegant and versatile. Altair implements the declarative visualization much like the grammar of graphics, a coherent system for describing and building graphs. With altair, you can do more faster by learning one system and applying it in many places. If you’d like to learn more about Altair before you start, I’d recommend reading “Altair: Interactive Statistical Visualizations for Python”, https://joss.theoj.org/papers/10.21105/joss.01057.pdf. We should note that we are building this book using R with the package bookdown. Rendering Altair graphics using a python chunk is not straight forward but is not important for our use in VS Code. In VS Code the example chunks will render in the interactive Python viewer automatically. The following R code chunks show how we are rendering the Altair graphics in this book. Thanks to ijlyttle for his GitHub Gist. # ```{R, echo=FALSE} # vegawidget::as_vegaspec(py$chart$to_json()) # ``` # For Python examples that show chart.save() #```{r, message = FALSE, echo=FALSE} #knitr::include_graphics(&quot;screenshots/chartp_chartleft.png&quot;) #``` 3.1.1 Prerequisites This chapter focusses on Altair. Language has been shifted using the material from Altair’s materials. To access the datasets, help pages, and functions that we will use in this chapter, load the Python data science tools by running this code: import pandas as pd import altair as alt If you run this code and get the error message “No module named ‘altair’” or “No module named ‘pandas’”, you’ll need to first install them. python -m pip install pandas altair You only need to install a package once, but you need to reload it every time you start a new session. 3.1.2 Altair data management When specifying data in Altair, we can use pandas DataFrame objects or other Altair options. According to the Altair documentation, the use of a pandas DataFrame will prompt Altair to store the entire data set in JSON format in the chart object. You should be carefully creating Altair specs with all the data in the chart object for use in HTML or Jupyter Notebooks. If you try to plot a data set with more than 5000 rows, Altair will return a maxRowsError. In this book, we will save the Altair chart as a ‘.png’ file to avoid dealing with large stored data in our ‘.html’ files. We have elected to use the Local Filesystem approach proposed by Altair. They do note that the filesystem approach may not work on some cloud-based Jupyter notebook services. alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) 3.2 First steps Let’s use our first graph to answer a question: Do cars with big engines use more fuel than cars with small engines? You probably already have an answer, but try to make your answer precise. What does the relationship between engine size and fuel efficiency look like? Is it positive? Negative? Linear? Nonlinear? 3.2.1 The mpg data frame You can test your answer with the mpg data frame found in ggplot2 (aka ggplot2::mpg). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). The ‘mpg’ data contains observations collected by the US Environmental Protection Agency on 38 models of car. We will identify the ‘mpg’ data using mpg for the remainder of this introduction. mpg = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv&quot;) Among the variables in mpg are: displ, a car’s engine size, in litres. hwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. To learn more about mpg, read informat at data4python4ds. 3.2.2 Creating an Altair plot To plot mpg, run this code to put displ on the x-axis and hwy on the y-axis: chart = (alt.Chart(mpg). encode( x=&#39;displ&#39;, y=&#39;hwy&#39;). mark_point() ) The plot shows a negative relationship between engine size (displ) and fuel efficiency (hwy). In other words, cars with big engines use more fuel. Does this confirm or refute your hypothesis about fuel efficiency and engine size? With Altair, you begin a plot with the function Chart(). Chart() creates a Chart object that you can add layers to. The only argument of Chart() is the dataset to use in the graph. So Chart(mpg) creates an Chart object upon which we can marks. You complete your graph by adding one or more marks to Chart(). The attribute mark_point() adds a layer of points to your plot, which creates a scatterplot. Altair comes with many mark methods that each add a different type of layer to a plot. You’ll learn a whole bunch of them throughout this chapter. Each mark method in Altair has an encode() attribute. This defines how variables in your dataset are encoded to visual properties. The encode() method is always paired with x and y arguments to specify which variables to map to the x and y axes. Altair looks for the encoded variables in the data argument, in this case, mpg. For pandas dataframes, Altair automatically determines the appropriate data type for the mapped column. 3.2.3 A graphing template Let’s turn this code into a reusable template for making graphs with ggplot2. To make a graph, replace the bracketed sections in the code below with a dataset, a geom function, or a collection of mappings. (alt.Chart(&lt;DATA&gt;). &lt;mark_*().&gt; encode(&lt;ENCODINGS&gt;)) The rest of this chapter will show you how to complete and extend this template to make different types of graphs. We will begin with the &lt;ENCODINGS&gt; component. 3.2.4 Exercises Run Chart(mpg).mark_points(). What do you see? How many rows are in mpg? How many columns? What does the drv variable describe? Make a scatterplot of hwy vs cyl. What happens if you make a scatterplot of class vs drv? Why is the plot not useful? 3.3 Aesthetic mappings “The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey In the plot below, one group of points (highlighted in red) seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars? Let’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular). You can add a third variable, like class, to a two dimensional scatterplot by mapping it to an encoding. An encoding is a visual property of the objects in your plot. Encodings include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its encoded properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe encoded properties. Here we change the levels of a point’s size, shape, and color to make the point small, triangular, or blue: You can convey information about your data by mapping the encodings in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. chart = (alt.Chart(mpg). mark_point(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color = &quot;class&quot; ) ) (We don’t prefer British English, like Hadley, so don’t use colour instead of color.) To map an encoding to a variable, associate the name of the encoding to the name of the variable inside encode(). Altair will automatically assign a unique level of the encoding (here a unique color) to each unique value of the variable, a process known as scaling. Altair will also add a legend that explains which levels correspond to which values. The colors reveal that many of the unusual points are two-seater cars. These cars don’t seem like hybrids, and are, in fact, sports cars! Sports cars have large engines like SUVs and pickup trucks, but small bodies like midsize and compact cars, which improves their gas mileage. In hindsight, these cars were unlikely to be hybrids since they have large engines. In the above example, we mapped class to the color encoding, but we could have mapped class to the size encoding in the same way. In this case, the exact size of each point would reveal its class affiliation. Mapping an unordered variable (class) to an ordered aesthetic (size) is not a good idea. chart = (alt.Chart(mpg). mark_point(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, size = &quot;class&quot; ) ) Or we could have mapped class to the opacity encoding, which controls the transparency of the points, or to the shape encoding, which controls the shape of the points. # First chart1 = (alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, opacity = &quot;class&quot; ) ) # Second chart2 = (alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, shape = &quot;class&quot; ) ) chart1.save(&quot;screenshots/altair_opacity.png&quot;) #&gt; WARN Channel opacity should not be used with an unsorted discrete field. chart2.save(&quot;screenshots/altair_shape.png&quot;) Altair will only use 8 shapes for one chart. Charting more than 8 shapes is not recommended as the shapes simply recycle. For each encoding, you use encode() to associate the name of the encoding with a variable to display. The encode() function gathers together each of the encoded mappings used by a layer and passes them to the layer’s mapping argument. The syntax highlights a useful insight about x and y: the x and y locations of a point are themselves encodings, visual properties that you can map to variables to display information about the data. Once you map an encoding, Altair takes care of the rest. It selects a reasonable scale to use with the encoding, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, Altair does not create a legend, but it creates an axis line with tick marks and a label. The axis line acts as a legend; it explains the mapping between locations and values. You can also configure the encoding properties of your mark manually. For example, we can make all of the points in our plot blue: chart = (alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color = alt.value(&quot;blue&quot;) ) ) Here, the color doesn’t convey information about a variable, but only changes the appearance of the plot. To set an encoding manually, use alt.value() by name as an argument of your encode() function; i.e. the value goes inside of alt.value(). You’ll need to pick a level that makes sense for that encoding: The name of a color as a character string. The size of a point in pixels. The shape of a point as a character string. Note that only a limited set of mark properties can be bound to encodings, so for some (e.g. fillOpacity, strokeOpacity, etc.) the encoding approach using alt.value() is not available. Encoding settings will always override local or global configuration settings. There are other methods for manually encoding properties as explained in the Altair documentation 3.3.1 Exercises Which variables in mpg are categorical? Which variables are continuous? How can you see this information when you run mpg? (Hint mpg.dtypes) Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables? What happens if you map the same variable to multiple encodings? What does the stroke encoding do? What shapes does it work with? (Hint: use mark_point()) 3.4 Common problems As you start to run Python code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing Python code for months, and every day I still write code that doesn’t work! Start by carefully comparing the code that you’re running to the code in the book. Python is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \". One common problem when creating Altair graphics as shown in this book, is to put the () in the wrong place: the ( comes before the alt.chart() command and the ) has to come at the end of the command. For example the code below works in Python. alt.Chart(mpg).mark_point(filled = True).encode(x = &quot;displ&quot;, y = &quot;hwy&quot;) However, the complexity of the more details graphics necessicates placing the code on multiple lines. When using multiple lines we need the enclosing (). Make sure you haven’t accidentally excluded a ( or ) like this (alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;) or placed the () incorrectly like this (chart = alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;) ) If you’re still stuck, try the help. You can get help about any Altair function from their website - https://altair-viz.github.io/, or hovering over the function name in VS Code. If that doesn’t help, carefully read the error message. Sometimes the answer will be buried there! But when you’re new to Python, the answer might be in the error message but you don’t yet know how to understand it. Another great tool is Google: try googling the error message, as it’s likely someone else has had the same problem, and has gotten help online. 3.5 Facets One way to add additional variables is with encodings. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data. To facet your plot by a single variable, use facet(). The first argument of facet() is . The variable that you pass to facet_wrap() should be discrete. chart_f = (alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). facet( facet = &quot;class&quot;, columns = 4 ) ) chart_f.save(&quot;screenshots/altair_facet_1.png&quot;) To facet your plot on the combination of two variables, The first argument of facet() is also column and the second is row. This time the formula should contain two variable names. chart_f2 = (alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). facet( column = &quot;drv&quot;, row = &quot;cyl&quot; ) ) chart_f2.save(&quot;screenshots/altair_facet_2.png&quot;) #&gt; WARN row encoding should be discrete (ordinal / nominal / binned). If you prefer to not facet in the rows or columns dimension, simply remove that facet argument. You can read more about compound charts in the Altair documentation. 3.5.1 Exercises What happens if you facet on a continuous variable? What do the empty cells in plot with facet(column = \"drv\", row = \"cyl\") mean? How do they relate to this plot? (alt.Chart(mpg). mark_point(). encode( x = &quot;drv&quot;, y = &quot;cyl&quot;) ) What plots does the following code make? What does . do? (alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;). facet(column = &quot;drv&quot;) ) (alt.Chart(mpg). mark_point(filled = True). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;). facet(row = &quot;cyl&quot;) ) Take the first faceted plot in this section: What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset? When using facet() you should usually put the variable with more unique levels in the columns. Why? 3.6 Geometric objects How are these two plots similar? chartp = (alt.Chart(mpg). mark_point(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chartf = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chartp.save(&quot;screenshots/altair_basic_points.png&quot;) chartf.save(&quot;screenshots/altair_smooth_line.png&quot;) Both plots contain the same x variable, the same y variable, and both describe the same data. But the plots are not identical. Each plot uses a different visual object to represent the data. In Altair syntax, we say that they use different marks. A mark is the geometrical object that a plot uses to represent data. People often describe plots by the type of mark that the plot uses. For example, bar charts use bar marks, line charts use line marks, boxplots use boxplot marks, and so on. Scatterplots break the trend; they use the point mark. As we see above, you can use different marks to plot the same data. The first plot uses the point mark, and the second plot uses the line mark, a smooth line fitted to the data is calculated using a transformation. To change the mark in your plot, change the mark function that you add to Chart(). Every mark function in Altair has encode arguments. However, not every encoding works with every mark. You could set the shape of a point, but you couldn’t set the “shape” of a line. On the other hand, you could set the type of line. mark_line() will draw a different line, with a different strokeDash, for each unique value of the variable that you map to strokeDash. chartl = (alt.Chart(mpg). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, strokeDash = &quot;drv&quot; ) ) chartl.save(&quot;screenshots/altair_dashed_lines.png&quot;) Here mark_line() separates the cars into three lines based on their drv value, which describes a car’s drivetrain. One line describes all of the points with a 4 value, one line describes all of the points with an f value, and one line describes all of the points with an r value. Here, 4 stands for four-wheel drive, f for front-wheel drive, and r for rear-wheel drive. If this sounds strange, we can make it more clear by overlaying the lines on top of the raw data and then coloring everything according to drv. Notice that this plot contains two marks in the same graph! If this makes you excited, buckle up. We will learn how to place multiple marks on the same chart very soon. Altair provides about 15 marks. The best way to get a comprehensive overview is the Altair marks page, which you can find at https://altair-viz.github.io/user_guide/marks.html. Many marks, like mark_line(), use a single mark object to display multiple rows of data. For these marks, you can set the detail encoding to a categorical variable to draw multiple objects. Altair will draw a separate object for each unique value of the detail variable. In practice, Altair will automatically group the data for these marks whenever you map an encoding to a discrete variable (as in the strokeDash example). It is convenient to rely on this feature because the detail encoding by itself does not add a legend or distinguishing features to the marks. chartleft = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chartmiddle = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, detail = &quot;drv&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) chartright = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color=alt.Color(&quot;drv&quot;, legend=None) ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) chartleft.save(&quot;screenshots/altair_chartleft.png&quot;) chartmiddle.save(&quot;screenshots/altair_chartmiddle.png&quot;) chartright.save(&quot;screenshots/altair_chartright.png&quot;) To display multiple marks in the same plot, you can used layered charts as shown in the example below that uses the chartleft object from the above code chunk: chartp = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ). mark_point() ) chart = chartp + chartleft chart.save(&quot;screenshots/altair_chartcombine.png&quot;) This, however, introduces some duplication in our code. Imagine if you wanted to change the y-axis to display cty instead of hwy. You’d need to change the variable in two places, and you might forget to update one. You can avoid this type of repetition by passing a set of encodings to a base alt.Chart(). Altair will treat these encodings as global encodings that apply to each mark layer in the layered chart. In other words, this code will produce the same plot as the previous code: base =(alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart = base.mark_point() + base.transform_loess(&quot;displ&quot;, &quot;hwy&quot;).mark_line() chart.save(&quot;screenshots/altair_combine_clean.png&quot;) If you place encodings in an encode function, Altair will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the base encodings for that layer only. This makes it possible to display different aesthetics in different layers. Alatair automatically chooses useful plot settings and chart configurations to allow you to think about data instead of the programming mechanics of the chart. You can review their guidance on customizing visualizations to see the varied ways to change the look of your graphic. base =(alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart = base.encode(color = &quot;drv&quot;).mark_point() + base.transform_loess(&quot;displ&quot;, &quot;hwy&quot;).mark_line() chart.save(&quot;screenshots/altair_combine_clean_color.png&quot;) You can use the same idea to specify different data for each layer. Here, our smooth line displays just a subset of the mpg dataset, the subcompact cars. The local data argument in geom_smooth() overrides the global data argument in ggplot() for that layer only. #column name of class does not work nicely with Altair filter. base = (alt.Chart(mpg.rename(columns = {&quot;class&quot;: &quot;class1&quot;})). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart_smooth_sub = (base. transform_filter( alt.datum.class1 == &quot;subcompact&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chart = base.encode(color = &quot;class1&quot;).mark_point() + chart_smooth_sub chart.save(&quot;screenshots/altair_combine_clean_color_filter.png&quot;) (You’ll learn how pandas filter works in the chapter on data transformations. To keep the same base chart, filtering is done with Altair in this example: for now, just know that this command selects only the subcompact cars.) 3.6.1 Exercises What geom would you use to draw a line chart? A boxplot? A histogram? An area chart? What does legend=None in alt.Color() do? What happens if you remove it? Why do you think I used it earlier in the chapter? Recreate the Python code necessary to generate the following graphs. 3.7 Statistical transformations Next, let’s take a look at a bar chart. Bar charts seem simple, but they are interesting because they reveal something subtle about plots. Consider a basic bar chart, as drawn with mark_bar(). The following chart displays the total number of diamonds in the diamonds dataset, grouped by cut. The diamonds dataset comes in ggplot2 R package and can be used in Python using the following Python command. Note that we also need to use pandas to format a few of the columns as ordered categorical to have the diamonds DataFrame act like it does in R. diamonds = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot;) diamonds[&#39;cut&#39;] = pd.Categorical(diamonds.cut, ordered = True, categories = [&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot; ]) diamonds[&#39;color&#39;] = pd.Categorical(diamonds.color, ordered = True, categories = [&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;]) diamonds[&#39;clarity&#39;] = pd.Categorical(diamonds.clarity, ordered = True, categories = [&quot;I1&quot;, &quot;SI2&quot;, &quot;SI1&quot;, &quot;VS2&quot;, &quot;VS1&quot;, &quot;VVS2&quot;, &quot;VVS1&quot;, &quot;IF&quot;]) It contains information about ~54,000 diamonds, including the price, carat, color, clarity, and cut of each diamond. The chart shows that more diamonds are available with high quality cuts than with low quality cuts. chart = (alt.Chart(diamonds). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count():Q&quot;) ). mark_bar(). properties(width = 400) ) chart.save(&quot;screenshots/altair_diamond_bar.png&quot;) On the x-axis, the chart displays cut, a variable from diamonds. On the y-axis, it displays count, but count is not a variable in diamonds! Many graphs, like scatterplots, plot the raw values of your dataset. Other graphs, like bar charts, calculate new values to plot: bar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin. smoothers fit a model to your data and then plot predictions from the model. boxplots compute a robust summary of the distribution and then display a specially formatted box. The algorithm used to calculate new values for a graph is called a transform, short for transformation. The figure below describes how this process works with mark_bar(). You must explicitely define the transformation a mark uses through transformations using alt.Y() or alt.X() function. For example, mark_bar() requires the y encoding alt.Y(\"count():Q\"). A histogram is created using mark_bar() with transformations on both the x and y axes. The bin argument accepts a boolean or an alt.Bin() function where the argument maxbins can be used - bin=alt.Bin(maxbins=100). chart = (alt.Chart(diamonds). encode( x =alt.X(&quot;price&quot;, bin=True), y =alt.Y(&quot;count()&quot;) ). mark_bar() ) chart.save(&quot;screenshots/altair_histogram.png&quot;) For more complicated transformations Altair provides transform functions. We saw one of these transforms previously when we used mark_line() to describe each drive type. If you are working with pandas DataFrames then you may want to do these transformations using pandas. Altair’s transformations can be used with DataFrames as well as JSON files or URL pointers to CSV files. chartright = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color=alt.Color(&quot;drv&quot;, legend=None) ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) Finally, mark_boxplot() is available which does the statistical transformations for you after you specify the encodings for the x and y axes. chart = (alt.Chart(diamonds). encode( y =&quot;price&quot;, x =&quot;cut&quot; ). mark_boxplot(size = 25). properties(width = 200) ) chart.save(&quot;screenshots/altair_boxplot.png&quot;) 3.8 Position adjustments There’s one more piece of magic associated with bar charts. You can colour a bar chart using either the stroke aesthetic, or, more usefully, color: chart_left = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), stroke = &quot;cut&quot; ). properties(width = 200) ) chart_right = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), color = &quot;cut&quot; ). properties(width = 200) ) chart_left.save(&quot;screenshots/altair_bar_linecolor.png&quot;) chart_right.save(&quot;screenshots/altair_bar_fillcolor.png&quot;) Note what happens if you map the color encoding to another variable, like clarity: the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity. chart = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), color = &quot;clarity&quot; ). properties(width = 200) ) chart.save(&quot;screenshots/stacked_barchart.png&quot;) The stacking is performed automatically by mark_bar(). If you don’t want a stacked bar chart, you can use use the stack argument in alt.Y() one of three other options: \"identity\", \"dodge\" or \"fill\". position = \"identity\" will place each object exactly where it falls in the context of the graph. This is not very useful for bars, because it overlaps them. To see that overlapping we either need to make the bars slightly transparent by setting alpha to a small value, or completely transparent by setting fill = NA. chart_left = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=None), color = &quot;clarity&quot;, opacity = alt.value(1/5) ). properties(width = 200) ) chart_right = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=None), stroke = &quot;clarity&quot;, color = alt.value(&quot;none&quot;) ). properties(width = 200) ) chart_left.save(&quot;screenshots/altair_nostack_opacity.png&quot;) chart_right.save(&quot;screenshots/altair_nostack_lines.png&quot;) position = \"fill\" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups. chart = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=&#39;normalize&#39;), color = &quot;clarity&quot; ). properties(width = 200) ) chart.save(&quot;screenshots/altair_normalize_bar.png&quot;) knitr::include_graphics(&quot;screenshots/altair_normalize_bar.png&quot;) Placing overlapping objects directly beside one another is done by using the column encoding. This makes it easier to compare individual values with a common baseline. chart = (alt.Chart(diamonds). mark_bar(). encode( x=&#39;clarity&#39;, y=alt.Y(&#39;count()&#39;), color=&#39;clarity&#39;, column=&#39;cut&#39; ) ) chart.save(&quot;screenshots/altair_position_dodge.png&quot;) Altair does not have a simple way to add random jitter to points using an encoding or simple argument to alt.X() or alt.Y(). Altair can create a jittered point plot, also called a stripplot. However, it is not as straight forward. We should note that Altair also does not provide piechart or donut chart marks as well. 3.9 Coordinate systems (maps) Coordinate systems are generally on an x and y axis in Altair. This coordinate system is the Cartesian coordinate system, where the x and y positions act independently to determine the location of each point. Maps are on a Cartesian coordinate system, but their behavior is different as they require a transformation or projection of the globe to an x and y Cartesian plane. Visualizing data on maps can get complicated quickly. Altair has a some mapping marks available for charts. The data formats for plotting with maps also get more involved. You can use the gpdvega package to help with spatial data in Altair. Many Python users us geopandas or Shapely to handle spatial data in Python. PennState has a nice list of spatial tools for Python. 3.10 The layered grammar of graphics In the previous sections, you learned much more than how to make scatterplots, bar charts, and boxplots. You learned a foundation that you can use to make any type of plot with ggplot2. To see this, let’s add position adjustments, stats, coordinate systems, and faceting to our code template: (alt.Chart(&lt;DATA&gt;). encoding( ).facet( column = &lt;FACET VARIABLE&gt; ). &lt;TRANFORM_FUNCTION&gt;(). &lt;MARK_FUNCTION&gt;(). properties( width = , height = ) ) Altair’s general template takes four main parameters - alt.Chart(), mark_*(), encode(), and facet(). In practice, only need to supply the first three parameters to make a chart because Altair will provide useful defaults for everything except the data, the encodings, and the mark function. The parameters in the template compose the grammar of graphics, a formal system for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a mark, a set of encodings, a transformation, a position adjustment, and a faceting scheme. To see how this works, consider how you could build a basic plot from scratch: you could start with a dataset and then transform it into the information that you want to display (with a stat). Next, you could choose a geometric object to represent each observation in the transformed data. You could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic. You’d then select a coordinate system to place the geoms into. You’d use the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables. At that point, you would have a complete graph, but you could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting). You could also extend the plot by adding one or more additional layers, where each additional layer uses a dataset, a geom, a set of mappings, a stat, and a position adjustment. You could use this method to build any plot that you imagine. In other words, you can use the code template that you’ve learned in this chapter to build hundreds of thousands of unique plots. 3.11 Altair’s grammar of graphics Plotnine is a Python implementation of ggplot2 in R’s grammar of graphics using matplotlib as the plotting backend. Altair’s implementation of the grammar of graphics, much like ggplot2 or plotnine at a high level. However, it uses the Vega-Lite grammar of graphics constructs and plotting backend. Below are some useful links that will help you dig deeper into the Altair implementation of the grammar of graphics. Altair mark guidance Altair encoding guidance Altair facet guidance Altair tranformations guidance Altair chart customization Altair sorting barcharts Altair theme editing Altair Encoding data types "],
["workflow-basics.html", "4 Workflow: basics 4.1 Coding basics 4.2 What’s in a name? 4.3 Calling functions 4.4 Exercises", " 4 Workflow: basics You now have some experience running Python code. I didn’t give you many details, but you’ve obviously figured out the basics, or you would’ve thrown this book away in frustration! Frustration is natural when you start programming in Python, because it is such a stickler for punctuation, and even one character out of place will cause it to complain. But while you should expect to be a little frustrated, take comfort in that it’s both typical and temporary: it happens to everyone, and the only way to get over it is to keep trying. Before we go any further, let’s make sure you’ve got a solid foundation in running Python code, and that you know about some of the most helpful VS Code features. 4.1 Coding basics Let’s review some basics we’ve so far omitted in the interests of getting you plotting as quickly as possible. You can use Python as a calculator: 1 / 200 * 30 (59 + 73 + 2) / 3 You can create new objects with =: x = 3 * 4 All Python statements where you create objects, assignment statements, have the same form: object_name = value When reading that code say “object name gets value” in your head. You will make lots of assignments in data science Python programming. It is also good code formatting practice to wrap = with spaces. Code is miserable to read on a good day, so giveyoureyesabreak and use spaces. 4.2 What’s in a name? Object names must start with a letter, and can only contain letters, numbers, and _. You cannot use . like R. You want your object names to be descriptive, so you’ll need a convention for multiple words. Python coding conventions recommend snake_case where you separate lowercase words with _. i_use_snake_case otherPeopleUseCamelCase some.people.use.periods And_aFew.People_RENOUNCEconvention We’ll come back to code style later, in [functions]. You can inspect an object by typing its name: x #&gt; 12 Make another assignment: this_is_a_really_long_name = 2.5 To inspect this object, try out VS Codes completion facility: type “this_”, pause, add characters until you have a unique prefix, then press shift + return. Make yet another assignment: python_rocks = 2 ^ 3 Let’s try to inspect it: python_rock #&gt; --------------------------------------------------------------------------- #&gt; NameError Traceback (most recent call last) #&gt; ~.../python4ds_practice.py in #&gt; ----&gt; 1 python_rock #&gt; NameError: name &#39;python_rock&#39; is not defined Python_rocks #&gt; --------------------------------------------------------------------------- #&gt; NameError Traceback (most recent call last) #&gt; ~.../python4ds_practice.py in #&gt; ----&gt; 1 Python_rocks #&gt; NameError: name &#39;Python_rocks&#39; is not defined There’s an implied contract between you and Python: it will do the tedious computation for you, but in return, you must be completely precise in your instructions. Typos matter. Case matters. 4.3 Calling functions Python does not have a large collection of built-in mathematical and statistical functions. You will need to use pandas, numpy, scikit-learn, and statsmodels to get the suite of functions for working and modeling with data. import pandas as pd import numpy as np import sklearn as sk import statsmodels.api as sm Functions are called like this: &lt;AS PACKAGE NAME&gt;.function_name(arg1 = val1, arg2 = val2, ...) Let’s try using np.arange() which returns regular arangement of numbers and, while we’re at it, learn more helpful features of intellisense in VS code. Type np.ar and pause. A popup shows you possible completions. Specify np.arange() by typing more (a “ange”) to disambiguate, or by using ↑/↓ arrows to select. If you hover over np.arange or type np.arange() a floating tooltip pops up, reminding you of the function’s arguments and purpose. If you want more help, can scroll through the arguments tool tip with your mouse. VS Code will add matching opening (() and closing ()) parentheses for you. Type the arguments 1, 10 and hit return. np.arange(1,10) #&gt; array([1, 2, 3, 4, 5, 6, 7, 8, 9]) Type this code and notice you get similar assistance with the paired quotation marks: x = &quot;hello world&quot; Quotation marks and parentheses must always come in a pair. VS Code does its best to help you, but it’s still possible to mess up and end up with a mismatch. Now look at your Python interactive environment in VS Code in the top toolbar by selecting the icon circled in red : Here you can see all of the objects that you’ve created. 4.4 Exercises Why does this code not work? my_variable &lt;- 10 #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_variable&#39; is not defined #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; my_varıable #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_varıable&#39; is not defined #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Look carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.) Navigate to Visual Studio Code keyboard shortcuts by going to the menu under File &gt; Preferences &gt; Keyboard Shortcuts. (Code &gt; Preferences &gt; Keyboard Shortcuts on macOS). What do you see? You can read more about keybindings on the VS Code website. "],
["transform.html", "5 Data transformation 5.1 Introduction 5.2 Filter rows with query() 5.3 Arrange or sort rows with sort_values() 5.4 Select columns with filter() or loc[] 5.5 Add new variables with assign() 5.6 Grouped summaries or aggregations with agg()", " 5 Data transformation 5.1 Introduction Visualization is an important tool for insight generation, but it is rare that you get the data in exactly the right form you need. Often you’ll need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. You’ll learn how to do all that (and more!) in this chapter, which will teach you how to transform your data using the pandas package and a new dataset on flights departing New York City in 2013. 5.1.1 Prerequisites In this chapter we’re going to focus on how to use the pandas package, the foundational package for data science in Python. We’ll illustrate the key ideas using data from the nycflights13 R package, and use Altair to help us understand the data. We will also need the math package that comes with Python. import pandas as pd import altair as alt import numpy as np flights_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/flights/flights.csv&quot; flights = pd.read_csv(flights_url) flights[&#39;time_hour&#39;] = pd.to_datetime(flights.time_hour, format = &quot;%Y-%m-%d %H:%M:%S&quot;) 5.1.2 nycflights13 To explore the basic data manipulation verbs of pandas, we’ll use flights. This data frame contains all 336,776 flights that departed from New York City in 2013. The data comes from the US Bureau of Transportation Statistics, and is documented here. #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 2013 9 30 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 2013 9 30 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 2013 9 30 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 2013 9 30 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] You might notice that this data frame does not print in its entirety as other data frames you might have seen in the past: it only shows the first few and last few rows with only the columns that fit on one screen. (To see the whole dataset, you can open the variable view in your interactive Python window and double click on the flights object which will open the dataset in the VS Code data viewer). Using flights.dtypes will show you the variables types for each column. These describe the type of each variable: #&gt; year int64 #&gt; month int64 #&gt; day int64 #&gt; dep_time float64 #&gt; sched_dep_time int64 #&gt; dep_delay float64 #&gt; arr_time float64 #&gt; sched_arr_time int64 #&gt; arr_delay float64 #&gt; carrier object #&gt; flight int64 #&gt; tailnum object #&gt; origin object #&gt; dest object #&gt; air_time float64 #&gt; distance int64 #&gt; hour int64 #&gt; minute int64 #&gt; time_hour datetime64[ns, UTC] #&gt; dtype: object int64 stands for integers. float64 stands for doubles, or real numbers. object stands for character vectors, or strings. datetime64 stands for date-times (a date + a time) and dates. You can read more about pandas datetime tools There are three other common types of variables that aren’t used in this dataset but you’ll encounter later in the book: bool stands for logical, vectors that contain only True or False. category stands for factors, which pandas uses to represent categorical variables with fixed possible values. 5.1.3 pandas data manipulation basics In this chapter you are going to learn five key pandas functions or object methods. Object methods are things the objects can perform. For example, pandas data frames know how to tell you their shape, the pandas object knows how to concatenate two data frames together. The way we tell an object we want it to do something is with the ‘dot operator’. Below are the five methods that allow you to solve the vast majority of your data manipulation challenges: Pick observations by their values (query()). Reorder the rows (sort()). Pick variables by their names (select()). Create new variables with functions of existing variables (assign()). Collapse many values down to a single summary (groupby()). The pandas package can handle all of the same functionality of dplyr in R. You can read pandas mapping guide and this towards data science article to get more details on the following brief table. R dplyr function | Python pandas function __________________|_______________________ filter() | query() arrange() | sort_values() select() | filter() or loc[] rename () | rename() mutate() | assign() group_by () | groupby() summarise() | agg() The groupby() changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These functions provide the verbs for a language of data manipulation. All verbs work similarly: The first argument is a pandas dataFrame. The subsequent methods describe what to do with the data frame. The result is a new data frame. Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let’s dive in and see how these verbs work. 5.2 Filter rows with query() query() allows you to subset observations based on their values. The first argument specifies the rows to be selected. This argument can be label names or a boolean series. The second argument specifies the columns to be selected. The bolean filter on the rows is our focus. For example, we can select all flights on January 1st with: flights.query(&#39;month == 1 &amp; day == 1&#39;) #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; .. ... ... ... ... ... ... ... #&gt; 837 2013 1 1 ... 23 59 2013-01-02 04:00:00+00:00 #&gt; 838 2013 1 1 ... 16 30 2013-01-01 21:00:00+00:00 #&gt; 839 2013 1 1 ... 19 35 2013-01-02 00:00:00+00:00 #&gt; 840 2013 1 1 ... 15 0 2013-01-01 20:00:00+00:00 #&gt; 841 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; #&gt; [842 rows x 19 columns] The previous expression is equivalent to flights[(flights.month == 1) &amp; (flights.day == 1)] When you run that line of code, pandas executes the filtering operation and returns a new data frame. pandas functions usually don’t modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, =: jan1 = flights.query(&#39;month == 1 &amp; day == 1&#39;) Interactive Python either prints out the results, or saves them to a variable. 5.2.1 Comparisons To use filtering effectively, you have to know how to select the observations that you want using the comparison operators. Python provides the standard suite: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). When you’re starting out with Python, the easiest mistake to make is to use = instead of == when testing for equality. When this happens you’ll get an error: flights.query(&#39;month = 1&#39;) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: cannot assign without a target object #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/frame.py&quot;, line 3231, in query #&gt; res = self.eval(expr, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/frame.py&quot;, line 3346, in eval #&gt; return _eval(expr, inplace=inplace, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/eval.py&quot;, line 332, in eval #&gt; parsed_expr = Expr(expr, engine=engine, parser=parser, env=env) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 764, in __init__ #&gt; self.terms = self.parse() #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 781, in parse #&gt; return self._visitor.visit(self.expr) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 375, in visit #&gt; return visitor(node, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 381, in visit_Module #&gt; return self.visit(expr, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 375, in visit #&gt; return visitor(node, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 585, in visit_Assign #&gt; raise ValueError(&quot;cannot assign without a target object&quot;) There’s another common problem you might encounter when using ==: floating point numbers. The following result might surprise you! np.sqrt(2) ** 2 == 2 #&gt; False 1 / 49 * 49 == 1 #&gt; False Computers use finite precision arithmetic (they obviously can’t store an infinite number of digits!) so remember that every number you see is an approximation. Instead of relying on ==, use np.isclose(): np.isclose(np.sqrt(2) ** 2, 2) #&gt; True np.isclose(1 / 49 * 49, 1) #&gt; True 5.2.2 Logical operators Multiple arguments to query() are combined with “and”: every expression must be true in order for a row to be included in the output. For other types of combinations, you’ll need to use Boolean operators yourself: &amp; is “and”, | is “or”, and ! is “not”. Figure 5.1 shows the complete set of Boolean operations. Figure 5.1: Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. The following code finds all flights that departed in November or December: flights.query(&#39;month == 11 | month == 12&#39;) The order of operations doesn’t work like English. You can’t write filter(flights, month == (11 | 12)), which you might literally translate into “finds all flights that departed in November or December”. Instead it finds all months that equal 11 | 12, an expression that evaluates to True. In a numeric context (like here), True becomes one, so this finds all flights in January, not November or December. This is quite confusing! A useful short-hand for this problem is x in y. This will select every row where x is one of the values in y. We could use it to rewrite the code above: nov_dec = flights.query(&#39;month in [11, 12]&#39;) Sometimes you can simplify complicated subsetting by remembering De Morgan’s law: !(x &amp; y) is the same as !x | !y, and !(x | y) is the same as !x &amp; !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters: flights.query(&#39;arr_delay &gt; 120 | dep_delay &gt; 120&#39;) flights.query(&#39;arr_delay &lt;= 120 | dep_delay &lt;= 120&#39;) Whenever you start using complicated, multipart expressions in query(), consider making them explicit variables instead. That makes it much easier to check your work. You’ll learn how to create new variables shortly. 5.2.3 Missing values One important feature of pandas in Python that can make comparison tricky are missing values, or NAs (“not availables”). NA represents an unknown value so missing values are “contagious”: almost any operation involving an unknown value will also be unknown. np.nan + 10 #&gt; nan np.nan / 2 #&gt; nan The most confusing result are the comparisons. They always return a False. The logic for this result is explained on stackoverflow. The pandas missing data guide is a helpful read. np.nan &gt; 5 #&gt; False 10 == np.nan #&gt; False np.nan == np.nan #&gt; False It’s easiest to understand why this is true with a bit more context: # Let x be Mary&#39;s age. We don&#39;t know how old she is. x = np.nan # Let y be John&#39;s age. We don&#39;t know how old he is. y = np.nan # Are John and Mary the same age? x == y # Illogical comparisons are False. #&gt; False The Python development team did decide to provide functionality to find np.nan objects in your code by allowing np.nan != np.nan to return True. Once again you can read the rationale for this decision. Python now has isnan() functions to make this comparison more straight forward in your code. Pandas uses the nan structure in Python to identify NA or ‘missing’ values. If you want to determine if a value is missing, use pd.isna(): pd.isna(x) #&gt; True query() only includes rows where the condition is TRUE; it excludes both FALSE and NA values. df = pd.DataFrame({&#39;x&#39;: [1, np.nan, 3]}) df.query(&#39;x &gt; 1&#39;) #&gt; x #&gt; 2 3.0 If you want to preserve missing values, ask for them explicitly using the trick mentioned in the previous paragraph or by using pd.isna() with the symbolic reference @ in your condition: df.query(&#39;x != x | x &gt; 1&#39;) #&gt; x #&gt; 1 NaN #&gt; 2 3.0 df.query(&#39;@pd.isna(x) | x &gt; 1&#39;) #&gt; x #&gt; 1 NaN #&gt; 2 3.0 5.2.4 Exercises Find all flights that Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United, American, or Delta Departed in summer (July, August, and September) Arrived more than two hours late, but didn’t leave late Were delayed by at least an hour, but made up over 30 minutes in flight Departed between midnight and 6am (inclusive) How many flights have a missing dep_time? What other variables are missing? What might these rows represent? 5.3 Arrange or sort rows with sort_values() sort() works similarly to query() except that instead of selecting rows, it changes their order. It takes a data frame and a column name or a list of column names to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns: flights.sort_values(by = [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 111291 2013 12 31 ... 7 5 2013-12-31 12:00:00+00:00 #&gt; 111292 2013 12 31 ... 8 25 2013-12-31 13:00:00+00:00 #&gt; 111293 2013 12 31 ... 16 15 2013-12-31 21:00:00+00:00 #&gt; 111294 2013 12 31 ... 6 0 2013-12-31 11:00:00+00:00 #&gt; 111295 2013 12 31 ... 8 30 2013-12-31 13:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] Use the argument ascending = False to re-order by a column in descending order: flights.sort_values(by = [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;], ascending = False) #&gt; year month day ... hour minute time_hour #&gt; 110520 2013 12 31 ... 23 59 2014-01-01 04:00:00+00:00 #&gt; 110521 2013 12 31 ... 23 59 2014-01-01 04:00:00+00:00 #&gt; 110522 2013 12 31 ... 22 45 2014-01-01 03:00:00+00:00 #&gt; 110523 2013 12 31 ... 5 0 2013-12-31 10:00:00+00:00 #&gt; 110524 2013 12 31 ... 5 15 2013-12-31 10:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 837 2013 1 1 ... 23 59 2013-01-02 04:00:00+00:00 #&gt; 838 2013 1 1 ... 16 30 2013-01-01 21:00:00+00:00 #&gt; 839 2013 1 1 ... 19 35 2013-01-02 00:00:00+00:00 #&gt; 840 2013 1 1 ... 15 0 2013-01-01 20:00:00+00:00 #&gt; 841 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] Missing values are always sorted at the end: df = pd.DataFrame({&#39;x&#39;: [5, 2, np.nan]}) df.sort_values(&#39;x&#39;) #&gt; x #&gt; 1 2.0 #&gt; 0 5.0 #&gt; 2 NaN df.sort_values(&#39;x&#39;, ascending = False) #&gt; x #&gt; 0 5.0 #&gt; 1 2.0 #&gt; 2 NaN 5.3.1 Exercises How could you use sort() to sort all missing values to the start? (Hint: use isna()). Sort flights to find the most delayed flights. Find the flights that left earliest. Sort flights to find the fastest (highest speed) flights. Which flights travelled the farthest? Which travelled the shortest? 5.4 Select columns with filter() or loc[] It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. fliter() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. filter() is not terribly useful with the flights data because we only have 19 variables, but you can still get the general idea: # Select columns by name flights.filter([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) # Select all columns except year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] flights.drop(columns = [&#39;year&#39;, &#39;day&#39;]) #&gt; month dep_time sched_dep_time ... hour minute time_hour #&gt; 0 1 517.0 515 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 1 533.0 529 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 1 542.0 540 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 1 544.0 545 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 1 554.0 600 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 9 NaN 1455 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 9 NaN 2200 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 9 NaN 1210 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 9 NaN 1159 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 9 NaN 840 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 17 columns] loc[] functions in a similar fashion. # Select columns by name flights.loc[:, [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]] # Select all columns between year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] flights.loc[:, &#39;year&#39;:&#39;day&#39;] # Select all columns except year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] There are a number of helper regular expressions you can use within filter(): flights.filter(regex = '^sch'): matches column names that begin with “sch”. flights.filter(regex = \"time$\"): matches names that end with “time”. flights.filter(regex = \"_dep_\"): matches names that contain “dep”. flights.filter(regex = '(.)\\\\1'): selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in [strings]. See pandas filter documentation for more details. Use rename() to rename a column or multiple columns. flights.rename(columns = {&#39;year&#39;: &#39;YEAR&#39;, &#39;month&#39;:&#39;MONTH&#39;}) #&gt; YEAR MONTH day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 2013 9 30 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 2013 9 30 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 2013 9 30 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 2013 9 30 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] 5.4.1 Exercises Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. What happens if you include the name of a variable multiple times in a filter() call? Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default? flights.filter(regex = &quot;TIME&quot;) 5.5 Add new variables with assign() Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. That’s the job of assign(). assign() always adds new columns at the end of your dataset so we’ll start by creating a narrower dataset so we can see the new variables. flights_sml = (flights. filter(regex = &quot;^year$|^month$|^day$|delay$|^distance$|^air_time$&quot;) ) flights_sml.assign( gain = lambda x: x.dep_delay - x.arr_delay, speed = lambda x: x.distance / x.air_time * 60 ).head() #&gt; year month day dep_delay arr_delay air_time distance gain speed #&gt; 0 2013 1 1 2.0 11.0 227.0 1400 -9.0 370.044053 #&gt; 1 2013 1 1 4.0 20.0 227.0 1416 -16.0 374.273128 #&gt; 2 2013 1 1 2.0 33.0 160.0 1089 -31.0 408.375000 #&gt; 3 2013 1 1 -1.0 -18.0 183.0 1576 17.0 516.721311 #&gt; 4 2013 1 1 -6.0 -25.0 116.0 762 19.0 394.137931 Note that you can refer to columns that you’ve just created: flights_sml.assign( gain = lambda x: x.dep_delay - x.arr_delay, hours = lambda x: x.air_time / 60, gain_per_hour = lambda x: x.gain / x.hours ).head() #&gt; year month day dep_delay ... distance gain hours gain_per_hour #&gt; 0 2013 1 1 2.0 ... 1400 -9.0 3.783333 -2.378855 #&gt; 1 2013 1 1 4.0 ... 1416 -16.0 3.783333 -4.229075 #&gt; 2 2013 1 1 2.0 ... 1089 -31.0 2.666667 -11.625000 #&gt; 3 2013 1 1 -1.0 ... 1576 17.0 3.050000 5.573770 #&gt; 4 2013 1 1 -6.0 ... 762 19.0 1.933333 9.827586 #&gt; #&gt; [5 rows x 10 columns] 5.5.1 Useful creation functions There are many functions for creating new variables that you can use with assign(). The key property is that the function must be vectorised: it must take a vector of values as input, return a vector with the same number of values as output. Some arithmetic operators are available in Python without the need for any additional packages. However, many arithmetic functions like mean() and std() are accessed through importing additional packages. Python comes with a math and statistics package. However, we recommend the NumPy package for accessing the suite of mathematical functions needed. You would import NumPy with import numpy as np. There’s no way to list every possible function that you might use, but here’s a selection of functions that are frequently useful: Arithmetic operators: +, -, *, /, ^. These are all vectorised, using the so called “recycling rules”. If one parameter is shorter than the other, it will be automatically extended to be the same length. This is most useful when one of the arguments is a single number: air_time / 60, hours * 60 + minute, etc. Arithmetic operators are also useful in conjunction with the aggregate functions you’ll learn about later. For example, x / np.sum(x) calculates the proportion of a total, and y - np.mean(y) computes the difference from the mean. Modular arithmetic: // (integer division) and % (remainder), where x == y * (x // y) + (x % y). Modular arithmetic is a handy tool because it allows you to break integers up into pieces. For example, in the flights dataset, you can compute hour and minute from dep_time with: (flights. filter([&#39;dep_time&#39;]). assign( hour = lambda x: x.dep_time // 100, minute = lambda x: x.dep_time % 100) ) #&gt; dep_time hour minute #&gt; 0 517.0 5.0 17.0 #&gt; 1 533.0 5.0 33.0 #&gt; 2 542.0 5.0 42.0 #&gt; 3 544.0 5.0 44.0 #&gt; 4 554.0 5.0 54.0 #&gt; ... ... ... ... #&gt; 336771 NaN NaN NaN #&gt; 336772 NaN NaN NaN #&gt; 336773 NaN NaN NaN #&gt; 336774 NaN NaN NaN #&gt; 336775 NaN NaN NaN #&gt; #&gt; [336776 rows x 3 columns] Logs: np.log(), np.log2(), np.log10(). Logarithms are an incredibly useful transformation for dealing with data that ranges across multiple orders of magnitude. They also convert multiplicative relationships to additive, a feature we’ll come back to in modelling. All else being equal, I recommend using np.log2() because it’s easy to interpret: a difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving. Offsets: shift(1) and shift(-1) allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - x.shift(1)) or find when values change (x != x.shift(1)). They are most useful in conjunction with groupby(), which you’ll learn about shortly. x = pd.Series(np.arange(1,10)) x.shift(1) #&gt; 0 NaN #&gt; 1 1.0 #&gt; 2 2.0 #&gt; 3 3.0 #&gt; 4 4.0 #&gt; 5 5.0 #&gt; 6 6.0 #&gt; 7 7.0 #&gt; 8 8.0 #&gt; dtype: float64 x.shift(-1) #&gt; 0 2.0 #&gt; 1 3.0 #&gt; 2 4.0 #&gt; 3 5.0 #&gt; 4 6.0 #&gt; 5 7.0 #&gt; 6 8.0 #&gt; 7 9.0 #&gt; 8 NaN #&gt; dtype: float64 Cumulative and rolling aggregates: pandas provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(). If you need rolling aggregates (i.e. a sum computed over a rolling window), try the rolling() in the pandas package. x #&gt; 0 1 #&gt; 1 2 #&gt; 2 3 #&gt; 3 4 #&gt; 4 5 #&gt; 5 6 #&gt; 6 7 #&gt; 7 8 #&gt; 8 9 #&gt; dtype: int64 x.cumsum() #&gt; 0 1 #&gt; 1 3 #&gt; 2 6 #&gt; 3 10 #&gt; 4 15 #&gt; 5 21 #&gt; 6 28 #&gt; 7 36 #&gt; 8 45 #&gt; dtype: int64 x.rolling(2).mean() #&gt; 0 NaN #&gt; 1 1.5 #&gt; 2 2.5 #&gt; 3 3.5 #&gt; 4 4.5 #&gt; 5 5.5 #&gt; 6 6.5 #&gt; 7 7.5 #&gt; 8 8.5 #&gt; dtype: float64 Logical comparisons, &lt;, &lt;=, &gt;, &gt;=, !=, and ==, which you learned about earlier. If you’re doing a complex sequence of logical operations it’s often a good idea to store the interim values in new variables so you can check that each step is working as expected. Ranking: there are a number of ranking functions, but you should start with min_rank(). It does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small ranks; use desc(x) to give the largest values the smallest ranks. y = pd.Series([1, 2, 2, np.nan, 3, 4]) y.rank(method = &#39;min&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 2.0 #&gt; 3 NaN #&gt; 4 4.0 #&gt; 5 5.0 #&gt; dtype: float64 y.rank(ascending=False, method = &#39;min&#39;) #&gt; 0 5.0 #&gt; 1 3.0 #&gt; 2 3.0 #&gt; 3 NaN #&gt; 4 2.0 #&gt; 5 1.0 #&gt; dtype: float64 If method = 'min'`` doesn't do what you need, look at the variantsmethod = ‘first’,method = ‘dense’,method = ‘percent’,pct = True`. See the rank help page for more details. y.rank(method = &#39;first&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 3.0 #&gt; 3 NaN #&gt; 4 4.0 #&gt; 5 5.0 #&gt; dtype: float64 y.rank(method = &#39;dense&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 2.0 #&gt; 3 NaN #&gt; 4 3.0 #&gt; 5 4.0 #&gt; dtype: float64 y.rank(pct = True) #&gt; 0 0.2 #&gt; 1 0.5 #&gt; 2 0.5 #&gt; 3 NaN #&gt; 4 0.8 #&gt; 5 1.0 #&gt; dtype: float64 5.5.2 Exercises Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for method = 'min'. What trigonometric functions does NumPy provide? 5.6 Grouped summaries or aggregations with agg() The last key verb is agg(). It collapses a data frame to a single row: flights.agg({&#39;dep_delay&#39;: np.mean}) #&gt; dep_delay 12.63907 #&gt; dtype: float64 (Pandas aggregate functions ignores the np.nan values like na.rm = TRUE in R.) agg() is not terribly useful unless we pair it with groupby(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the pandas functions on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied similiar code to a data frame grouped by date, we get the average delay per date. Note that with the groupby() function we used tuple to identify the column (first entry) and the function to apply on the column (second entry). This is called named aggregation in pandas: by_day = flights.groupby([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) by_day.agg(delay = (&#39;dep_delay&#39;, np.mean)).reset_index() #&gt; year month day delay #&gt; 0 2013 1 1 11.548926 #&gt; 1 2013 1 2 13.858824 #&gt; 2 2013 1 3 10.987832 #&gt; 3 2013 1 4 8.951595 #&gt; 4 2013 1 5 5.732218 #&gt; .. ... ... ... ... #&gt; 360 2013 12 27 10.937630 #&gt; 361 2013 12 28 7.981550 #&gt; 362 2013 12 29 22.309551 #&gt; 363 2013 12 30 10.698113 #&gt; 364 2013 12 31 6.996053 #&gt; #&gt; [365 rows x 4 columns] Note the use of reset_index() to remove pandas creation of a MultiIndex. You can read more about the use of grouby in pandas with their Group By: split-apply-combine user Guid documentation Together groupby() and agg() provide one of the tools that you’ll use most commonly when working with pandas: grouped summaries. But before we go any further with this, we need to introduce a structure for pandas code when doing data science work. We structure our code much like ‘the pipe’, %&gt;% in the tidyverse packages from R-Studio. 5.6.1 Combining multiple operations with the pipe Imagine that we want to explore the relationship between the distance and average delay for each location. Using what you know about pandas, you might write code like this: by_dest = flights.groupby(&#39;dest&#39;) delay = by_dest.agg( count = (&#39;distance&#39;, &#39;size&#39;), dist = (&#39;distance&#39;, np.mean), delay = (&#39;arr_delay&#39;, np.mean) ) delay = delay.query(&#39;count &gt; 20 &amp; dest != &quot;HNL&quot;&#39;) # It looks like delays increase with distance up to ~750 miles # and then decrease. Maybe as flights get longer there&#39;s more # ability to make up delays in the air? chart_base = (alt.Chart(delay). encode( x = &#39;dist&#39;, y = &#39;delay&#39; )) chart = chart_base.mark_point() + chart_base.transform_loess(&#39;dist&#39;, &#39;delay&#39;).mark_line() vegawidget::as_vegaspec(py$chart$to_json()) There are three steps to prepare this data: Group flights by destination. Summarise to compute distance, average delay, and number of flights. Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport. This code is a little frustrating to write because we have to give each intermediate data frame a name, even though we don’t care about it. Naming things is hard, so this slows down our analysis. There’s another way to tackle the same problem without the additional objects: delays = (flights. groupby(&#39;dest&#39;). agg( count = (&#39;distance&#39;, &#39;size&#39;), dist = (&#39;distance&#39;, np.mean), delay = (&#39;arr_delay&#39;, np.mean) ). query(&#39;count &gt; 20 &amp; dest != &quot;HNL&quot;&#39;)) This focuses on the transformations, not what’s being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarise, then filter. As suggested by this reading, a good way to pronounce . when reading pandas code is “then”. You can use the () with . to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. We’ll use this format frequently from now on because it considerably improves the readability of complex pandas code. "],
["workflow-scripts.html", "6 Workflow: scripts 6.1 Running code", " 6 Workflow: scripts So far you’ve been using the console to run code. That’s a great place to start, but you’ll find it gets cramped pretty quickly as you create more complex ggplot2 graphics and dplyr pipes. To give yourself more room to work, it’s a great idea to use the script editor. Open it up either by clicking the File menu, and selecting New File or using the keyboard shortcut Cmd/Ctrl + N. Now you’ll see one pane with a tab named ‘Untitled-1’. After saving the file as *.py you can start a new coding cell by typing # %% in your script which will prompt VS Code to give you an interactive Python framework. You will see options to ‘Run Cell’, ‘Run Below’, and ‘Debug cell’ just above the text you typed. Clicking the ‘Run Cell’ will open the Python Interactive console in a side panel. : The script editor is a great place to put code you care about. Keep experimenting in the console, but once you have written code that works and does what you want, put it in the script editor. VS Code does not automatically save the contents of the editor by default. However, you can turn on autosave in the settings and when you quit VS Code it will save and automatically load it when you re-open. Nevertheless, it’s a good idea to save your scripts regularly and to back them up. 6.1 Running code The script editor is also a great place to build up complex Altair charts or long sequences of pandas manipulations. The key to using the script editor effectively is to memorise one of the most important keyboard shortcuts: shift + Enter. This executes the current cell from your Python script in the console. I recommend that you always start your script with the packages that you need. That way, if you share your code with others, they can easily see what packages they need to install. Note, however, that you should never include os.chdir() in a script that you share. It’s very antisocial to change settings on someone else’s computer! When working through future chapters, I highly recommend starting in the editor and practicing your keyboard shortcuts. Over time, sending code to the console in this way will become so natural that you won’t even think about it. "],
["workflow-projects.html", "7 Workflow: projects 7.1 What is real? 7.2 Where does your analysis live? 7.3 Paths and directories 7.4 VS Code workspaces 7.5 Summary", " 7 Workflow: projects One day you will need to quit Python, go do something else and return to your analysis the next day. One day you will be working on multiple analyses simultaneously that all use Python and you want to keep them separate. One day you will need to bring data from the outside world into Python and send numerical results and figures from Python back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. what will you save as your lasting record of what happened? Where does your analysis “live”? 7.1 What is real? As a beginning Python user, it’s OK to consider your environment (i.e. the objects listed in the variables pane) “real”. However, in the long run, you’ll be much better off if you consider your Python scripts as “real”. With your Python scripts (and your data files), you can recreate the environment. It’s much harder to recreate your Python scripts from your environment! You’ll either have to retype a lot of code from memory (making mistakes all the way) or you’ll have to carefully mine your Python history. There’s nothing worse than discovering three months after the fact that you’ve only stored the results of an important calculation in your workspace, not the calculation itself in your code. 7.2 Where does your analysis live? VS Code’s interactive Python has a powerful notion of the working directory. This is where Python looks for files that you ask it to load, and where it will put any files that you ask it to save. The interactive console will default the working directory to the location of your .py script. And you can print this out in Python code by running os.getcwd(): import os os.getcwd() #&gt; &#39;/Users/hathawayj/git/byuidatascience/python4ds&#39; As a beginning Python user, it’s OK to let your home directory, documents directory, or any other weird directory on your computer be Python’s working directory. But you’re six chapters into this book, and you’re no longer a rank beginner. Very soon now you should evolve to organising your analytical projects into directories and, when working on a project, setting Python’s working directory to the associated directory. I do not recommend it, but you can also set the working directory from within Python: os.chdir(&quot;/path/to/my/CoolProject&quot;) But you should never do this because there’s a better way; a way that also puts you on the path to managing your Python data science work like an expert. 7.3 Paths and directories Paths and directories are a little complicated because there are two basic styles of paths: Mac/Linux and Windows. There are three chief ways in which they differ: The most important difference is how you separate the components of the path. Mac and Linux uses slashes (e.g. plots/diamonds.pdf) and Windows uses backslashes (e.g. plots\\diamonds.pdf). Python can work with either type (no matter what platform you’re currently using), but unfortunately, backslashes mean something special to Python, and to get a single backslash in the path, you need to type two backslashes! That makes life frustrating, so I recommend always using the Linux/Mac style with forward slashes. Absolute paths (i.e. paths that point to the same place regardless of your working directory) look different. In Windows they start with a drive letter (e.g. C:) or two backslashes (e.g. \\\\servername) and in Mac/Linux they start with a slash “/” (e.g. /users/hadley). You should never use absolute paths in your scripts, because they hinder sharing: no one else will have exactly the same directory configuration as you. The last minor difference is the place that ~ points to. ~ is a convenient shortcut to your home directory. Windows doesn’t really have the notion of a home directory, so it instead points to your documents directory. 7.4 VS Code workspaces Python experts keep all the files associated with a project together — input data, R scripts, analytical results, figures. This is such a wise and common practice that VS Code has built-in support for this via workspaces. Let’s make a workspace for you to use while you’re working through the rest of this book. Click File &gt; Open and select a newly created folder for your work. New Project, then: Name your folder python4ds and think carefully about which subdirectory you put the folder in. If you don’t store it somewhere sensible, it will be hard to find it in the future! Once this process is complete, you’ll get a new VS Code workspace just for this book. Under the Welcome screen select ‘New File’ and save the file, calling it “diamonds.py”. Upon saving the file as a Python file VS Code will make sure your workspace is setup to work with Python with a few prompts. Check that the “home” directory of your workspace is the current working directory: import os os.getcwd() #&gt; &#39;/Users/hathawayj/Downloads/python4ds&#39; Whenever you refer to a file with a relative path it will look for it here. Now enter the following commands in the script editor, and save the file, calling it “diamonds.py”. Next, run the complete script which will save a PNG, CSV, and JSON file into your project directory. Don’t worry about the details, you’ll learn them later in the book. import pandas as pd import altair as alt alt.data_transformers.enable(&#39;json&#39;) url_path = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot; diamonds = pd.read_csv(url_path) chart = (alt.Chart(diamonds). mark_circle(). encode( x = alt.X(&quot;carat&quot;, bin=True), y = alt.Y(&quot;price&quot;, bin=True), size = &quot;count()&quot; ) ) chart.save(&quot;diamonds.png&quot;) diamonds.to_csv(&quot;diamonds.csv&quot;) Inspect the folder associated with your project — notice the .vscode folder. Double-click that folder to see the default workspace settings for your project. You can read more about VS code workspaces to understand the other available options. In your favorite OS-specific way, search your computer for diamonds.png and you will find the PNG (no surprise) but also the script that created it (diamonds.py). This is huge win! One day you will want to remake a figure or just understand where it came from. If you rigorously save figures to files with Python code and never with the mouse or the clipboard, you will be able to reproduce old work with ease! 7.5 Summary In summary, VS Code workspaces give you a solid workflow that will serve you well in the future: Create an workspace for each data analysis project. Keep data files there; we’ll talk about loading them into Python in [data import]. Keep scripts there; edit them, run them in bits or as a whole. Save your outputs (plots and cleaned data) there. Only ever use relative paths, not absolute paths. Everything you need is in one place, and cleanly separated from all the other projects that you are working on. "]
]
