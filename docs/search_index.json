[
["index.html", "Python for Data Science Welcome", " Python for Data Science J. Hathaway Welcome This is the port of the website for “R for Data Science” into Python. I am keeping Garrett Grolemund and Hadley Wickham’s writing and examples as much as possible while demonstrating Python instead of R. This book will teach you how to do data science with Python: You’ll learn how to get your data into Python, get it into the most useful structure, transform it, visualise it and model it. In this book, you will find a practicum of skills for data science. Just as a chemist learns how to clean test tubes and stock a lab, you’ll learn how to clean data and draw plots—and many other things besides. These are the skills that allow data science to happen, and here you will find the best practices for doing each of these things with Python. You’ll learn how to use the grammar of graphics, literate programming, and reproducible research to save time. You’ll also learn how to manage cognitive resources to facilitate discoveries when wrangling, visualising, and exploring data. This website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. The book is written in RMarkdown with bookdown. The orignial authors and contributors can be found here "],
["introduction.html", "1 Introduction 1.1 What you will learn 1.2 How this book is organised 1.3 What you won’t learn 1.4 Prerequisites 1.5 Running Python code 1.6 Getting help and learning more 1.7 Datasets access 1.8 Acknowledgements 1.9 Colophon", " 1 Introduction Data science is an exciting discipline that allows you to turn raw data into understanding, insight, and knowledge. The goal of “Python for Data Science” is to help you learn some of the tools in Python that will allow you to begin your data science journey. After reading this book, you’ll have the tools to tackle a wide variety of data science challenges, using the best parts of Python. 1.1 What you will learn Data science is a huge field, and there’s no way you can master it by reading a single book. The goal of this book is to give you a foundation in the essential tools. Our model of the tools needed in a typical data science project looks something like this: First you must import your data into Python. This typically means that you take data stored in a file, database, or web API, and load it into a data frame in Python. If you can’t get your data into Python, you can’t do data science on it! Once you’ve imported your data, it is a good idea to tidy it. Tidying your data means storing it in a consistent form that matches the semantics of the dataset with the way it is stored. In brief, when your data is tidy, each column is a variable, and each row is an observation. Tidy data is important because the consistent structure lets you focus your struggle on questions about the data, not fighting to get the data into the right form for different functions. Once you have tidy data, a common first step is to transform it. Transformation includes narrowing in on observations of interest (like all people in one city, or all data from the last year), creating new variables that are functions of existing variables (like computing speed from distance and time), and calculating a set of summary statistics (like counts or means). Together, tidying and transforming are called wrangling, because getting your data in a form that’s natural to work with often feels like a fight! Once you have tidy data with the variables you need, there are two main engines of knowledge generation: visualisation and modelling. These have complementary strengths and weaknesses so any real analysis will iterate between them many times. Visualisation is a fundamentally human activity. A good visualisation will show you things that you did not expect, or raise new questions about the data. A good visualisation might also hint that you’re asking the wrong question, or you need to collect different data. Visualisations can surprise you, but don’t scale particularly well because they require a human to interpret them. Models are complementary tools to visualisation. Once you have made your questions sufficiently precise, you can use a model to answer them. Models are a fundamentally mathematical or computational tool, so they generally scale well. Even when they don’t, it’s usually cheaper to buy more computers than it is to buy more brains! But every model makes assumptions, and by its very nature a model cannot question its own assumptions. That means a model cannot fundamentally surprise you. The last step of data science is communication, an absolutely critical part of any data analysis project. It doesn’t matter how well your models and visualisation have led you to understand the data unless you can also communicate your results to others. Surrounding all these tools is programming. Programming is a cross-cutting tool that you use in every part of the project. You don’t need to be an expert programmer to be a data scientist, but learning more about programming pays off because becoming a better programmer allows you to automate common tasks, and solve new problems with greater ease. You’ll use these tools in every data science project, but for most projects they’re not enough. There’s a rough 80-20 rule at play; you can tackle about 80% of every project using the tools that you’ll learn in this book, but you’ll need other tools to tackle the remaining 20%. Throughout this book we’ll point you to resources where you can learn more. 1.2 How this book is organised The previous description of the tools of data science is organised roughly according to the order in which you use them in an analysis (although of course you’ll iterate through them multiple times). In our experience, however, this is not the best way to learn them: Starting with data ingest and tidying is sub-optimal because 80% of the time it’s routine and boring, and the other 20% of the time it’s weird and frustrating. That’s a bad place to start learning a new subject! Instead, we’ll start with visualisation and transformation of data that’s already been imported and tidied. That way, when you ingest and tidy your own data, your motivation will stay high because you know the pain is worth it. Some topics are best explained with other tools. For example, we believe that it’s easier to understand how models work if you already know about visualisation, tidy data, and programming. Programming tools are not necessarily interesting in their own right, but do allow you to tackle considerably more challenging problems. We’ll give you a selection of programming tools in the middle of the book, and then you’ll see how they can combine with the data science tools to tackle interesting modelling problems. Within each chapter, we try and stick to a similar pattern: start with some motivating examples so you can see the bigger picture, and then dive into the details. Each section of the book is paired with exercises to help you practice what you’ve learned. While it’s tempting to skip the exercises, there’s no better way to learn than practicing on real problems. 1.3 What you won’t learn There are some important topics that this book doesn’t cover. We believe it’s important to stay ruthlessly focused on the essentials so you can get up and running as quickly as possible. That means this book can’t cover every important topic. 1.3.1 Big data This book proudly focuses on small, in-memory datasets. This is the right place to start because you can’t tackle big data unless you have experience with small data. The tools you learn in this book will easily handle hundreds of megabytes of data, and with a little care you can typically use them to work with 1-2 Gb of data. If you’re routinely working with larger data (10-100 Gb, say), you should learn more about data.table. This book doesn’t teach data.table because it has a very concise interface which makes it harder to learn since it offers fewer linguistic cues. But if you’re working with large data, the performance payoff is worth the extra effort required to learn it. If your data is bigger than this, carefully consider if your big data problem might actually be a small data problem in disguise. While the complete data might be big, often the data needed to answer a specific question is small. You might be able to find a subset, subsample, or summary that fits in memory and still allows you to answer the question that you’re interested in. The challenge here is finding the right small data, which often requires a lot of iteration. Another possibility is that your big data problem is actually a large number of small data problems. Each individual problem might fit in memory, but you have millions of them. For example, you might want to fit a model to each person in your dataset. That would be trivial if you had just 10 or 100 people, but instead you have a million. Fortunately each problem is independent of the others (a setup that is sometimes called embarrassingly parallel), so you just need a system (like Hadoop or Spark) that allows you to send different datasets to different computers for processing. Once you’ve figured out how to answer the question for a single subset using the tools described in this book, you learn new tools like sparklyr, rhipe, and ddr to solve it for the full dataset. 1.3.2 R, Julia, and friends In this book, you won’t learn anything about R, Julia, or any other programming language useful for data science. This isn’t because we think these tools are bad. They’re not! And in practice, most data science teams use a mix of languages, often at least R and Python. However, we strongly believe that it’s best to master one tool at a time. You will get better faster if you dive deep, rather than spreading yourself thinly over many topics. This doesn’t mean you should only know one thing, just that you’ll generally learn faster if you stick to one thing at a time. You should strive to learn new things throughout your career, but make sure your understanding is solid before you move on to the next interesting thing. We think Python is a great place to start your data science journey because it is an environment designed from the ground up to support data science. Python is not just a programming language, but it is also an interactive environment for doing data science. To support interaction, Python is a much more flexible language than many of its peers. 1.3.3 Non-rectangular data This book focuses exclusively on rectangular data: collections of values that are each associated with a variable and an observation. There are lots of datasets that do not naturally fit in this paradigm: including images, sounds, trees, and text. But rectangular data frames are extremely common in science and industry, and we believe that they are a great place to start your data science journey. 1.3.4 Hypothesis confirmation It’s possible to divide data analysis into two camps: hypothesis generation and hypothesis confirmation (sometimes called confirmatory analysis). The focus of this book is unabashedly on hypothesis generation, or data exploration. Here you’ll look deeply at the data and, in combination with your subject knowledge, generate many interesting hypotheses to help explain why the data behaves the way it does. You evaluate the hypotheses informally, using your scepticism to challenge the data in multiple ways. The complement of hypothesis generation is hypothesis confirmation. Hypothesis confirmation is hard for two reasons: You need a precise mathematical model in order to generate falsifiable predictions. This often requires considerable statistical sophistication. You can only use an observation once to confirm a hypothesis. As soon as you use it more than once you’re back to doing exploratory analysis. This means to do hypothesis confirmation you need to “preregister” (write out in advance) your analysis plan, and not deviate from it even when you have seen the data. We’ll talk a little about some strategies you can use to make this easier in modelling. It’s common to think about modelling as a tool for hypothesis confirmation, and visualisation as a tool for hypothesis generation. But that’s a false dichotomy: models are often used for exploration, and with a little care you can use visualisation for confirmation. The key difference is how often do you look at each observation: if you look only once, it’s confirmation; if you look more than once, it’s exploration. 1.4 Prerequisites We’ve made a few assumptions about what you already know in order to get the most out of this book. You should be generally numerically literate, and it’s helpful if you have some programming experience already. There are four things you need to run the code in this book: Python, VS Code, a package managment system, and a handful of other packages. Packages are the fundamental units of reproducible Python code. They include reusable functions, the documentation that describes how to use them, and sample data. 1.4.1 Python To download Python, go to python.org and download Python for your OS. A new major version of Python is released every few years, and there are 5-12 minor releases each year. 1.4.2 Visual Studio Code (VS Code) Visual Studio Code is a lightweight but powerful source code editor which runs on your desktop and is available for Windows, macOS and Linux. It comes with built-in support for JavaScript, TypeScript and Node.js and has a rich ecosystem of extensions for other languages (such as C++, C#, Java, Python, PHP, Go) and runtimes (such as .NET and Unity). Begin your journey with VS Code with these introductory videos. Microsoft provides a great overview of VS Code in their docs. When you use VS code, you’ll have four key regions in the interface: VS Code comes with a simple and intuitive layout that maximizes the space provided for the editor while leaving ample room to browse and access the full context of your folder or project. The UI is divided into five areas: Editor - The main area to edit your files. You can open as many editors as you like side by side vertically and horizontally. Side Bar - Contains different views like the Explorer to assist you while working on your project. Status Bar - Information about the opened project and the files you edit. Activity Bar - Located on the far left-hand side, this lets you switch between views and gives you additional context-specific indicators, like the number of outgoing changes when Git is enabled. Panels - You can display different panels below the editor region for output or debug information, errors and warnings, or an integrated terminal. Panel can also be moved to the right for more vertical space. Each time you start VS Code, it opens up in the same state it was in when you last closed it. The folder, layout, and opened files are preserved. Microsoft provides a great tutorial to get you through the following sections if you would like to follow their guide. 1.4.2.1 VS Code Python Extension Install the Python extension for VS Code from the Visual Studio Marketplace. For additional details on installing extensions, see Extension Marketplace. The Python extension is named Python and it’s published by Microsoft. You can follow more of their tutorial at code.visualstudio.com. The third video and fourth video in this Python for Beginners set of videos done by Microsoft can also guide you through the Python Extension. 1.4.2.2 VS Code Interactive Python Window An open-source project called Jupyter is the standard method for interactive Python use for data science or scientific computing. However, there are some issues with its use in a development environment. VS Code has provided a way for us to have the best of Python and Jupyter Notebooks with their Python Interactive Window. You will need to install the jupyter python package using pip or pip3 for the interactive Python window to work. See the following section for guidance on using pip. Using the VS Code functionality, you will work with a standard .py file instead of the .ipynb extension typically used with jupyter notebooks. The Python extension in VS Code will recognize # %% as a cell or chunk of python code and add notebook options to ‘Run Cell’ as well as other actions. You can see the code example bellow with the image of the view in VS Code as an example. Microsoft’s documentation goes into more detail (https://code.visualstudio.com/docs/python/jupyter-support-py). To make the interactive window use more functional you can ctrl + , or cmd + , on a mac to open the settings. From there you can search ‘Send Selection to Interactive Window’ and make sure the box is checked. Now you will be able to use shift + return to send a selected chunk of code or an entire cell. # %% msg = &quot;Hello World&quot; print(msg) # %% msg = &quot;Hello again&quot; print(msg) 1.4.3 Package Management (pip) You can install a standard set of data science python packages using pip. However, there are some complications using pip on computers with multiple versions of Python. pip: If your path environment is correct, then a standard pip install [package] will work. This is how most packages direct users to install Python packages. pip3: If your OS has Python 2 and Python 3 installed, you may need to use pip3 install [package]. Force Python version: You can run the pip related to a specific Python installation by using python -m pip install [package]. Some may need to provide the path to their Python installation if your Python path environment is not understood. A few cautions about package management with pip. Never run sudo pip install. If you don’t have root permissions or the OS package manager doesn’t have the package you need, use pip install --user. If you want multiple versions of the same library to coexist, to do Python development, or to isolate dependencies for any other reason, use virtual environments. Generally, you will want to update pip before installing packages - python -m pip install --user --upgrade pip setuptools wheel Conda, poetry, and pipenv are three other options for package management. However, we will focus on using pip. 1.4.3.1 pip package installation examples If we wanted to install the numpy, pandas, xlrd, matplolib, and seaborn packages, we would use pip. Depending on your OS configuration, one of the following should work. Everything in your path is clean and you are an admin on your computer pip install numpy pandas xlrd matplotlib seaborn Everything in your path is clean and you want to install package for the user pip install --user numpy pandas xlrd matplotlib seaborn You have multiple Python versions installed you want to install package for the user without a need to understand which pip maps to which Python python -m pip install --user numpy pandas xlrd matplotlib seaborn 1.4.4 The Data Science Packages You’ll also need to install some Python packages. A Python package is a collection of functions, data, and documentation that extends the capabilities of base Python. Using packages is key to the successful use of Python for data science. The majority of the packages that you will learn in this book are related to the so-called tidyverse packages in R. There are attempts to port they tidyverse package process into Python. We are not showing the tools that recreate the tidyverse in Python but those that current Data Scientists use to do equivelent work in Python. You will notice that pandas is the primary tool with a few packages that come with base Python. Pandas user guide will be referenced heavily as we progress. R Tidyverse Package Python Package dplyr pandas tidyr pandas tibble pandas stringr string and re forcats pandas categorical data readr pandas io tools readxl xlrd and openpyxl ggplot2 seaborn, altair, plotnine purrr built in map function base R stats package statsmodels tidymodels scikit-learn R tensorflow tensorflow R keras keras rmarkdown jupyter Notice that the visualization space in Python does not have a force like ggplot2. Chris Moffitt provided an efficient visualization tools diagram to help Python users with this decision. The following packages will give us a broad data science toolset in Python. pip install numpy pandas xlrd matplotlib pip install seaborn plotnine altair vega_datasets pip install statsmodels scikit-learn pip install jupyter On your computer, type that above of code in the command line. The Python package manager pip will download the packages from PyPi and install them on to your computer. If you have problems installing, make sure that you are connected to the internet. You will not be able to use the functions and objects in a package until you load it with import. It is common in Python for each package to have a standard abbreviated name. For example, numpy is imported as ‘np’, and pandas is imported as ‘pd’ in the code chunk below. import numpy as np import pandas as pd import string import re import matplotlib import matplotlib.pyplot as plt import seaborn as sns from plotnine import * import altair as alt There are many other excellent packages that are not included here. 1.5 Running Python code The previous section showed you a couple of examples of running Python code. Code in the book looks like this: 1 + 2 #&gt; 3 #&gt; 3 If you run the same code in interactive python with VS Code, it will look like this: [1] 1 + 2 3 The Python Interactive window can be used as a standalone console with arbitrary code (with or without code cells). To use the window as a console, open it with the Python: Show Python Interactive window command from the Command Palette. You can then type in code, using Enter to go to a new line and Shift+Enter to run the code. There are two main differences. In your interactive window, you type after the [#]; we don’t show the line number in the book. In the book, output is commented out with #&gt;; in your console it appears directly after your code. These two differences mean that if you’re working with an electronic version of the book, you can easily copy code out of the book and into the interactive window. Throughout the book we use a consistent set of conventions to refer to code: Functions are in a code font and followed by parentheses, like sum(), or mean(). Other Python objects (like data or function arguments) are in a code font, without parentheses, like flights or x. 1.6 Getting help and learning more This book is not an island; there is no single resource that will allow you to master Python for data science. As you start to apply the techniques described in this book to your own data you will soon find questions that we do not answer. This section describes a few tips on how to get help, and to help you keep learning. If you get stuck, start with Google. Typically adding “python” to a query is enough to restrict it to relevant results: if the search isn’t useful, it often means that there aren’t any Python-specific results available. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it! Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = \"en\") and re-run the code; you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Start by spending a little time searching for an existing answer, including [python] to restrict your search to questions and answers that use Python. If you don’t find anything useful, prepare a minimal reproducible example or reprex. A good reprex makes it easier for other people to help you, and often you’ll figure out the problem yourself in the course of making it. There are three things you need to include to make your example reproducible: required packages, data, and code. Packages should be imported at the top of the script, so it’s easy to see which ones the example needs. This is a good time to check that you’re using the latest version of each package; it’s possible you’ve discovered a bug that’s been fixed since you installed the package. The easiest way to include data in a question is to create a minimal example to that recreates it. Try and find the smallest subset of your data that still reveals the problem. Spend a little bit of time ensuring that your code is easy for others to read: Make sure you’ve used spaces and your variable names are concise, yet informative. Use comments to indicate where your problem lies. Do your best to remove everything that is not related to the problem. The shorter your code is, the easier it is to understand, and the easier it is to fix. Finish by checking that you have actually made a reproducible example by starting a fresh Python session to run your script in. You should also spend some time preparing yourself to solve problems before they occur. Investing a little time in learning Python each day will pay off handsomely in the long run. One way is to follow what Wes McKinney, Garrett, and everyone else at RStudio are doing on the Pandas blog or the ossdata blog. This is where they post announcements about new packages and new IDE features. You might also want to follow Wes McKinney (@wesmckinn) on Twitter, or follow @code to keep up with new features in VS Code. To keep up with the data science community more broadly, we recommend reading https://planet.scipy.org/#. If you’re an active Twitter user, follow the #datascience hashtag. 1.7 Datasets access The data used in R for Data Science is generally within the R packages themselves. Many of the Python data science packages also come with datasets upon import. However, we will use the original datasets presented in R for Data Science. We have a data4python4ds GitHub data repository that contains all the datasets in varied file formats and all examples will use Pandas to read the data from GitHub. 1.8 Acknowledgements The text of this book is largely the product of Hadley and Garrett. J. Hathaway has ported the code and descriptions for using VS Code. You can see the original acknowledgements here. 1.9 Colophon An online version of this book is available at https://byuidatascience.github.io/python4ds/. It will continue to evolve. The source of the book is available at https://github.com/byuidatascience/python4ds. The book is powered by https://bookdown.org which makes it easy to turn R markdown files into HTML, PDF, and EPUB. This book was built with: sessioninfo::session_info() #&gt; ─ Session info ─────────────────────────────────────────────────────────────── #&gt; setting value #&gt; version R version 3.6.3 (2020-02-29) #&gt; os macOS Catalina 10.15.4 #&gt; system x86_64, darwin15.6.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz America/Boise #&gt; date 2020-05-13 #&gt; #&gt; ─ Packages ─────────────────────────────────────────────────────────────────── #&gt; package * version date lib source #&gt; assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.6.0) #&gt; backports 1.1.6 2020-04-05 [1] CRAN (R 3.6.3) #&gt; bookdown 0.18 2020-03-05 [1] CRAN (R 3.6.0) #&gt; broom 0.5.6 2020-04-20 [1] CRAN (R 3.6.2) #&gt; cellranger 1.1.0 2016-07-27 [1] CRAN (R 3.6.0) #&gt; cli 2.0.2 2020-02-28 [1] CRAN (R 3.6.0) #&gt; codetools 0.2-16 2018-12-24 [1] CRAN (R 3.6.3) #&gt; colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.6.0) #&gt; crayon 1.3.4 2017-09-16 [1] CRAN (R 3.6.0) #&gt; DBI 1.0.0 2018-05-02 [1] CRAN (R 3.6.0) #&gt; dbplyr 1.4.2 2019-06-17 [1] CRAN (R 3.6.0) #&gt; digest 0.6.25 2020-02-23 [1] CRAN (R 3.6.0) #&gt; dplyr * 0.8.5 2020-03-07 [1] CRAN (R 3.6.0) #&gt; ellipsis 0.3.0 2019-09-20 [1] CRAN (R 3.6.0) #&gt; evaluate 0.14 2019-05-28 [1] CRAN (R 3.6.0) #&gt; fansi 0.4.1 2020-01-08 [1] CRAN (R 3.6.0) #&gt; forcats * 0.5.0 2020-03-01 [1] CRAN (R 3.6.1) #&gt; fs 1.4.1 2020-04-04 [1] CRAN (R 3.6.3) #&gt; generics 0.0.2 2018-11-29 [1] CRAN (R 3.6.0) #&gt; ggplot2 * 3.3.0 2020-03-05 [1] CRAN (R 3.6.1) #&gt; glue 1.4.0 2020-04-03 [1] CRAN (R 3.6.3) #&gt; gtable 0.3.0 2019-03-25 [1] CRAN (R 3.6.0) #&gt; haven 2.2.0 2019-11-08 [1] CRAN (R 3.6.0) #&gt; hms 0.5.3 2020-01-08 [1] CRAN (R 3.6.0) #&gt; htmltools 0.4.0 2019-10-04 [1] CRAN (R 3.6.0) #&gt; httr 1.4.1 2019-08-05 [1] CRAN (R 3.6.0) #&gt; jsonlite 1.6.1 2020-02-02 [1] CRAN (R 3.6.0) #&gt; knitr 1.28 2020-02-06 [1] CRAN (R 3.6.0) #&gt; lattice 0.20-38 2018-11-04 [1] CRAN (R 3.6.3) #&gt; lifecycle 0.2.0 2020-03-06 [1] CRAN (R 3.6.0) #&gt; lubridate 1.7.4 2018-04-11 [1] CRAN (R 3.6.0) #&gt; magrittr 1.5 2014-11-22 [1] CRAN (R 3.6.0) #&gt; modelr 0.1.5 2019-08-08 [1] CRAN (R 3.6.0) #&gt; munsell 0.5.0 2018-06-12 [1] CRAN (R 3.6.0) #&gt; nlme 3.1-144 2020-02-06 [1] CRAN (R 3.6.3) #&gt; pillar 1.4.4 2020-05-05 [1] CRAN (R 3.6.2) #&gt; pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 3.6.0) #&gt; purrr * 0.3.4 2020-04-17 [1] CRAN (R 3.6.2) #&gt; R6 2.4.1 2019-11-12 [1] CRAN (R 3.6.1) #&gt; Rcpp 1.0.4.6 2020-04-09 [1] CRAN (R 3.6.3) #&gt; readr * 1.3.1 2018-12-21 [1] CRAN (R 3.6.0) #&gt; readxl 1.3.1 2019-03-13 [1] CRAN (R 3.6.0) #&gt; reprex 0.3.0 2019-05-16 [1] CRAN (R 3.6.0) #&gt; reticulate 1.14 2019-12-17 [1] CRAN (R 3.6.1) #&gt; rlang 0.4.6 2020-05-02 [1] CRAN (R 3.6.2) #&gt; rmarkdown 2.1.3 2020-05-07 [1] Github (rstudio/rmarkdown@d7e1bda) #&gt; rstudioapi 0.11 2020-02-07 [1] CRAN (R 3.6.0) #&gt; rvest 0.3.5 2019-11-08 [1] CRAN (R 3.6.0) #&gt; scales 1.1.0 2019-11-18 [1] CRAN (R 3.6.0) #&gt; sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.6.0) #&gt; stringi 1.4.6 2020-02-17 [1] CRAN (R 3.6.0) #&gt; stringr * 1.4.0 2019-02-10 [1] CRAN (R 3.6.0) #&gt; tibble * 3.0.1 2020-04-20 [1] CRAN (R 3.6.2) #&gt; tidyr * 1.0.3 2020-05-07 [1] CRAN (R 3.6.2) #&gt; tidyselect 1.0.0 2020-01-27 [1] CRAN (R 3.6.0) #&gt; tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 3.6.0) #&gt; vctrs 0.2.4 2020-03-10 [1] CRAN (R 3.6.0) #&gt; withr 2.2.0 2020-04-20 [1] CRAN (R 3.6.2) #&gt; xfun 0.13 2020-04-13 [1] CRAN (R 3.6.2) #&gt; xml2 1.3.2 2020-04-23 [1] CRAN (R 3.6.2) #&gt; yaml 2.2.1 2020-02-01 [1] CRAN (R 3.6.0) #&gt; #&gt; [1] /Library/Frameworks/R.framework/Versions/3.6/Resources/library "],
["explore-intro.html", "2 Introduction", " 2 Introduction The goal of the first part of this book is to get you up to speed with the basic tools of data exploration as quickly as possible. Data exploration is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again. The goal of data exploration is to generate many promising leads that you can later explore in more depth. In this part of the book you will learn some useful tools that have an immediate payoff: Visualisation is a fun place to start with Python programming, because the payoff is so clear: you get to make elegant and informative plots that help you understand data. In data visualisation you’ll dive into visualisation, learning the basic structure of a pandas plot, and powerful techniques for turning data into plots. Visualisation alone is typically not enough, so in data transformation you’ll learn the key verbs that allow you to select important variables, filter out key observations, create new variables, and compute summaries. Finally, in exploratory data analysis, you’ll combine visualisation and transformation with your curiosity and scepticism to ask and answer interesting questions about data. Modelling is an important part of the exploratory process, but you don’t have the skills to effectively learn or apply it yet. We’ll come back to it in modelling, once you’re better equipped with more data wrangling and programming tools. Nestled among these three chapters that teach you the tools of exploration are three chapters that focus on your Python workflow. In workflow: basics, workflow: scripts, and workflow: projects you’ll learn good practices for writing and organising your Python code. These will set you up for success in the long run, as they’ll give you the tools to stay organised when you tackle real projects. "],
["data-visualisation.html", "3 Data visualisation 3.1 Introduction 3.2 First steps 3.3 Aesthetic mappings 3.4 Common problems 3.5 Facets 3.6 Geometric objects 3.7 Statistical transformations 3.8 Position adjustments 3.9 Coordinate systems (maps) 3.10 The layered grammar of graphics 3.11 Altair’s grammar of graphics", " 3 Data visualisation 3.1 Introduction “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey This chapter will teach you how to visualise your data using Altair. Python has several systems for making graphs, but altiar is one of the most elegant and versatile. Altair implements the declarative visualization much like the grammar of graphics, a coherent system for describing and building graphs. With altair, you can do more faster by learning one system and applying it in many places. If you’d like to learn more about Altair before you start, I’d recommend reading “Altair: Interactive Statistical Visualizations for Python”, https://joss.theoj.org/papers/10.21105/joss.01057.pdf. We should note that we are building this book using R with the package bookdown. Rendering Altair graphics using a python chunk is not straight forward but is not important for our use in VS Code. In VS Code the example chunks will render in the interactive Python viewer automatically. The following R code chunks show how we are rendering the Altair graphics in this book. Thanks to ijlyttle for his GitHub Gist. # ```{R, echo=FALSE} # vegawidget::as_vegaspec(py$chart$to_json()) # ``` # For Python examples that show chart.save() #```{r, message = FALSE, echo=FALSE} #knitr::include_graphics(&quot;screenshots/chartp_chartleft.png&quot;) #``` 3.1.1 Prerequisites This chapter focusses on Altair. Language has been shifted using the material from Altair’s materials. To access the datasets, help pages, and functions that we will use in this chapter, load the Python data science tools by running this code: import pandas as pd import altair as alt If you run this code and get the error message “No module named ‘altair’” or “No module named ‘pandas’”, you’ll need to first install them. python -m pip install pandas altair You only need to install a package once, but you need to reload it every time you start a new session. 3.1.2 Altair data management When specifying data in Altair, we can use pandas DataFrame objects or other Altair options. According to the Altair documentation, the use of a pandas DataFrame will prompt Altair to store the entire data set in JSON format in the chart object. You should be carefully creating Altair specs with all the data in the chart object for use in HTML or Jupyter Notebooks. If you try to plot a data set with more than 5000 rows, Altair will return a maxRowsError. In this book, we will save the Altair chart as a ‘.png’ file to avoid dealing with large stored data in our ‘.html’ files. We have elected to use the Local Filesystem approach proposed by Altair. They do note that the filesystem approach may not work on some cloud-based Jupyter notebook services. alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) 3.2 First steps Let’s use our first graph to answer a question: Do cars with big engines use more fuel than cars with small engines? You probably already have an answer, but try to make your answer precise. What does the relationship between engine size and fuel efficiency look like? Is it positive? Negative? Linear? Nonlinear? 3.2.1 The mpg data frame You can test your answer with the mpg data frame found in ggplot2 (aka ggplot2::mpg). A data frame is a rectangular collection of variables (in the columns) and observations (in the rows). The ‘mpg’ data contains observations collected by the US Environmental Protection Agency on 38 models of car. We will identify the ‘mpg’ data using mpg for the remainder of this introduction. mpg = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv&quot;) Among the variables in mpg are: displ, a car’s engine size, in litres. hwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distance. To learn more about mpg, read informat at data4python4ds. 3.2.2 Creating an Altair plot To plot mpg, run this code to put displ on the x-axis and hwy on the y-axis: chart = (alt.Chart(mpg). encode( x=&#39;displ&#39;, y=&#39;hwy&#39;). mark_circle() ) The plot shows a negative relationship between engine size (displ) and fuel efficiency (hwy). In other words, cars with big engines use more fuel. Does this confirm or refute your hypothesis about fuel efficiency and engine size? With Altair, you begin a plot with the function Chart(). Chart() creates a Chart object that you can add layers to. The only argument of Chart() is the dataset to use in the graph. So Chart(mpg) creates an Chart object upon which we can marks. You complete your graph by adding one or more marks to Chart(). The attribute mark_point() adds a layer of points to your plot, which creates a scatterplot. Altair comes with many mark methods that each add a different type of layer to a plot. You’ll learn a whole bunch of them throughout this chapter. Each mark method in Altair has an encode() attribute. This defines how variables in your dataset are encoded to visual properties. The encode() method is always paired with x and y arguments to specify which variables to map to the x and y axes. Altair looks for the encoded variables in the data argument, in this case, mpg. For pandas dataframes, Altair automatically determines the appropriate data type for the mapped column. 3.2.3 A graphing template Let’s turn this code into a reusable template for making graphs with ggplot2. To make a graph, replace the bracketed sections in the code below with a dataset, a geom function, or a collection of mappings. (alt.Chart(&lt;DATA&gt;). &lt;mark_*().&gt; encode(&lt;ENCODINGS&gt;)) The rest of this chapter will show you how to complete and extend this template to make different types of graphs. We will begin with the &lt;ENCODINGS&gt; component. 3.2.4 Exercises Run Chart(mpg).mark_points(). What do you see? How many rows are in mpg? How many columns? What does the drv variable describe? Make a scatterplot of hwy vs cyl. What happens if you make a scatterplot of class vs drv? Why is the plot not useful? 3.3 Aesthetic mappings “The greatest value of a picture is when it forces us to notice what we never expected to see.” — John Tukey In the plot below, one group of points (highlighted in red) seems to fall outside of the linear trend. These cars have a higher mileage than you might expect. How can you explain these cars? Let’s hypothesize that the cars are hybrids. One way to test this hypothesis is to look at the class value for each car. The class variable of the mpg dataset classifies cars into groups such as compact, midsize, and SUV. If the outlying points are hybrids, they should be classified as compact cars or, perhaps, subcompact cars (keep in mind that this data was collected before hybrid trucks and SUVs became popular). You can add a third variable, like class, to a two dimensional scatterplot by mapping it to an encoding. An encoding is a visual property of the objects in your plot. Encodings include things like the size, the shape, or the color of your points. You can display a point (like the one below) in different ways by changing the values of its encoded properties. Since we already use the word “value” to describe data, let’s use the word “level” to describe encoded properties. Here we change the levels of a point’s size, shape, and color to make the point small, triangular, or blue: You can convey information about your data by mapping the encodings in your plot to the variables in your dataset. For example, you can map the colors of your points to the class variable to reveal the class of each car. chart = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color = &quot;class&quot; ) ) (We don’t prefer British English, like Hadley, so don’t use colour instead of color.) To map an encoding to a variable, associate the name of the encoding to the name of the variable inside encode(). Altair will automatically assign a unique level of the encoding (here a unique color) to each unique value of the variable, a process known as scaling. Altair will also add a legend that explains which levels correspond to which values. The colors reveal that many of the unusual points are two-seater cars. These cars don’t seem like hybrids, and are, in fact, sports cars! Sports cars have large engines like SUVs and pickup trucks, but small bodies like midsize and compact cars, which improves their gas mileage. In hindsight, these cars were unlikely to be hybrids since they have large engines. In the above example, we mapped class to the color encoding, but we could have mapped class to the size encoding in the same way. In this case, the exact size of each point would reveal its class affiliation. Mapping an unordered variable (class) to an ordered aesthetic (size) is not a good idea. chart = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, size = &quot;class&quot; ) ) Or we could have mapped class to the opacity encoding, which controls the transparency of the points, or to the shape encoding, which controls the shape of the points. # First chart1 = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, opacity = &quot;class&quot; ) ) # Second chart2 = (alt.Chart(mpg). mark_point(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, shape = &quot;class&quot; ) ) chart1.save(&quot;screenshots/altair_opacity.png&quot;) #&gt; WARN Channel opacity should not be used with an unsorted discrete field. chart2.save(&quot;screenshots/altair_shape.png&quot;) Altair will only use 8 shapes for one chart. Charting more than 8 shapes is not recommended as the shapes simply recycle. For each encoding, you use encode() to associate the name of the encoding with a variable to display. The encode() function gathers together each of the encoded mappings used by a layer and passes them to the layer’s mapping argument. The syntax highlights a useful insight about x and y: the x and y locations of a point are themselves encodings, visual properties that you can map to variables to display information about the data. Once you map an encoding, Altair takes care of the rest. It selects a reasonable scale to use with the encoding, and it constructs a legend that explains the mapping between levels and values. For x and y aesthetics, Altair does not create a legend, but it creates an axis line with tick marks and a label. The axis line acts as a legend; it explains the mapping between locations and values. You can also configure the encoding properties of your mark manually. For example, we can make all of the points in our plot blue: chart = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color = alt.value(&quot;blue&quot;) ) ) Here, the color doesn’t convey information about a variable, but only changes the appearance of the plot. To set an encoding manually, use alt.value() by name as an argument of your encode() function; i.e. the value goes inside of alt.value(). You’ll need to pick a level that makes sense for that encoding: The name of a color as a character string. The size of a point in pixels. The shape of a point as a character string. Note that only a limited set of mark properties can be bound to encodings, so for some (e.g. fillOpacity, strokeOpacity, etc.) the encoding approach using alt.value() is not available. Encoding settings will always override local or global configuration settings. There are other methods for manually encoding properties as explained in the Altair documentation 3.3.1 Exercises Which variables in mpg are categorical? Which variables are continuous? How can you see this information when you run mpg? (Hint mpg.dtypes) Map a continuous variable to color, size, and shape. How do these aesthetics behave differently for categorical vs. continuous variables? What happens if you map the same variable to multiple encodings? What does the stroke encoding do? What shapes does it work with? (Hint: use mark_point()) 3.4 Common problems As you start to run Python code, you’re likely to run into problems. Don’t worry — it happens to everyone. I have been writing Python code for months, and every day I still write code that doesn’t work! Start by carefully comparing the code that you’re running to the code in the book. Python is extremely picky, and a misplaced character can make all the difference. Make sure that every ( is matched with a ) and every \" is paired with another \". One common problem when creating Altair graphics as shown in this book, is to put the () in the wrong place: the ( comes before the alt.chart() command and the ) has to come at the end of the command. For example the code below works in Python. alt.Chart(mpg).mark_circle().encode(x = &quot;displ&quot;, y = &quot;hwy&quot;) However, the complexity of the more details graphics necessicates placing the code on multiple lines. When using multiple lines we need the enclosing (). Make sure you haven’t accidentally excluded a ( or ) like this (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;) or placed the () incorrectly like this (chart = alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;) ) If you’re still stuck, try the help. You can get help about any Altair function from their website - https://altair-viz.github.io/, or hovering over the function name in VS Code. If that doesn’t help, carefully read the error message. Sometimes the answer will be buried there! But when you’re new to Python, the answer might be in the error message but you don’t yet know how to understand it. Another great tool is Google: try googling the error message, as it’s likely someone else has had the same problem, and has gotten help online. 3.5 Facets One way to add additional variables is with encodings. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data. To facet your plot by a single variable, use facet(). The first argument of facet() is . The variable that you pass to facet_wrap() should be discrete. chart_f = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). facet( facet = &quot;class&quot;, columns = 4 ) ) chart_f.save(&quot;screenshots/altair_facet_1.png&quot;) To facet your plot on the combination of two variables, The first argument of facet() is also column and the second is row. This time the formula should contain two variable names. chart_f2 = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). facet( column = &quot;drv&quot;, row = &quot;cyl&quot; ) ) chart_f2.save(&quot;screenshots/altair_facet_2.png&quot;) #&gt; WARN row encoding should be discrete (ordinal / nominal / binned). If you prefer to not facet in the rows or columns dimension, simply remove that facet argument. You can read more about compound charts in the Altair documentation. 3.5.1 Exercises What happens if you facet on a continuous variable? What do the empty cells in plot with facet(column = \"drv\", row = \"cyl\") mean? How do they relate to this plot? (alt.Chart(mpg). mark_circle(). encode( x = &quot;drv&quot;, y = &quot;cyl&quot;) ) What plots does the following code make? What does . do? (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;). facet(column = &quot;drv&quot;) ) (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;). facet(row = &quot;cyl&quot;) ) Take the first faceted plot in this section: What are the advantages to using faceting instead of the colour aesthetic? What are the disadvantages? How might the balance change if you had a larger dataset? When using facet() you should usually put the variable with more unique levels in the columns. Why? 3.6 Geometric objects How are these two plots similar? chartp = (alt.Chart(mpg). mark_circle(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chartf = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chartp.save(&quot;screenshots/altair_basic_points.png&quot;) chartf.save(&quot;screenshots/altair_smooth_line.png&quot;) Both plots contain the same x variable, the same y variable, and both describe the same data. But the plots are not identical. Each plot uses a different visual object to represent the data. In Altair syntax, we say that they use different marks. A mark is the geometrical object that a plot uses to represent data. People often describe plots by the type of mark that the plot uses. For example, bar charts use bar marks, line charts use line marks, boxplots use boxplot marks, and so on. Scatterplots break the trend; they use the point mark. As we see above, you can use different marks to plot the same data. The first plot uses the point mark, and the second plot uses the line mark, a smooth line fitted to the data is calculated using a transformation. To change the mark in your plot, change the mark function that you add to Chart(). Every mark function in Altair has encode arguments. However, not every encoding works with every mark. You could set the shape of a point, but you couldn’t set the “shape” of a line. On the other hand, you could set the type of line. mark_line() will draw a different line, with a different strokeDash, for each unique value of the variable that you map to strokeDash. chartl = (alt.Chart(mpg). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line(). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, strokeDash = &quot;drv&quot; ) ) chartl.save(&quot;screenshots/altair_dashed_lines.png&quot;) Here mark_line() separates the cars into three lines based on their drv value, which describes a car’s drivetrain. One line describes all of the points with a 4 value, one line describes all of the points with an f value, and one line describes all of the points with an r value. Here, 4 stands for four-wheel drive, f for front-wheel drive, and r for rear-wheel drive. If this sounds strange, we can make it more clear by overlaying the lines on top of the raw data and then coloring everything according to drv. Notice that this plot contains two marks in the same graph! If this makes you excited, buckle up. We will learn how to place multiple marks on the same chart very soon. Altair provides about 15 marks. The best way to get a comprehensive overview is the Altair marks page, which you can find at https://altair-viz.github.io/user_guide/marks.html. Many marks, like mark_line(), use a single mark object to display multiple rows of data. For these marks, you can set the detail encoding to a categorical variable to draw multiple objects. Altair will draw a separate object for each unique value of the detail variable. In practice, Altair will automatically group the data for these marks whenever you map an encoding to a discrete variable (as in the strokeDash example). It is convenient to rely on this feature because the detail encoding by itself does not add a legend or distinguishing features to the marks. chartleft = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chartmiddle = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, detail = &quot;drv&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) chartright = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color=alt.Color(&quot;drv&quot;, legend=None) ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) chartleft.save(&quot;screenshots/altair_chartleft.png&quot;) chartmiddle.save(&quot;screenshots/altair_chartmiddle.png&quot;) chartright.save(&quot;screenshots/altair_chartright.png&quot;) To display multiple marks in the same plot, you can used layered charts as shown in the example below that uses the chartleft object from the above code chunk: chartp = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ). mark_circle() ) chart = chartp + chartleft chart.save(&quot;screenshots/altair_chartcombine.png&quot;) This, however, introduces some duplication in our code. Imagine if you wanted to change the y-axis to display cty instead of hwy. You’d need to change the variable in two places, and you might forget to update one. You can avoid this type of repetition by passing a set of encodings to a base alt.Chart(). Altair will treat these encodings as global encodings that apply to each mark layer in the layered chart. In other words, this code will produce the same plot as the previous code: base =(alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart = base.mark_circle() + base.transform_loess(&quot;displ&quot;, &quot;hwy&quot;).mark_line() chart.save(&quot;screenshots/altair_combine_clean.png&quot;) If you place encodings in an encode function, Altair will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the base encodings for that layer only. This makes it possible to display different aesthetics in different layers. Alatair automatically chooses useful plot settings and chart configurations to allow you to think about data instead of the programming mechanics of the chart. You can review their guidance on customizing visualizations to see the varied ways to change the look of your graphic. base =(alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart = base.encode(color = &quot;drv&quot;).mark_circle() + base.transform_loess(&quot;displ&quot;, &quot;hwy&quot;).mark_line() chart.save(&quot;screenshots/altair_combine_clean_color.png&quot;) You can use the same idea to specify different data for each layer. Here, our smooth line displays just a subset of the mpg dataset, the subcompact cars. The local data argument in geom_smooth() overrides the global data argument in ggplot() for that layer only. #column name of class does not work nicely with Altair filter. base = (alt.Chart(mpg.rename(columns = {&quot;class&quot;: &quot;class1&quot;})). encode( x = &quot;displ&quot;, y = &quot;hwy&quot; ) ) chart_smooth_sub = (base. transform_filter( alt.datum.class1 == &quot;subcompact&quot; ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;). mark_line() ) chart = base.encode(color = &quot;class1&quot;).mark_circle() + chart_smooth_sub chart.save(&quot;screenshots/altair_combine_clean_color_filter.png&quot;) (You’ll learn how pandas filter works in the chapter on data transformations. To keep the same base chart, filtering is done with Altair in this example: for now, just know that this command selects only the subcompact cars.) 3.6.1 Exercises What geom would you use to draw a line chart? A boxplot? A histogram? An area chart? What does legend=None in alt.Color() do? What happens if you remove it? Why do you think I used it earlier in the chapter? Recreate the Python code necessary to generate the following graphs. 3.7 Statistical transformations Next, let’s take a look at a bar chart. Bar charts seem simple, but they are interesting because they reveal something subtle about plots. Consider a basic bar chart, as drawn with mark_bar(). The following chart displays the total number of diamonds in the diamonds dataset, grouped by cut. The diamonds dataset comes in ggplot2 R package and can be used in Python using the following Python command. Note that we also need to use pandas to format a few of the columns as ordered categorical to have the diamonds DataFrame act like it does in R. diamonds = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot;) diamonds[&#39;cut&#39;] = pd.Categorical(diamonds.cut, ordered = True, categories = [&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot; ]) diamonds[&#39;color&#39;] = pd.Categorical(diamonds.color, ordered = True, categories = [&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;]) diamonds[&#39;clarity&#39;] = pd.Categorical(diamonds.clarity, ordered = True, categories = [&quot;I1&quot;, &quot;SI2&quot;, &quot;SI1&quot;, &quot;VS2&quot;, &quot;VS1&quot;, &quot;VVS2&quot;, &quot;VVS1&quot;, &quot;IF&quot;]) It contains information about ~54,000 diamonds, including the price, carat, color, clarity, and cut of each diamond. The chart shows that more diamonds are available with high quality cuts than with low quality cuts. chart = (alt.Chart(diamonds). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count():Q&quot;) ). mark_bar(). properties(width = 400) ) chart.save(&quot;screenshots/altair_diamond_bar.png&quot;) On the x-axis, the chart displays cut, a variable from diamonds. On the y-axis, it displays count, but count is not a variable in diamonds! Many graphs, like scatterplots, plot the raw values of your dataset. Other graphs, like bar charts, calculate new values to plot: bar charts, histograms, and frequency polygons bin your data and then plot bin counts, the number of points that fall in each bin. smoothers fit a model to your data and then plot predictions from the model. boxplots compute a robust summary of the distribution and then display a specially formatted box. The algorithm used to calculate new values for a graph is called a transform, short for transformation. The figure below describes how this process works with mark_bar(). You must explicitely define the transformation a mark uses through transformations using alt.Y() or alt.X() function. For example, mark_bar() requires the y encoding alt.Y(\"count():Q\"). A histogram is created using mark_bar() with transformations on both the x and y axes. The bin argument accepts a boolean or an alt.Bin() function where the argument maxbins can be used - bin=alt.Bin(maxbins=100). chart = (alt.Chart(diamonds). encode( x =alt.X(&quot;price&quot;, bin=True), y =alt.Y(&quot;count()&quot;) ). mark_bar() ) chart.save(&quot;screenshots/altair_histogram.png&quot;) For more complicated transformations Altair provides transform functions. We saw one of these transforms previously when we used mark_line() to describe each drive type. If you are working with pandas DataFrames then you may want to do these transformations using pandas. Altair’s transformations can be used with DataFrames as well as JSON files or URL pointers to CSV files. chartright = (alt.Chart(mpg). encode( x = &quot;displ&quot;, y = &quot;hwy&quot;, color=alt.Color(&quot;drv&quot;, legend=None) ). transform_loess(&quot;displ&quot;, &quot;hwy&quot;, groupby = [&quot;drv&quot;]). mark_line() ) Finally, mark_boxplot() is available which does the statistical transformations for you after you specify the encodings for the x and y axes. chart = (alt.Chart(diamonds). encode( y =&quot;price&quot;, x =&quot;cut&quot; ). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_boxplot.png&quot;) 3.8 Position adjustments There’s one more piece of magic associated with bar charts. You can colour a bar chart using either the stroke aesthetic, or, more usefully, color: chart_left = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), stroke = &quot;cut&quot; ). properties(width = 200) ) chart_right = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), color = &quot;cut&quot; ). properties(width = 200) ) chart_left.save(&quot;screenshots/altair_bar_linecolor.png&quot;) chart_right.save(&quot;screenshots/altair_bar_fillcolor.png&quot;) Note what happens if you map the color encoding to another variable, like clarity: the bars are automatically stacked. Each colored rectangle represents a combination of cut and clarity. chart = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;), color = &quot;clarity&quot; ). properties(width = 200) ) chart.save(&quot;screenshots/stacked_barchart.png&quot;) The stacking is performed automatically by mark_bar(). If you don’t want a stacked bar chart, you can use use the stack argument in alt.Y() one of three other options: \"identity\", \"dodge\" or \"fill\". position = \"identity\" will place each object exactly where it falls in the context of the graph. This is not very useful for bars, because it overlaps them. To see that overlapping we either need to make the bars slightly transparent by setting alpha to a small value, or completely transparent by setting fill = NA. chart_left = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=None), color = &quot;clarity&quot;, opacity = alt.value(1/5) ). properties(width = 200) ) chart_right = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=None), stroke = &quot;clarity&quot;, color = alt.value(&quot;none&quot;) ). properties(width = 200) ) chart_left.save(&quot;screenshots/altair_nostack_opacity.png&quot;) chart_right.save(&quot;screenshots/altair_nostack_lines.png&quot;) position = \"fill\" works like stacking, but makes each set of stacked bars the same height. This makes it easier to compare proportions across groups. chart = (alt.Chart(diamonds). mark_bar(). encode( x = &quot;cut&quot;, y = alt.Y(&quot;count()&quot;, stack=&#39;normalize&#39;), color = &quot;clarity&quot; ). properties(width = 200) ) chart.save(&quot;screenshots/altair_normalize_bar.png&quot;) knitr::include_graphics(&quot;screenshots/altair_normalize_bar.png&quot;) Placing overlapping objects directly beside one another is done by using the column encoding. This makes it easier to compare individual values with a common baseline. chart = (alt.Chart(diamonds). mark_bar(). encode( x=&#39;clarity&#39;, y=alt.Y(&#39;count()&#39;), color=&#39;clarity&#39;, column=&#39;cut&#39; ) ) chart.save(&quot;screenshots/altair_position_dodge.png&quot;) Altair does not have a simple way to add random jitter to points using an encoding or simple argument to alt.X() or alt.Y(). Altair can create a jittered point plot, also called a stripplot. However, it is not as straight forward. We should note that Altair also does not provide piechart or donut chart marks as well. 3.9 Coordinate systems (maps) Coordinate systems are generally on an x and y axis in Altair. This coordinate system is the Cartesian coordinate system, where the x and y positions act independently to determine the location of each point. Maps are on a Cartesian coordinate system, but their behavior is different as they require a transformation or projection of the globe to an x and y Cartesian plane. Visualizing data on maps can get complicated quickly. Altair has a some mapping marks available for charts. The data formats for plotting with maps also get more involved. You can use the gpdvega package to help with spatial data in Altair. Many Python users us geopandas or Shapely to handle spatial data in Python. PennState has a nice list of spatial tools for Python. 3.10 The layered grammar of graphics In the previous sections, you learned much more than how to make scatterplots, bar charts, and boxplots. You learned a foundation that you can use to make any type of plot with ggplot2. To see this, let’s add position adjustments, stats, coordinate systems, and faceting to our code template: (alt.Chart(&lt;DATA&gt;). encoding( ).facet( column = &lt;FACET VARIABLE&gt; ). &lt;TRANFORM_FUNCTION&gt;(). &lt;MARK_FUNCTION&gt;(). properties( width = , height = ) ) Altair’s general template takes four main parameters - alt.Chart(), mark_*(), encode(), and facet(). In practice, only need to supply the first three parameters to make a chart because Altair will provide useful defaults for everything except the data, the encodings, and the mark function. The parameters in the template compose the grammar of graphics, a formal system for building plots. The grammar of graphics is based on the insight that you can uniquely describe any plot as a combination of a dataset, a mark, a set of encodings, a transformation, a position adjustment, and a faceting scheme. To see how this works, consider how you could build a basic plot from scratch: you could start with a dataset and then transform it into the information that you want to display (with a stat). Next, you could choose a geometric object to represent each observation in the transformed data. You could then use the aesthetic properties of the geoms to represent variables in the data. You would map the values of each variable to the levels of an aesthetic. You’d then select a coordinate system to place the geoms into. You’d use the location of the objects (which is itself an aesthetic property) to display the values of the x and y variables. At that point, you would have a complete graph, but you could further adjust the positions of the geoms within the coordinate system (a position adjustment) or split the graph into subplots (faceting). You could also extend the plot by adding one or more additional layers, where each additional layer uses a dataset, a geom, a set of mappings, a stat, and a position adjustment. You could use this method to build any plot that you imagine. In other words, you can use the code template that you’ve learned in this chapter to build hundreds of thousands of unique plots. 3.11 Altair’s grammar of graphics Plotnine is a Python implementation of ggplot2 in R’s grammar of graphics using matplotlib as the plotting backend. Altair’s implementation of the grammar of graphics, much like ggplot2 or plotnine at a high level. However, it uses the Vega-Lite grammar of graphics constructs and plotting backend. Below are some useful links that will help you dig deeper into the Altair implementation of the grammar of graphics. Altair mark guidance Altair encoding guidance Altair facet guidance Altair tranformations guidance Altair chart customization Altair sorting barcharts Altair theme editing Altair Encoding data types "],
["workflow-basics.html", "4 Workflow: basics 4.1 Coding basics 4.2 What’s in a name? 4.3 Calling functions 4.4 Exercises", " 4 Workflow: basics You now have some experience running Python code. I didn’t give you many details, but you’ve obviously figured out the basics, or you would’ve thrown this book away in frustration! Frustration is natural when you start programming in Python, because it is such a stickler for punctuation, and even one character out of place will cause it to complain. But while you should expect to be a little frustrated, take comfort in that it’s both typical and temporary: it happens to everyone, and the only way to get over it is to keep trying. Before we go any further, let’s make sure you’ve got a solid foundation in running Python code, and that you know about some of the most helpful VS Code features. 4.1 Coding basics Let’s review some basics we’ve so far omitted in the interests of getting you plotting as quickly as possible. You can use Python as a calculator: 1 / 200 * 30 (59 + 73 + 2) / 3 You can create new objects with =: x = 3 * 4 All Python statements where you create objects, assignment statements, have the same form: object_name = value When reading that code say “object name gets value” in your head. You will make lots of assignments in data science Python programming. It is also good code formatting practice to wrap = with spaces. Code is miserable to read on a good day, so giveyoureyesabreak and use spaces. 4.2 What’s in a name? Object names must start with a letter, and can only contain letters, numbers, and _. You cannot use . like R. You want your object names to be descriptive, so you’ll need a convention for multiple words. Python coding conventions recommend snake_case where you separate lowercase words with _. i_use_snake_case otherPeopleUseCamelCase some.people.use.periods And_aFew.People_RENOUNCEconvention We’ll come back to code style later, in [functions]. You can inspect an object by typing its name: x #&gt; 12 Make another assignment: this_is_a_really_long_name = 2.5 To inspect this object, try out VS Codes completion facility: type “this_”, pause, add characters until you have a unique prefix, then press shift + return. Make yet another assignment: python_rocks = 2 ^ 3 Let’s try to inspect it: python_rock #&gt; --------------------------------------------------------------------------- #&gt; NameError Traceback (most recent call last) #&gt; ~.../python4ds_practice.py in #&gt; ----&gt; 1 python_rock #&gt; NameError: name &#39;python_rock&#39; is not defined Python_rocks #&gt; --------------------------------------------------------------------------- #&gt; NameError Traceback (most recent call last) #&gt; ~.../python4ds_practice.py in #&gt; ----&gt; 1 Python_rocks #&gt; NameError: name &#39;Python_rocks&#39; is not defined There’s an implied contract between you and Python: it will do the tedious computation for you, but in return, you must be completely precise in your instructions. Typos matter. Case matters. 4.3 Calling functions Python does not have a large collection of built-in mathematical and statistical functions. You will need to use pandas, numpy, scikit-learn, and statsmodels to get the suite of functions for working and modeling with data. import pandas as pd import numpy as np import sklearn as sk import statsmodels.api as sm Functions are called like this: &lt;AS PACKAGE NAME&gt;.function_name(arg1 = val1, arg2 = val2, ...) Let’s try using np.arange() which returns regular arangement of numbers and, while we’re at it, learn more helpful features of intellisense in VS code. Type np.ar and pause. A popup shows you possible completions. Specify np.arange() by typing more (a “ange”) to disambiguate, or by using ↑/↓ arrows to select. If you hover over np.arange or type np.arange() a floating tooltip pops up, reminding you of the function’s arguments and purpose. If you want more help, can scroll through the arguments tool tip with your mouse. VS Code will add matching opening (() and closing ()) parentheses for you. Type the arguments 1, 10 and hit return. np.arange(1,10) #&gt; array([1, 2, 3, 4, 5, 6, 7, 8, 9]) Type this code and notice you get similar assistance with the paired quotation marks: x = &quot;hello world&quot; Quotation marks and parentheses must always come in a pair. VS Code does its best to help you, but it’s still possible to mess up and end up with a mismatch. Now look at your Python interactive environment in VS Code in the top toolbar by selecting the icon circled in red : Here you can see all of the objects that you’ve created. 4.4 Exercises Why does this code not work? my_variable &lt;- 10 #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_variable&#39; is not defined #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; my_varıable #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): NameError: name &#39;my_varıable&#39; is not defined #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; Look carefully! (This may seem like an exercise in pointlessness, but training your brain to notice even the tiniest difference will pay off when programming.) Navigate to Visual Studio Code keyboard shortcuts by going to the menu under File &gt; Preferences &gt; Keyboard Shortcuts. (Code &gt; Preferences &gt; Keyboard Shortcuts on macOS). What do you see? You can read more about keybindings on the VS Code website. "],
["transform.html", "5 Data transformation 5.1 Introduction 5.2 Filter rows with query() 5.3 Arrange or sort rows with sort_values() 5.4 Select columns with filter() or loc[] 5.5 Add new variables with assign() 5.6 Grouped summaries or aggregations with agg() 5.7 Grouped transforms (and filters)", " 5 Data transformation 5.1 Introduction Visualization is an important tool for insight generation, but it is rare that you get the data in exactly the right form you need. Often you’ll need to create some new variables or summaries, or maybe you just want to rename the variables or reorder the observations in order to make the data a little easier to work with. You’ll learn how to do all that (and more!) in this chapter, which will teach you how to transform your data using the pandas package and a new dataset on flights departing New York City in 2013. 5.1.1 Prerequisites In this chapter we’re going to focus on how to use the pandas package, the foundational package for data science in Python. We’ll illustrate the key ideas using data from the nycflights13 R package, and use Altair to help us understand the data. We will also need two additional Python packages to help us with mathematical and statistical functions - NumPy and SciPy. Notice the from ____ import ____ follows the SciPy guidance to import functions from submodule spaces. Now we will call functions using the SciPy package with the stats.&lt;FUNCTION&gt; structure. import pandas as pd import altair as alt import numpy as np from scipy import stats flights_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/flights/flights.csv&quot; flights = pd.read_csv(flights_url) flights[&#39;time_hour&#39;] = pd.to_datetime(flights.time_hour, format = &quot;%Y-%m-%d %H:%M:%S&quot;) 5.1.2 nycflights13 To explore the basic data manipulation verbs of pandas, we’ll use flights. This data frame contains all 336,776 flights that departed from New York City in 2013. The data comes from the US Bureau of Transportation Statistics, and is documented here. #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 2013 9 30 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 2013 9 30 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 2013 9 30 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 2013 9 30 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] You might notice that this data frame does not print in its entirety as other data frames you might have seen in the past: it only shows the first few and last few rows with only the columns that fit on one screen. (To see the whole dataset, you can open the variable view in your interactive Python window and double click on the flights object which will open the dataset in the VS Code data viewer). Using flights.dtypes will show you the variables types for each column. These describe the type of each variable: #&gt; year int64 #&gt; month int64 #&gt; day int64 #&gt; dep_time float64 #&gt; sched_dep_time int64 #&gt; dep_delay float64 #&gt; arr_time float64 #&gt; sched_arr_time int64 #&gt; arr_delay float64 #&gt; carrier object #&gt; flight int64 #&gt; tailnum object #&gt; origin object #&gt; dest object #&gt; air_time float64 #&gt; distance int64 #&gt; hour int64 #&gt; minute int64 #&gt; time_hour datetime64[ns, UTC] #&gt; dtype: object int64 stands for integers. float64 stands for doubles, or real numbers. object stands for character vectors, or strings. datetime64 stands for date-times (a date + a time) and dates. You can read more about pandas datetime tools There are three other common types of variables that aren’t used in this dataset but you’ll encounter later in the book: bool stands for logical, vectors that contain only True or False. category stands for factors, which pandas uses to represent categorical variables with fixed possible values. Using flights.info() also provides a print out of data types on other useful information about your pandas data frame. flights.info() #&gt; &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; #&gt; RangeIndex: 336776 entries, 0 to 336775 #&gt; Data columns (total 19 columns): #&gt; # Column Non-Null Count Dtype #&gt; --- ------ -------------- ----- #&gt; 0 year 336776 non-null int64 #&gt; 1 month 336776 non-null int64 #&gt; 2 day 336776 non-null int64 #&gt; 3 dep_time 328521 non-null float64 #&gt; 4 sched_dep_time 336776 non-null int64 #&gt; 5 dep_delay 328521 non-null float64 #&gt; 6 arr_time 328063 non-null float64 #&gt; 7 sched_arr_time 336776 non-null int64 #&gt; 8 arr_delay 327346 non-null float64 #&gt; 9 carrier 336776 non-null object #&gt; 10 flight 336776 non-null int64 #&gt; 11 tailnum 334264 non-null object #&gt; 12 origin 336776 non-null object #&gt; 13 dest 336776 non-null object #&gt; 14 air_time 327346 non-null float64 #&gt; 15 distance 336776 non-null int64 #&gt; 16 hour 336776 non-null int64 #&gt; 17 minute 336776 non-null int64 #&gt; 18 time_hour 336776 non-null datetime64[ns, UTC] #&gt; dtypes: datetime64[ns, UTC](1), float64(5), int64(9), object(4) #&gt; memory usage: 48.8+ MB 5.1.3 pandas data manipulation basics In this chapter you are going to learn five key pandas functions or object methods. Object methods are things the objects can perform. For example, pandas data frames know how to tell you their shape, the pandas object knows how to concatenate two data frames together. The way we tell an object we want it to do something is with the ‘dot operator’. We will refer to these object operators as functions or methods. Below are the five methods that allow you to solve the vast majority of your data manipulation challenges: Pick observations by their values (query()). Reorder the rows (sort()). Pick variables by their names (select()). Create new variables with functions of existing variables (assign()). Collapse many values down to a single summary (groupby()). The pandas package can handle all of the same functionality of dplyr in R. You can read pandas mapping guide and this towards data science article to get more details on the following brief table. Table 5.1: Comparable functions in R-Dplyr and Python-Pandas R dplyr function Python pandas function filter() query() arrange() sort_values() select() filter() or loc[] rename () rename() mutate() assign() (see note) group_by () groupby() summarise() agg() Note: The dpylr::mutate() function works similar to assign() in pandas on data frames. But you cannot use assign() on grouped data frame in pandas like you would use dplyr::mutate() on a grouped object. In that case you would use transform() and even then the functionality is not quite the same. The groupby() changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These functions provide the verbs for a language of data manipulation. All verbs work similarly: The first argument is a pandas dataFrame. The subsequent methods describe what to do with the data frame. The result is a new data frame. Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let’s dive in and see how these verbs work. 5.2 Filter rows with query() query() allows you to subset observations based on their values. The first argument specifies the rows to be selected. This argument can be label names or a boolean series. The second argument specifies the columns to be selected. The bolean filter on the rows is our focus. For example, we can select all flights on January 1st with: flights.query(&#39;month == 1 &amp; day == 1&#39;) #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; .. ... ... ... ... ... ... ... #&gt; 837 2013 1 1 ... 23 59 2013-01-02 04:00:00+00:00 #&gt; 838 2013 1 1 ... 16 30 2013-01-01 21:00:00+00:00 #&gt; 839 2013 1 1 ... 19 35 2013-01-02 00:00:00+00:00 #&gt; 840 2013 1 1 ... 15 0 2013-01-01 20:00:00+00:00 #&gt; 841 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; #&gt; [842 rows x 19 columns] The previous expression is equivalent to flights[(flights.month == 1) &amp; (flights.day == 1)] When you run that line of code, pandas executes the filtering operation and returns a new data frame. pandas functions usually don’t modify their inputs, so if you want to save the result, you’ll need to use the assignment operator, =: jan1 = flights.query(&#39;month == 1 &amp; day == 1&#39;) Interactive Python either prints out the results, or saves them to a variable. 5.2.1 Comparisons To use filtering effectively, you have to know how to select the observations that you want using the comparison operators. Python provides the standard suite: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). When you’re starting out with Python, the easiest mistake to make is to use = instead of == when testing for equality. When this happens you’ll get an error: flights.query(&#39;month = 1&#39;) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: cannot assign without a target object #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/frame.py&quot;, line 3231, in query #&gt; res = self.eval(expr, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/frame.py&quot;, line 3346, in eval #&gt; return _eval(expr, inplace=inplace, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/eval.py&quot;, line 332, in eval #&gt; parsed_expr = Expr(expr, engine=engine, parser=parser, env=env) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 764, in __init__ #&gt; self.terms = self.parse() #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 781, in parse #&gt; return self._visitor.visit(self.expr) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 375, in visit #&gt; return visitor(node, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 381, in visit_Module #&gt; return self.visit(expr, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 375, in visit #&gt; return visitor(node, **kwargs) #&gt; File &quot;/usr/local/lib/python3.7/site-packages/pandas/core/computation/expr.py&quot;, line 585, in visit_Assign #&gt; raise ValueError(&quot;cannot assign without a target object&quot;) There’s another common problem you might encounter when using ==: floating point numbers. The following result might surprise you! np.sqrt(2) ** 2 == 2 #&gt; False 1 / 49 * 49 == 1 #&gt; False Computers use finite precision arithmetic (they obviously can’t store an infinite number of digits!) so remember that every number you see is an approximation. Instead of relying on ==, use np.isclose(): np.isclose(np.sqrt(2) ** 2, 2) #&gt; True np.isclose(1 / 49 * 49, 1) #&gt; True 5.2.2 Logical operators Multiple arguments to query() are combined with “and”: every expression must be true in order for a row to be included in the output. For other types of combinations, you’ll need to use Boolean operators yourself: &amp; is “and”, | is “or”, and ! is “not”. Figure 5.1 shows the complete set of Boolean operations. Figure 5.1: Complete set of boolean operations. x is the left-hand circle, y is the right-hand circle, and the shaded region show which parts each operator selects. The following code finds all flights that departed in November or December: flights.query(&#39;month == 11 | month == 12&#39;) The order of operations doesn’t work like English. You can’t write filter(flights, month == (11 | 12)), which you might literally translate into “finds all flights that departed in November or December”. Instead it finds all months that equal 11 | 12, an expression that evaluates to True. In a numeric context (like here), True becomes one, so this finds all flights in January, not November or December. This is quite confusing! A useful short-hand for this problem is x in y. This will select every row where x is one of the values in y. We could use it to rewrite the code above: nov_dec = flights.query(&#39;month in [11, 12]&#39;) Sometimes you can simplify complicated subsetting by remembering De Morgan’s law: !(x &amp; y) is the same as !x | !y, and !(x | y) is the same as !x &amp; !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters: flights.query(&#39;arr_delay &gt; 120 | dep_delay &gt; 120&#39;) flights.query(&#39;arr_delay &lt;= 120 | dep_delay &lt;= 120&#39;) Whenever you start using complicated, multipart expressions in query(), consider making them explicit variables instead. That makes it much easier to check your work. You’ll learn how to create new variables shortly. 5.2.3 Missing values One important feature of pandas in Python that can make comparison tricky are missing values, or NAs (“not availables”). NA represents an unknown value so missing values are “contagious”: almost any operation involving an unknown value will also be unknown. np.nan + 10 #&gt; nan np.nan / 2 #&gt; nan The most confusing result are the comparisons. They always return a False. The logic for this result is explained on stackoverflow. The pandas missing data guide is a helpful read. np.nan &gt; 5 #&gt; False 10 == np.nan #&gt; False np.nan == np.nan #&gt; False It’s easiest to understand why this is true with a bit more context: # Let x be Mary&#39;s age. We don&#39;t know how old she is. x = np.nan # Let y be John&#39;s age. We don&#39;t know how old he is. y = np.nan # Are John and Mary the same age? x == y # Illogical comparisons are False. #&gt; False The Python development team did decide to provide functionality to find np.nan objects in your code by allowing np.nan != np.nan to return True. Once again you can read the rationale for this decision. Python now has isnan() functions to make this comparison more straight forward in your code. Pandas uses the nan structure in Python to identify NA or ‘missing’ values. If you want to determine if a value is missing, use pd.isna(): pd.isna(x) #&gt; True query() only includes rows where the condition is TRUE; it excludes both FALSE and NA values. df = pd.DataFrame({&#39;x&#39;: [1, np.nan, 3]}) df.query(&#39;x &gt; 1&#39;) #&gt; x #&gt; 2 3.0 If you want to preserve missing values, ask for them explicitly using the trick mentioned in the previous paragraph or by using pd.isna() with the symbolic reference @ in your condition: df.query(&#39;x != x | x &gt; 1&#39;) #&gt; x #&gt; 1 NaN #&gt; 2 3.0 df.query(&#39;@pd.isna(x) | x &gt; 1&#39;) #&gt; x #&gt; 1 NaN #&gt; 2 3.0 5.2.4 Exercises Find all flights that Had an arrival delay of two or more hours Flew to Houston (IAH or HOU) Were operated by United, American, or Delta Departed in summer (July, August, and September) Arrived more than two hours late, but didn’t leave late Were delayed by at least an hour, but made up over 30 minutes in flight Departed between midnight and 6am (inclusive) How many flights have a missing dep_time? What other variables are missing? What might these rows represent? 5.3 Arrange or sort rows with sort_values() sort() works similarly to query() except that instead of selecting rows, it changes their order. It takes a data frame and a column name or a list of column names to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns: flights.sort_values(by = [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 111291 2013 12 31 ... 7 5 2013-12-31 12:00:00+00:00 #&gt; 111292 2013 12 31 ... 8 25 2013-12-31 13:00:00+00:00 #&gt; 111293 2013 12 31 ... 16 15 2013-12-31 21:00:00+00:00 #&gt; 111294 2013 12 31 ... 6 0 2013-12-31 11:00:00+00:00 #&gt; 111295 2013 12 31 ... 8 30 2013-12-31 13:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] Use the argument ascending = False to re-order by a column in descending order: flights.sort_values(by = [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;], ascending = False) #&gt; year month day ... hour minute time_hour #&gt; 110520 2013 12 31 ... 23 59 2014-01-01 04:00:00+00:00 #&gt; 110521 2013 12 31 ... 23 59 2014-01-01 04:00:00+00:00 #&gt; 110522 2013 12 31 ... 22 45 2014-01-01 03:00:00+00:00 #&gt; 110523 2013 12 31 ... 5 0 2013-12-31 10:00:00+00:00 #&gt; 110524 2013 12 31 ... 5 15 2013-12-31 10:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 837 2013 1 1 ... 23 59 2013-01-02 04:00:00+00:00 #&gt; 838 2013 1 1 ... 16 30 2013-01-01 21:00:00+00:00 #&gt; 839 2013 1 1 ... 19 35 2013-01-02 00:00:00+00:00 #&gt; 840 2013 1 1 ... 15 0 2013-01-01 20:00:00+00:00 #&gt; 841 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] Missing values are always sorted at the end: df = pd.DataFrame({&#39;x&#39;: [5, 2, np.nan]}) df.sort_values(&#39;x&#39;) #&gt; x #&gt; 1 2.0 #&gt; 0 5.0 #&gt; 2 NaN df.sort_values(&#39;x&#39;, ascending = False) #&gt; x #&gt; 0 5.0 #&gt; 1 2.0 #&gt; 2 NaN 5.3.1 Exercises How could you use sort() to sort all missing values to the start? (Hint: use isna()). Sort flights to find the most delayed flights. Find the flights that left earliest. Sort flights to find the fastest (highest speed) flights. Which flights travelled the farthest? Which travelled the shortest? 5.4 Select columns with filter() or loc[] It’s not uncommon to get datasets with hundreds or even thousands of variables. In this case, the first challenge is often narrowing in on the variables you’re actually interested in. fliter() allows you to rapidly zoom in on a useful subset using operations based on the names of the variables. filter() is not terribly useful with the flights data because we only have 19 variables, but you can still get the general idea: # Select columns by name flights.filter([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) # Select all columns except year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] flights.drop(columns = [&#39;year&#39;, &#39;day&#39;]) #&gt; month dep_time sched_dep_time ... hour minute time_hour #&gt; 0 1 517.0 515 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 1 533.0 529 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 1 542.0 540 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 1 544.0 545 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 1 554.0 600 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 9 NaN 1455 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 9 NaN 2200 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 9 NaN 1210 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 9 NaN 1159 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 9 NaN 840 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 17 columns] loc[] functions in a similar fashion. # Select columns by name flights.loc[:, [&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]] # Select all columns between year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] flights.loc[:, &#39;year&#39;:&#39;day&#39;] # Select all columns except year and day (inclusive) #&gt; year month day #&gt; 0 2013 1 1 #&gt; 1 2013 1 1 #&gt; 2 2013 1 1 #&gt; 3 2013 1 1 #&gt; 4 2013 1 1 #&gt; ... ... ... ... #&gt; 336771 2013 9 30 #&gt; 336772 2013 9 30 #&gt; 336773 2013 9 30 #&gt; 336774 2013 9 30 #&gt; 336775 2013 9 30 #&gt; #&gt; [336776 rows x 3 columns] There are a number of helper regular expressions you can use within filter(): flights.filter(regex = '^sch'): matches column names that begin with “sch”. flights.filter(regex = \"time$\"): matches names that end with “time”. flights.filter(regex = \"_dep_\"): matches names that contain “dep”. flights.filter(regex = '(.)\\\\1'): selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in strings. See pandas filter documentation for more details. Use rename() to rename a column or multiple columns. flights.rename(columns = {&#39;year&#39;: &#39;YEAR&#39;, &#39;month&#39;:&#39;MONTH&#39;}) #&gt; YEAR MONTH day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 2013 9 30 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 2013 9 30 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 2013 9 30 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 2013 9 30 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [336776 rows x 19 columns] 5.4.1 Exercises Brainstorm as many ways as possible to select dep_time, dep_delay, arr_time, and arr_delay from flights. What happens if you include the name of a variable multiple times in a filter() call? Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default? flights.filter(regex = &quot;TIME&quot;) 5.5 Add new variables with assign() Besides selecting sets of existing columns, it’s often useful to add new columns that are functions of existing columns. That’s the job of assign(). assign() always adds new columns at the end of your dataset so we’ll start by creating a narrower dataset so we can see the new variables. flights_sml = (flights. filter(regex = &quot;^year$|^month$|^day$|delay$|^distance$|^air_time$&quot;) ) flights_sml.assign( gain = lambda x: x.dep_delay - x.arr_delay, speed = lambda x: x.distance / x.air_time * 60 ).head() #&gt; year month day dep_delay arr_delay air_time distance gain speed #&gt; 0 2013 1 1 2.0 11.0 227.0 1400 -9.0 370.044053 #&gt; 1 2013 1 1 4.0 20.0 227.0 1416 -16.0 374.273128 #&gt; 2 2013 1 1 2.0 33.0 160.0 1089 -31.0 408.375000 #&gt; 3 2013 1 1 -1.0 -18.0 183.0 1576 17.0 516.721311 #&gt; 4 2013 1 1 -6.0 -25.0 116.0 762 19.0 394.137931 Note that you can refer to columns that you’ve just created: flights_sml.assign( gain = lambda x: x.dep_delay - x.arr_delay, hours = lambda x: x.air_time / 60, gain_per_hour = lambda x: x.gain / x.hours ).head() #&gt; year month day dep_delay ... distance gain hours gain_per_hour #&gt; 0 2013 1 1 2.0 ... 1400 -9.0 3.783333 -2.378855 #&gt; 1 2013 1 1 4.0 ... 1416 -16.0 3.783333 -4.229075 #&gt; 2 2013 1 1 2.0 ... 1089 -31.0 2.666667 -11.625000 #&gt; 3 2013 1 1 -1.0 ... 1576 17.0 3.050000 5.573770 #&gt; 4 2013 1 1 -6.0 ... 762 19.0 1.933333 9.827586 #&gt; #&gt; [5 rows x 10 columns] 5.5.1 Useful creation functions There are many functions for creating new variables that you can use with assign(). The key property is that the function must be vectorised: it must take a vector of values as input, return a vector with the same number of values as output. Some arithmetic operators are available in Python without the need for any additional packages. However, many arithmetic functions like mean() and std() are accessed through importing additional packages. Python comes with a math and statistics package. However, we recommend the NumPy package for accessing the suite of mathematical functions needed. You would import NumPy with import numpy as np. There’s no way to list every possible function that you might use, but here’s a selection of functions that are frequently useful: Arithmetic operators: +, -, *, /, ^. These are all vectorised, using the so called “recycling rules”. If one parameter is shorter than the other, it will be automatically extended to be the same length. This is most useful when one of the arguments is a single number: air_time / 60, hours * 60 + minute, etc. Arithmetic operators are also useful in conjunction with the aggregate functions you’ll learn about later. For example, x / np.sum(x) calculates the proportion of a total, and y - np.mean(y) computes the difference from the mean. Modular arithmetic: // (integer division) and % (remainder), where x == y * (x // y) + (x % y). Modular arithmetic is a handy tool because it allows you to break integers up into pieces. For example, in the flights dataset, you can compute hour and minute from dep_time with: (flights. filter([&#39;dep_time&#39;]). assign( hour = lambda x: x.dep_time // 100, minute = lambda x: x.dep_time % 100) ) #&gt; dep_time hour minute #&gt; 0 517.0 5.0 17.0 #&gt; 1 533.0 5.0 33.0 #&gt; 2 542.0 5.0 42.0 #&gt; 3 544.0 5.0 44.0 #&gt; 4 554.0 5.0 54.0 #&gt; ... ... ... ... #&gt; 336771 NaN NaN NaN #&gt; 336772 NaN NaN NaN #&gt; 336773 NaN NaN NaN #&gt; 336774 NaN NaN NaN #&gt; 336775 NaN NaN NaN #&gt; #&gt; [336776 rows x 3 columns] Logs: np.log(), np.log2(), np.log10(). Logarithms are an incredibly useful transformation for dealing with data that ranges across multiple orders of magnitude. They also convert multiplicative relationships to additive, a feature we’ll come back to in modelling. All else being equal, I recommend using np.log2() because it’s easy to interpret: a difference of 1 on the log scale corresponds to doubling on the original scale and a difference of -1 corresponds to halving. Offsets: shift(1) and shift(-1) allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - x.shift(1)) or find when values change (x != x.shift(1)). They are most useful in conjunction with groupby(), which you’ll learn about shortly. x = pd.Series(np.arange(1,10)) x.shift(1) #&gt; 0 NaN #&gt; 1 1.0 #&gt; 2 2.0 #&gt; 3 3.0 #&gt; 4 4.0 #&gt; 5 5.0 #&gt; 6 6.0 #&gt; 7 7.0 #&gt; 8 8.0 #&gt; dtype: float64 x.shift(-1) #&gt; 0 2.0 #&gt; 1 3.0 #&gt; 2 4.0 #&gt; 3 5.0 #&gt; 4 6.0 #&gt; 5 7.0 #&gt; 6 8.0 #&gt; 7 9.0 #&gt; 8 NaN #&gt; dtype: float64 Cumulative and rolling aggregates: pandas provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(). If you need rolling aggregates (i.e. a sum computed over a rolling window), try the rolling() in the pandas package. x #&gt; 0 1 #&gt; 1 2 #&gt; 2 3 #&gt; 3 4 #&gt; 4 5 #&gt; 5 6 #&gt; 6 7 #&gt; 7 8 #&gt; 8 9 #&gt; dtype: int64 x.cumsum() #&gt; 0 1 #&gt; 1 3 #&gt; 2 6 #&gt; 3 10 #&gt; 4 15 #&gt; 5 21 #&gt; 6 28 #&gt; 7 36 #&gt; 8 45 #&gt; dtype: int64 x.rolling(2).mean() #&gt; 0 NaN #&gt; 1 1.5 #&gt; 2 2.5 #&gt; 3 3.5 #&gt; 4 4.5 #&gt; 5 5.5 #&gt; 6 6.5 #&gt; 7 7.5 #&gt; 8 8.5 #&gt; dtype: float64 Logical comparisons, &lt;, &lt;=, &gt;, &gt;=, !=, and ==, which you learned about earlier. If you’re doing a complex sequence of logical operations it’s often a good idea to store the interim values in new variables so you can check that each step is working as expected. Ranking: there are a number of ranking functions, but you should start with min_rank(). It does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th). The default gives smallest values the small ranks; use desc(x) to give the largest values the smallest ranks. y = pd.Series([1, 2, 2, np.nan, 3, 4]) y.rank(method = &#39;min&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 2.0 #&gt; 3 NaN #&gt; 4 4.0 #&gt; 5 5.0 #&gt; dtype: float64 y.rank(ascending=False, method = &#39;min&#39;) #&gt; 0 5.0 #&gt; 1 3.0 #&gt; 2 3.0 #&gt; 3 NaN #&gt; 4 2.0 #&gt; 5 1.0 #&gt; dtype: float64 If method = 'min'`` doesn't do what you need, look at the variantsmethod = ‘first’,method = ‘dense’,method = ‘percent’,pct = True`. See the rank help page for more details. y.rank(method = &#39;first&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 3.0 #&gt; 3 NaN #&gt; 4 4.0 #&gt; 5 5.0 #&gt; dtype: float64 y.rank(method = &#39;dense&#39;) #&gt; 0 1.0 #&gt; 1 2.0 #&gt; 2 2.0 #&gt; 3 NaN #&gt; 4 3.0 #&gt; 5 4.0 #&gt; dtype: float64 y.rank(pct = True) #&gt; 0 0.2 #&gt; 1 0.5 #&gt; 2 0.5 #&gt; 3 NaN #&gt; 4 0.8 #&gt; 5 1.0 #&gt; dtype: float64 5.5.2 Exercises Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they’re not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight. Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it? Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related? Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for method = 'min'. What trigonometric functions does NumPy provide? 5.6 Grouped summaries or aggregations with agg() The last key verb is agg(). It collapses a data frame to a single row: flights.agg({&#39;dep_delay&#39;: np.mean}) #&gt; dep_delay 12.63907 #&gt; dtype: float64 (Pandas aggregate functions ignores the np.nan values like na.rm = TRUE in R.) agg() is not terribly useful unless we pair it with groupby(). This changes the unit of analysis from the complete dataset to individual groups. Then, when you use the pandas functions on a grouped data frame they’ll be automatically applied “by group”. For example, if we applied similiar code to a data frame grouped by date, we get the average delay per date. Note that with the groupby() function we used tuple to identify the column (first entry) and the function to apply on the column (second entry). This is called named aggregation in pandas: by_day = flights.groupby([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]) by_day.agg(delay = (&#39;dep_delay&#39;, np.mean)).reset_index() #&gt; year month day delay #&gt; 0 2013 1 1 11.548926 #&gt; 1 2013 1 2 13.858824 #&gt; 2 2013 1 3 10.987832 #&gt; 3 2013 1 4 8.951595 #&gt; 4 2013 1 5 5.732218 #&gt; .. ... ... ... ... #&gt; 360 2013 12 27 10.937630 #&gt; 361 2013 12 28 7.981550 #&gt; 362 2013 12 29 22.309551 #&gt; 363 2013 12 30 10.698113 #&gt; 364 2013 12 31 6.996053 #&gt; #&gt; [365 rows x 4 columns] Note the use of reset_index() to remove pandas creation of a MultiIndex. You can read more about the use of grouby in pandas with their Group By: split-apply-combine user Guid documentation Together groupby() and agg() provide one of the tools that you’ll use most commonly when working with pandas: grouped summaries. But before we go any further with this, we need to introduce a structure for pandas code when doing data science work. We structure our code much like ‘the pipe’, %&gt;% in the tidyverse packages from R-Studio. 5.6.1 Combining multiple operations Imagine that we want to explore the relationship between the distance and average delay for each location. Using what you know about pandas, you might write code like this: by_dest = flights.groupby(&#39;dest&#39;) delay = by_dest.agg( count = (&#39;distance&#39;, &#39;size&#39;), dist = (&#39;distance&#39;, np.mean), delay = (&#39;arr_delay&#39;, np.mean) ) delay = delay.query(&#39;count &gt; 20 &amp; dest != &quot;HNL&quot;&#39;) # It looks like delays increase with distance up to ~750 miles # and then decrease. Maybe as flights get longer there&#39;s more # ability to make up delays in the air? chart_base = (alt.Chart(delay). encode( x = &#39;dist&#39;, y = &#39;delay&#39; )) chart = chart_base.mark_point() + chart_base.transform_loess(&#39;dist&#39;, &#39;delay&#39;).mark_line() There are three steps to prepare this data: Group flights by destination. Summarise to compute distance, average delay, and number of flights. Filter to remove noisy points and Honolulu airport, which is almost twice as far away as the next closest airport. This code is a little frustrating to write because we have to give each intermediate data frame a name, even though we don’t care about it. Naming things is hard, so this slows down our analysis. There’s another way to tackle the same problem without the additional objects: delays = (flights. groupby(&#39;dest&#39;). agg( count = (&#39;distance&#39;, &#39;size&#39;), dist = (&#39;distance&#39;, np.mean), delay = (&#39;arr_delay&#39;, np.mean) ). query(&#39;count &gt; 20 &amp; dest != &quot;HNL&quot;&#39;)) This focuses on the transformations, not what’s being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarise, then filter. As suggested by this reading, a good way to pronounce . when reading pandas code is “then”. You can use the () with . to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. We’ll use this format frequently from now on because it considerably improves the readability of complex pandas code. 5.6.2 Missing values You may have wondered about the np.nan values we put into our pandas data frame above. Pandas just started an experimental options (version 1.0) for pd.NA but it is not standard as in the R language. You can read the full details about missing data in pandas. Pandas’ and NumPy’s handling of missing values defaults to the opposite functionality of R and the Tidyverse. Here are three key defaults when using Pandas. When summing data, NA (missing) values will be treated as zero. If the data are all NA, the result will be 0. Cumulative methods ignore NA values by default, but preserve them in the resulting arrays. To override this behaviour and include missing values, use skipna=False. All the .groupby() methods exclude missing values in their calculations as described in the pandas groupby documentation. In our case, where missing values represent cancelled flights, we could also tackle the problem by first removing the cancelled flights. We’ll save this dataset so we can reuse it in the next few examples. not_cancelled = flights.dropna(subset = [&#39;dep_delay&#39;, &#39;arr_delay&#39;]) 5.6.3 Counts Whenever you do any aggregation, it’s always a good idea to include either a count (size()), or a count of non-missing values (sum(!is.na(x))). That way you can check that you’re not drawing conclusions based on very small amounts of data. For example, let’s look at the planes (identified by their tail number) that have the highest average delays: delays = not_cancelled.groupby(&#39;tailnum&#39;).agg( delay = (&quot;arr_delay&quot;, np.mean) ) chart = (alt.Chart(delays). transform_density( density = &#39;delay&#39;, as_ = [&#39;delay&#39;, &#39;density&#39;], bandwidth=10 ). encode( x = &#39;delay:Q&#39;, y = &#39;density:Q&#39; ). mark_line() ) Wow, there are some planes that have an average delay of 5 hours (300 minutes)! The story is actually a little more nuanced. We can get more insight if we draw a scatterplot of number of flights vs. average delay: delays = not_cancelled.groupby(&#39;tailnum&#39;).agg( delay = (&quot;arr_delay&quot;, np.mean), n = (&#39;arr_delay&#39;, &#39;size&#39;) ) chart = (alt.Chart(delays). encode( x = &#39;n&#39;, y = &#39;delay&#39; ). mark_point( filled = True, opacity = 1/10) ) Not surprisingly, there is much greater variation in the average delay when there are few flights. The shape of this plot is very characteristic: whenever you plot a mean (or other summary) vs. group size, you’ll see that the variation decreases as the sample size increases. When looking at this sort of plot, it’s often useful to filter out the groups with the smallest numbers of observations, so you can see more of the pattern and less of the extreme variation in the smallest groups. This is what the following code does, as well as showing you a handy pattern for simple data frame manipulations only needed for a chart. chart = (alt.Chart(delays.query(&quot;n &gt; 25&quot;)). encode( x = &#39;n&#39;, y = &#39;delay&#39; ). mark_point( filled = True, opacity = 1/10) ) chart.save(&quot;screenshots/altair_delays.png&quot;) There’s another common variation of this type of pattern. Let’s look at how the average performance of batters in baseball is related to the number of times they’re at bat. Here I use data from the Lahman package to compute the batting average (number of hits / number of attempts) of every major league baseball player. When I plot the skill of the batter (measured by the batting average, ba) against the number of opportunities to hit the ball (measured by at bat, ab), you see two patterns: As above, the variation in our aggregate decreases as we get more data points. There’s a positive correlation between skill (ba) and opportunities to hit the ball (ab). This is because teams control who gets to play, and obviously they’ll pick their best players. # settings for Altair to handle large data alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) batting_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/batting/batting.csv&quot; batting = pd.read_csv(batting_url) batters = (batting. groupby(&#39;playerID&#39;). agg( ab = (&quot;AB&quot;, &quot;sum&quot;), h = (&quot;H&quot;, &quot;sum&quot;) ). assign( ba = lambda x: x.h/x.ab ) ) chart = (alt.Chart(batters.query(&#39;ab &gt; 100&#39;)). encode( x = &#39;ab&#39;, y = &#39;ba&#39; ). mark_point() ) chart.save(&quot;screenshots/altair_batters.png&quot;) This also has important implications for ranking. If you naively sort on desc(ba), the people with the best batting averages are clearly lucky, not skilled: batters.sort_values(&#39;ba&#39;, ascending = False).head(10) #&gt; ab h ba #&gt; playerID #&gt; egeco01 1 1 1.0 #&gt; simspe01 1 1 1.0 #&gt; paciojo01 3 3 1.0 #&gt; bruneju01 1 1 1.0 #&gt; liddeda01 1 1 1.0 #&gt; garcimi02 1 1 1.0 #&gt; meehabi01 1 1 1.0 #&gt; rodried01 1 1 1.0 #&gt; hopkimi01 2 2 1.0 #&gt; gallaja01 1 1 1.0 You can find a good explanation of this problem at http://varianceexplained.org/r/empirical_bayes_baseball/ and http://www.evanmiller.org/how-not-to-sort-by-average-rating.html. 5.6.4 Useful summary functions Just using means, counts, and sum can get you a long way, but NumPy, SciPy, and Pandas provide many other useful summary functions (remember we are using the SciPy stats submodule): Measures of location: we’ve used np.mean(), but np.median() is also useful. The mean is the sum divided by the length; the median is a value where 50% of x is above it, and 50% is below it. It’s sometimes useful to combine aggregation with logical subsetting. We haven’t talked about this sort of subsetting yet, but you’ll learn more about it in subsetting. (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]). agg( avg_delay1 = (&#39;arr_delay&#39;, np.mean), avg_delay2 = (&#39;arr_delay&#39;, lambda x: np.mean(x[x &gt; 0])) ) ) #&gt; avg_delay1 avg_delay2 #&gt; year month day #&gt; 2013 1 1 12.651023 32.481562 #&gt; 2 12.692888 32.029907 #&gt; 3 5.733333 27.660870 #&gt; 4 -1.932819 28.309764 #&gt; 5 -1.525802 22.558824 #&gt; ... ... ... #&gt; 12 27 -0.148803 29.046832 #&gt; 28 -3.259533 25.607692 #&gt; 29 18.763825 47.256356 #&gt; 30 10.057712 31.243802 #&gt; 31 6.212121 24.455959 #&gt; #&gt; [365 rows x 2 columns] Measures of spread: np.sd(), stats.iqr(), stats.median_absolute_deviation(). The root mean squared deviation, or standard deviation np.sd(), is the standard measure of spread. The interquartile range stats.iqr() and median absolute deviation stats.median_absolute_deviation() are robust equivalents that may be more useful if you have outliers. # Why is distance to some destinations more variable than to others? (not_cancelled. groupby([&#39;dest&#39;]). agg(distance_sd = (&#39;distance&#39;, np.std)). sort_values(&#39;distance_sd&#39;, ascending = False) ) #&gt; distance_sd #&gt; dest #&gt; EGE 10.542765 #&gt; SAN 10.350094 #&gt; SFO 10.216017 #&gt; HNL 10.004197 #&gt; SEA 9.977993 #&gt; ... ... #&gt; BZN 0.000000 #&gt; BUR 0.000000 #&gt; PSE 0.000000 #&gt; ABQ 0.000000 #&gt; LEX NaN #&gt; #&gt; [104 rows x 1 columns] Measures of rank: np.min(), np.quantile(), np.max(). Quantiles are a generalisation of the median. For example, np.quantile(x, 0.25) will find a value of x that is greater than 25% of the values, and less than the remaining 75%. # When do the first and last flights leave each day? (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;]). agg( first = (&#39;dep_time&#39;, np.min), last = (&#39;dep_time&#39;, np.max) ) ) #&gt; first last #&gt; year month day #&gt; 2013 1 1 517.0 2356.0 #&gt; 2 42.0 2354.0 #&gt; 3 32.0 2349.0 #&gt; 4 25.0 2358.0 #&gt; 5 14.0 2357.0 #&gt; ... ... ... #&gt; 12 27 2.0 2351.0 #&gt; 28 7.0 2358.0 #&gt; 29 3.0 2400.0 #&gt; 30 1.0 2356.0 #&gt; 31 13.0 2356.0 #&gt; #&gt; [365 rows x 2 columns] Measures of position: first(), nth(), last(). These work similarly to x[1], x[2], and x[size(x)] but let you set a default value if that position does not exist (i.e. you’re trying to get the 3rd element from a group that only has two elements). For example, we can find the first and last departure for each day: # using first and last (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( first_dep = (&#39;dep_time&#39;, &#39;first&#39;), last_dep = (&#39;dep_time&#39;, &#39;last&#39;) ) ) #&gt; first_dep last_dep #&gt; year month day #&gt; 2013 1 1 517.0 2356.0 #&gt; 2 42.0 2354.0 #&gt; 3 32.0 2349.0 #&gt; 4 25.0 2358.0 #&gt; 5 14.0 2357.0 #&gt; ... ... ... #&gt; 12 27 2.0 2351.0 #&gt; 28 7.0 2358.0 #&gt; 29 3.0 2400.0 #&gt; 30 1.0 2356.0 #&gt; 31 13.0 2356.0 #&gt; #&gt; [365 rows x 2 columns] # using position (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( first_dep = (&#39;dep_time&#39;, lambda x: list(x)[1]), last_dep = (&#39;dep_time&#39;, lambda x: list(x)[-1]) ) ) #&gt; first_dep last_dep #&gt; year month day #&gt; 2013 1 1 533.0 2356.0 #&gt; 2 126.0 2354.0 #&gt; 3 50.0 2349.0 #&gt; 4 106.0 2358.0 #&gt; 5 37.0 2357.0 #&gt; ... ... ... #&gt; 12 27 8.0 2351.0 #&gt; 28 56.0 2358.0 #&gt; 29 39.0 2400.0 #&gt; 30 23.0 2356.0 #&gt; 31 18.0 2356.0 #&gt; #&gt; [365 rows x 2 columns] Counts: You’ve seen size(), which takes no arguments, and returns the size of the current group. To count the number of non-missing values, use isnull().sum(). To count the number of unique (distinct) values, use nunique(). # Which destinations have the most carriers? (flights. groupby(&#39;dest&#39;). agg( carriers_unique = (&#39;carrier&#39;, &#39;nunique&#39;), carriers_count = (&#39;carrier&#39;, &#39;size&#39;), missing_time = (&#39;dep_time&#39;, lambda x: x.isnull().sum()) ) ) #&gt; carriers_unique carriers_count missing_time #&gt; dest #&gt; ABQ 1 254 0.0 #&gt; ACK 1 265 0.0 #&gt; ALB 1 439 20.0 #&gt; ANC 1 8 0.0 #&gt; ATL 7 17215 317.0 #&gt; ... ... ... ... #&gt; TPA 7 7466 59.0 #&gt; TUL 1 315 16.0 #&gt; TVC 2 101 5.0 #&gt; TYS 2 631 52.0 #&gt; XNA 2 1036 25.0 #&gt; #&gt; [105 rows x 3 columns] Counts are useful and pandas provides a simple helper if all you want is a count: not_cancelled[&#39;dest&#39;].value_counts() #&gt; ATL 16837 #&gt; ORD 16566 #&gt; LAX 16026 #&gt; BOS 15022 #&gt; MCO 13967 #&gt; ... #&gt; MTJ 14 #&gt; HDN 14 #&gt; SBN 10 #&gt; ANC 8 #&gt; LEX 1 #&gt; Name: dest, Length: 104, dtype: int64 Counts and proportions of logical values: sum(x &gt; 10), mean(y == 0). When used with numeric functions, TRUE is converted to 1 and FALSE to 0. This makes sum() and mean() very useful: sum(x) gives the number of TRUEs in x, and mean(x) gives the proportion. # How many flights left before 5am? (these usually indicate delayed # flights from the previous day) (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( n_early = (&#39;dep_time&#39;, lambda x: np.sum(x &lt; 500)) ) ) # What proportion of flights are delayed by more than an hour? #&gt; n_early #&gt; year month day #&gt; 2013 1 1 0.0 #&gt; 2 3.0 #&gt; 3 4.0 #&gt; 4 3.0 #&gt; 5 3.0 #&gt; ... ... #&gt; 12 27 7.0 #&gt; 28 2.0 #&gt; 29 3.0 #&gt; 30 6.0 #&gt; 31 4.0 #&gt; #&gt; [365 rows x 1 columns] (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( hour_prop = (&#39;arr_delay&#39;, lambda x: np.sum(x &gt; 60)) ) ) #&gt; hour_prop #&gt; year month day #&gt; 2013 1 1 60.0 #&gt; 2 79.0 #&gt; 3 51.0 #&gt; 4 36.0 #&gt; 5 25.0 #&gt; ... ... #&gt; 12 27 51.0 #&gt; 28 31.0 #&gt; 29 129.0 #&gt; 30 69.0 #&gt; 31 33.0 #&gt; #&gt; [365 rows x 1 columns] 5.6.5 Grouping by multiple variables Be careful when progressively rolling up summaries: it’s OK for sums and counts, but you need to think about weighting means and variances, and it’s not possible to do it exactly for rank-based statistics like the median. In other words, the sum of groupwise sums is the overall sum, but the median of groupwise medians is not the overall median. 5.6.6 Ungrouping (reseting the index) If you need to remove grouping and MultiIndex use reset.index(). This is a rough equivalent to ungroup() in R but it is not the same thing. Notice the column names are no longer in multiple levels. dat = (not_cancelled. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). agg( hour_prop = (&#39;arr_delay&#39;, lambda x: np.sum(x &gt; 60)) ) ) dat.head() #&gt; hour_prop #&gt; year month day #&gt; 2013 1 1 60.0 #&gt; 2 79.0 #&gt; 3 51.0 #&gt; 4 36.0 #&gt; 5 25.0 dat.reset_index().head() #&gt; year month day hour_prop #&gt; 0 2013 1 1 60.0 #&gt; 1 2013 1 2 79.0 #&gt; 2 2013 1 3 51.0 #&gt; 3 2013 1 4 36.0 #&gt; 4 2013 1 5 25.0 5.6.7 Exercises Brainstorm at least 5 different ways to assess the typical delay characteristics of a group of flights. Consider the following scenarios: A flight is 15 minutes early 50% of the time, and 15 minutes late 50% of the time. A flight is always 10 minutes late. A flight is 30 minutes early 50% of the time, and 30 minutes late 50% of the time. 99% of the time a flight is on time. 1% of the time it’s 2 hours late. Which is more important: arrival delay or departure delay? Our definition of cancelled flights (is.na(dep_delay) | is.na(arr_delay) ) is slightly suboptimal. Why? Which is the most important column? Look at the number of cancelled flights per day. Is there a pattern? Is the proportion of cancelled flights related to the average delay? Which carrier has the worst delays? Challenge: can you disentangle the effects of bad airports vs. bad carriers? Why/why not? (Hint: think about flights.groupby(['carrier', 'dest']).agg(n = ('dep_time', 'size'))) 5.7 Grouped transforms (and filters) Grouping is most useful in conjunction with agg(), but you can also do convenient operations with transform(). This is a difference in pandas as compared to dplyr. Once you create a .groupby() object you cannot use assign() and the best equivalent is transform(). Following pandas groupby guide on ‘split-apply-combine’, we would assign our transfomred variables to our data frame and then perform filters on the full data frame. Find the worst members of each group: flights_sml[&#39;ranks&#39;] = (flights_sml. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;]). arr_delay.rank(ascending = False)) #&gt; /usr/local/bin/python3:3: SettingWithCopyWarning: #&gt; A value is trying to be set on a copy of a slice from a DataFrame. #&gt; Try using .loc[row_indexer,col_indexer] = value instead #&gt; #&gt; See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy flights_sml.query(&#39;ranks &lt; 10&#39;).drop(columns = &#39;ranks&#39;) #&gt; year month day dep_delay arr_delay air_time distance #&gt; 151 2013 1 1 853.0 851.0 41.0 184 #&gt; 649 2013 1 1 290.0 338.0 213.0 1134 #&gt; 673 2013 1 1 260.0 263.0 46.0 266 #&gt; 729 2013 1 1 157.0 174.0 60.0 213 #&gt; 746 2013 1 1 216.0 222.0 121.0 708 #&gt; ... ... ... ... ... ... ... ... #&gt; 336579 2013 9 30 158.0 121.0 95.0 765 #&gt; 336668 2013 9 30 182.0 174.0 95.0 708 #&gt; 336724 2013 9 30 158.0 136.0 91.0 746 #&gt; 336757 2013 9 30 194.0 194.0 50.0 301 #&gt; 336763 2013 9 30 154.0 130.0 123.0 944 #&gt; #&gt; [3306 rows x 7 columns] Find all groups bigger than a threshold: popular_dests = flights popular_dests[&#39;n&#39;] = popular_dests.groupby(&#39;dest&#39;).arr_delay.transform(&#39;size&#39;) popular_dests = flights.query(&#39;n &gt; 365&#39;).drop(columns = &#39;n&#39;) popular_dests #&gt; year month day ... hour minute time_hour #&gt; 0 2013 1 1 ... 5 15 2013-01-01 10:00:00+00:00 #&gt; 1 2013 1 1 ... 5 29 2013-01-01 10:00:00+00:00 #&gt; 2 2013 1 1 ... 5 40 2013-01-01 10:00:00+00:00 #&gt; 3 2013 1 1 ... 5 45 2013-01-01 10:00:00+00:00 #&gt; 4 2013 1 1 ... 6 0 2013-01-01 11:00:00+00:00 #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 ... 14 55 2013-09-30 18:00:00+00:00 #&gt; 336772 2013 9 30 ... 22 0 2013-10-01 02:00:00+00:00 #&gt; 336773 2013 9 30 ... 12 10 2013-09-30 16:00:00+00:00 #&gt; 336774 2013 9 30 ... 11 59 2013-09-30 15:00:00+00:00 #&gt; 336775 2013 9 30 ... 8 40 2013-09-30 12:00:00+00:00 #&gt; #&gt; [332577 rows x 19 columns] Standardise to compute per group metrics: (popular_dests. query(&#39;arr_delay &gt; 0&#39;). assign(prop_delay = lambda x: x.arr_delay / x.groupby(&#39;dest&#39;).arr_delay.transform(&#39;sum&#39;)). filter([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;, &#39;dest&#39;, &#39;arr_delay&#39;, &#39;prop_delay&#39;]) ) #&gt; year month day dest arr_delay prop_delay #&gt; 0 2013 1 1 IAH 11.0 0.000111 #&gt; 1 2013 1 1 IAH 20.0 0.000201 #&gt; 2 2013 1 1 MIA 33.0 0.000235 #&gt; 5 2013 1 1 ORD 12.0 0.000042 #&gt; 6 2013 1 1 FLL 19.0 0.000094 #&gt; ... ... ... ... ... ... ... #&gt; 336759 2013 9 30 BNA 7.0 0.000057 #&gt; 336760 2013 9 30 STL 57.0 0.000717 #&gt; 336762 2013 9 30 SFO 42.0 0.000204 #&gt; 336763 2013 9 30 MCO 130.0 0.000631 #&gt; 336768 2013 9 30 BOS 1.0 0.000005 #&gt; #&gt; [131106 rows x 6 columns] 5.7.1 Exercises Which plane (tailnum) has the worst on-time record? What time of day should you fly if you want to avoid delays as much as possible? For each destination, compute the total minutes of delay. For each flight, compute the proportion of the total delay for its destination. Delays are typically temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Explore how the delay of a flight is related to the delay of the immediately preceding flight. Look at each destination. Can you find flights that are suspiciously fast? (i.e. flights that represent a potential data entry error). Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air? Find all destinations that are flown by at least two carriers. Use that information to rank the carriers. For each plane, count the number of flights before the first delay of greater than 1 hour. "],
["workflow-scripts.html", "6 Workflow: scripts 6.1 Running code", " 6 Workflow: scripts So far you’ve been using the console to run code. That’s a great place to start, but you’ll find it gets cramped pretty quickly as you create more complex ggplot2 graphics and dplyr pipes. To give yourself more room to work, it’s a great idea to use the script editor. Open it up either by clicking the File menu, and selecting New File or using the keyboard shortcut Cmd/Ctrl + N. Now you’ll see one pane with a tab named ‘Untitled-1’. After saving the file as *.py you can start a new coding cell by typing # %% in your script which will prompt VS Code to give you an interactive Python framework. You will see options to ‘Run Cell’, ‘Run Below’, and ‘Debug cell’ just above the text you typed. Clicking the ‘Run Cell’ will open the Python Interactive console in a side panel. : The script editor is a great place to put code you care about. Keep experimenting in the console, but once you have written code that works and does what you want, put it in the script editor. VS Code does not automatically save the contents of the editor by default. However, you can turn on autosave in the settings and when you quit VS Code it will save and automatically load it when you re-open. Nevertheless, it’s a good idea to save your scripts regularly and to back them up. 6.1 Running code The script editor is also a great place to build up complex Altair charts or long sequences of pandas manipulations. The key to using the script editor effectively is to memorise one of the most important keyboard shortcuts: shift + Enter. This executes the current cell from your Python script in the console. I recommend that you always start your script with the packages that you need. That way, if you share your code with others, they can easily see what packages they need to install. Note, however, that you should never include os.chdir() in a script that you share. It’s very antisocial to change settings on someone else’s computer! When working through future chapters, I highly recommend starting in the editor and practicing your keyboard shortcuts. Over time, sending code to the console in this way will become so natural that you won’t even think about it. "],
["exploratory-data-analysis.html", "7 Exploratory Data Analysis 7.1 Introduction 7.2 Questions 7.3 Variation 7.4 Missing values 7.5 Covariation 7.6 Patterns and models 7.7 Altair calls 7.8 Learning more", " 7 Exploratory Data Analysis 7.1 Introduction This chapter will show you how to use visualisation and transformation to explore your data in a systematic way, a task that statisticians call exploratory data analysis, or EDA for short. EDA is an iterative cycle. You: Generate questions about your data. Search for answers by visualising, transforming, and modelling your data. Use what you learn to refine your questions and/or generate new questions. EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends. As your exploration continues, you will home in on a few particularly productive areas that you’ll eventually write up and communicate to others. EDA is an important part of any data analysis, even if the questions are handed to you on a platter, because you always need to investigate the quality of your data. Data cleaning is just one application of EDA: you ask questions about whether your data meets your expectations or not. To do data cleaning, you’ll need to deploy all the tools of EDA: visualisation, transformation, and modelling. 7.1.1 Prerequisites In this chapter we’ll combine what you’ve learned about pandas and Altair to interactively ask questions, answer them with data, and then ask new questions. import pandas as pd import altair as alt import numpy as np from scipy import stats alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) Then make sure you have the diamonds data loaded. diamonds = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot;) diamonds[&#39;cut&#39;] = pd.Categorical(diamonds.cut, ordered = True, categories = [&quot;Fair&quot;, &quot;Good&quot;, &quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot; ]) diamonds[&#39;color&#39;] = pd.Categorical(diamonds.color, ordered = True, categories = [&quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;]) diamonds[&#39;clarity&#39;] = pd.Categorical(diamonds.clarity, ordered = True, categories = [&quot;I1&quot;, &quot;SI2&quot;, &quot;SI1&quot;, &quot;VS2&quot;, &quot;VS1&quot;, &quot;VVS2&quot;, &quot;VVS1&quot;, &quot;IF&quot;]) 7.2 Questions “There are no routine statistical questions, only questionable statistical routines.” — Sir David Cox “Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise.” — John Tukey Your goal during EDA is to develop an understanding of your data. The easiest way to do this is to use questions as tools to guide your investigation. When you ask a question, the question focuses your attention on a specific part of your dataset and helps you decide which graphs, models, or transformations to make. EDA is fundamentally a creative process. And like most creative processes, the key to asking quality questions is to generate a large quantity of questions. It is difficult to ask revealing questions at the start of your analysis because you do not know what insights are contained in your dataset. On the other hand, each new question that you ask will expose you to a new aspect of your data and increase your chance of making a discovery. You can quickly drill down into the most interesting parts of your data—and develop a set of thought-provoking questions—if you follow up each question with a new question based on what you find. There is no rule about which questions you should ask to guide your research. However, two types of questions will always be useful for making discoveries within your data. You can loosely word these questions as: What type of variation occurs within my variables? What type of covariation occurs between my variables? The rest of this chapter will look at these two questions. I’ll explain what variation and covariation are, and I’ll show you several ways to answer each question. To make the discussion easier, let’s define some terms: A variable is a quantity, quality, or property that you can measure. A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement. An observation is a set of measurements made under similar conditions (you usually make all of the measurements in an observation at the same time and on the same object). An observation will contain several values, each associated with a different variable. I’ll sometimes refer to an observation as a data point. Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row. So far, all of the data that you’ve seen has been tidy. In real-life, most data isn’t tidy, so we’ll come back to these ideas again in tidy data. 7.3 Variation Variation is the tendency of the values of a variable to change from measurement to measurement. You can see variation easily in real life; if you measure any continuous variable twice, you will get two different results. This is true even if you measure quantities that are constant, like the speed of light. Each of your measurements will include a small amount of error that varies from measurement to measurement. Categorical variables can also vary if you measure across different subjects (e.g. the eye colors of different people), or different times (e.g. the energy levels of an electron at different moments). Every variable has its own pattern of variation, which can reveal interesting information. The best way to understand that pattern is to visualise the distribution of the variable’s values. 7.3.1 Visualising distributions How you visualise the distribution of a variable will depend on whether the variable is categorical or continuous. A variable is categorical if it can only take one of a small set of values. In Python, categorical variables are usually saved as Category or object datatypes. To examine the distribution of a categorical variable, use a bar chart: We want the height of the bars to display how many observations occurred with each x value. You can compute these values with size(): chart_dat = (diamonds. groupby(&#39;cut&#39;). agg(count = (&#39;carat&#39;, &#39;size&#39;)). reset_index()) chart = (alt.Chart(chart_dat). encode( x = &#39;cut&#39;, y = &#39;count&#39; ). mark_bar(). properties(width = 400) ) chart.save(&quot;screenshots/altair_diamonds_barchart.png&quot;) A variable is continuous if it can take any of an infinite set of ordered values. Numbers and date-times are two examples of continuous variables. To examine the distribution of a continuous variable, use a histogram: chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;carat&#39;, bin = alt.Bin(step = 0.5)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_hist_bin.png&quot;) A histogram divides the x-axis into equally spaced bins and then uses the height of a bar to display the number of observations that fall in each bin. In the graph above, the tallest bar shows that almost 30,000 observations have a carat value between 0.25 and 0.75, which are the left and right edges of the bar. You can set the width of the intervals in a histogram with the alt.Bin() and the step argument, which is measured in the units of the x variable. You should always explore a variety of binwidths when working with histograms, as different binwidths can reveal different patterns. For example, here is how the graph above looks when we zoom into just the diamonds with a size of less than three carats and choose a smaller binwidth. smaller = diamonds.query(&#39;carat &lt; 3&#39;) chart = (alt.Chart(smaller). encode( x = alt.X(&#39;carat&#39;, bin = alt.Bin(step = 0.1)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_hist_smallbin.png&quot;) If you wish to see multiple histograms, Altair doesn’t have a straightforward way to overly histograms. We recommend using facet(). facet() performs the same calculation for each group within the facet varaible. chart = (alt.Chart(smaller). encode( x = alt.X(&#39;carat&#39;, bin = alt.Bin(step = 0.1)), y = &#39;count()&#39;, color = &#39;cut&#39;). mark_bar(). facet(facet = &#39;cut&#39;, columns = 2) ) chart.save(&quot;screenshots/altair_diamonds_facet_hist.png&quot;) There are a few challenges with this type of plot, which we will come back to in visualising a categorical and a continuous variable. Now that you can visualise variation, what should you look for in your plots? And what type of follow-up questions should you ask? I’ve put together a list below of the most useful types of information that you will find in your graphs, along with some follow-up questions for each type of information. The key to asking good follow-up questions will be to rely on your curiosity (What do you want to learn more about?) as well as your skepticism (How could this be misleading?). 7.3.2 Typical values In both bar charts and histograms, tall bars show the common values of a variable, and shorter bars show less-common values. Places that do not have bars reveal values that were not seen in your data. To turn this information into useful questions, look for anything unexpected: Which values are the most common? Why? Which values are rare? Why? Does that match your expectations? Can you see any unusual patterns? What might explain them? As an example, the histogram below suggests several interesting questions: Why are there more diamonds at whole carats and common fractions of carats? Why are there more diamonds slightly to the right of each peak than there are slightly to the left of each peak? Why are there no diamonds bigger than 3 carats? chart = (alt.Chart(smaller). encode( x = alt.X(&#39;carat&#39;, bin = alt.Bin(step = 0.01)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_hist_smallestbin.png&quot;) Clusters of similar values suggest that subgroups exist in your data. To understand the subgroups, ask: How are the observations within each cluster similar to each other? How are the observations in separate clusters different from each other? How can you explain or describe the clusters? Why might the appearance of clusters be misleading? The histogram below shows the length (in minutes) of 272 eruptions of the Old Faithful Geyser in Yellowstone National Park. Eruption times appear to be clustered into two groups: there are short eruptions (of around 2 minutes) and long eruptions (4-5 minutes), but little in between. faithful = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/faithful/faithful.csv&quot;) chart = (alt.Chart(faithful). encode( x = alt.X(&#39;eruptions&#39;, bin = alt.Bin(step = 0.25)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_faithful_hist.png&quot;) Many of the questions above will prompt you to explore a relationship between variables, for example, to see if the values of one variable can explain the behavior of another variable. We’ll get to that shortly. 7.3.3 Unusual values Outliers are observations that are unusual; data points that don’t seem to fit the pattern. Sometimes outliers are data entry errors; other times outliers suggest important new science. When you have a lot of data, outliers are sometimes difficult to see in a histogram. For example, take the distribution of the y variable from the diamonds dataset. The only evidence of outliers is the unusually wide limits on the x-axis. chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;y&#39;, bin = alt.Bin(step = 0.5)), y = &#39;count()&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_y_hist.png&quot;) There are so many observations in the common bins that the rare bins are so short that you can’t see them (although maybe if you stare intently at 0 you’ll spot something). To make it easy to see the unusual values, we need to zoom to small values of the y-axis with alt.Scale() and the argument clip set to True within mark_bar() (note that Altair has other axis options): chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;y&#39;, bin = alt.Bin(step = 0.5)), y = alt.Y(&#39;count()&#39;, scale = alt.Scale(domain = (0, 50)))). mark_bar(clip = True) ) chart.save(&quot;screenshots/altair_diamonds_y_hist_domain.png&quot;) (alt.Scale() can be used within alt.Y() and alt.X() for zooming on either axis. Altair also has a clamp argument that functions that work slightly differently: it stacks all the data right at the limits. But it does not work with mark_bar()) This allows us to see that there are three unusual values: 0, ~30, and ~60. We pluck them out with dplyr: unusual = (diamonds. query(&#39;y &lt; 3 | y &gt; 20&#39;). filter([&#39;price&#39;, &#39;x&#39;, &#39;y&#39;, &#39;z&#39;]). sort_values(&#39;y&#39;) ) unusual #&gt; price x y z #&gt; 11963 5139 0.00 0.0 0.00 #&gt; 15951 6381 0.00 0.0 0.00 #&gt; 24520 12800 0.00 0.0 0.00 #&gt; 26243 15686 0.00 0.0 0.00 #&gt; 27429 18034 0.00 0.0 0.00 #&gt; 49556 2130 0.00 0.0 0.00 #&gt; 49557 2130 0.00 0.0 0.00 #&gt; 49189 2075 5.15 31.8 5.12 #&gt; 24067 12210 8.09 58.9 8.06 The y variable measures one of the three dimensions of these diamonds, in mm. We know that diamonds can’t have a width of 0mm, so these values must be incorrect. We might also suspect that measurements of 32mm and 59mm are implausible: those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars! It’s good practice to repeat your analysis with and without the outliers. If they have minimal effect on the results, and you can’t figure out why they’re there, it’s reasonable to replace them with missing values, and move on. However, if they have a substantial effect on your results, you shouldn’t drop them without justification. You’ll need to figure out what caused them (e.g. a data entry error) and disclose that you removed them in your write-up. 7.3.4 Exercises Explore the distribution of each of the x, y, and z variables in diamonds. What do you learn? Think about a diamond and how you might decide which dimension is the length, width, and depth. Explore the distribution of price. Do you discover anything unusual or surprising? (Hint: Carefully think about the step or the binwidth and make sure you try a wide range of values.) How many diamonds are 0.99 carat? How many are 1 carat? What do you think is the cause of the difference? 7.4 Missing values If you’ve encountered unusual values in your dataset, and simply want to move on to the rest of your analysis, you have two options. Drop the entire row with the strange values: diamonds2 = diamonds.query(&#39;3 &lt;= y &lt;= 20&#39;) I don’t recommend this option because just because one measurement is invalid, doesn’t mean all the measurements are. Additionally, if you have low quality data, by time that you’ve applied this approach to every variable you might find that you don’t have any data left! Instead, I recommend replacing the unusual values with missing values. The easiest way to do this is to use assign() to replace the variable with a modified copy. You can use the np.where() function to replace unusual values with np.nan: diamonds2 = diamonds.assign( y = lambda x: np.where((x.y &lt; 3) | (x.y &gt; 20), np.nan, x.y) ) np.where() has three arguments. The first argument condition should be a logical vector. The result will contain the value of the second argument, yes, when condition is True, and the value of the third argument, no, when it is False. Altair has two ways to handled missing values, np.nan. It’s not obvious where you should plot missing values, so Altair excludes them in the plot. If you would like to treat the missing values as 0 then you can use invalid = None. chart = (alt.Chart(diamonds2). encode( x = &#39;x&#39;, y = &#39;y&#39;). mark_point() ) chart.save(&quot;screenshots/altair_diamonds2_missing.png&quot;) To plot np.nan as 0 then set invalid = None: chart = (alt.Chart(diamonds2). encode( x = &#39;x&#39;, y = &#39;y&#39;). mark_circle(invalid = None) ) chart.save(&quot;screenshots/altair_diamonds2_zero.png&quot;) Other times you want to understand what makes observations with missing values different to observations with recorded values. For example, in flights, missing values in the dep_time variable indicate that the flight was cancelled. So you might want to compare the scheduled departure times for cancelled and non-cancelled times. You can do this by making a new variable with pd.isna(). flights_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/flights/flights.csv&quot; flights = pd.read_csv(flights_url) flights[&#39;time_hour&#39;] = pd.to_datetime(flights.time_hour, format = &quot;%Y-%m-%d %H:%M:%S&quot;) pdat = flights.assign( cancelled = lambda x: pd.isna(x.dep_time), sched_hour = lambda x: x.sched_dep_time // 100, sched_minute = lambda x: x.sched_dep_time % 100, sched_dep_time = lambda x: x.sched_hour + x.sched_minute / 60 ) chart = (alt.Chart(pdat). encode( x = alt.X(&#39;sched_dep_time&#39;, bin = alt.Bin(step = .25)), y = &#39;count()&#39;, color = &#39;cancelled&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_flights_scheduled.png&quot;) However this plot isn’t great because there are many more non-cancelled flights than cancelled flights. In the next section we’ll explore some techniques for improving this comparison. 7.4.1 Exercises What does invalid = None do in Altair? 7.5 Covariation If variation describes the behavior within a variable, covariation describes the behavior between variables. Covariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is to visualise the relationship between two or more variables. How you do that should again depend on the type of variables involved. 7.5.1 A categorical and continuous variable It’s common to want to explore the distribution of a continuous variable broken down by a categorical variable, as in the previous frequency polygon. The default appearance of geom_freqpoly() is not that useful for that sort of comparison because the height is given by the count. That means if one of the groups is much smaller than the others, it’s hard to see the differences in shape. For example, let’s explore how the price of a diamond varies with its quality: chart = (alt.Chart(diamonds). encode( x= alt.X(&#39;price&#39;, bin = alt.Bin(step = 500)), y = &#39;count()&#39;, color = &#39;cut&#39;). mark_bar() ) chart.save(&quot;screenshots/altair_diamonds_price_cut.png&quot;) It’s hard to see the difference in distribution because the overall counts differ so much: chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;cut:O&#39;, scale=alt.Scale(domain=[&#39;Fair&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Premium&#39;, &#39;Ideal&#39;])), y = &#39;count()&#39;). mark_bar(). properties(width = 400)) chart.save(&quot;screenshots/altair_diamonds_price_cut_bar.png&quot;) To make the comparison easier we need to create a chart with density displayed on the y-axis, which is the count standardised so that the area under each curve is one. To do this we must use transform_density which is one of many transform_ functions in Altair. chart = (alt.Chart(diamonds). transform_density( density = &#39;price&#39;, bandwidth = 500, counts = True, steps = 500, as_ = [&#39;price&#39;, &#39;density&#39;], groupby = [&#39;cut&#39;]). encode( x = &#39;price&#39;, y = alt.Y(&#39;density:Q&#39;, stack = &#39;zero&#39;), color = alt.Color(&#39;cut:O&#39;, scale=alt.Scale(scheme=&#39;dark2&#39;, domain=[&#39;Fair&#39;, &#39;Good&#39;, &#39;Very Good&#39;, &#39;Premium&#39;, &#39;Ideal&#39;]), )). mark_bar(opacity = .20) ) chart.save(&quot;screenshots/altair_diamonds_price_density.png&quot;) There’s something rather surprising about this plot - it appears that fair diamonds (the lowest quality) have the highest average price! But maybe that’s because frequency polygons are a little hard to interpret - there’s a lot going on in this plot. Another alternative to display the distribution of a continuous variable broken down by a categorical variable is the boxplot. A boxplot is a type of visual shorthand for a distribution of values that is popular among statisticians. Each boxplot consists of: A box that stretches from the 25th percentile of the distribution to the 75th percentile, a distance known as the interquartile range (IQR). In the middle of the box is a line that displays the median, i.e. 50th percentile, of the distribution. These three lines give you a sense of the spread of the distribution and whether or not the distribution is symmetric about the median or skewed to one side. Visual points that display observations that fall more than 1.5 times the IQR from either edge of the box. These outlying points are unusual so are plotted individually. A line (or whisker) that extends from each end of the box and goes to the farthest non-outlier point in the distribution. Let’s take a look at the distribution of price by cut using geom_boxplot(): chart = (alt.Chart(diamonds). encode( x = &#39;cut&#39;, y = &#39;price&#39;). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_boxplot_2.png&quot;) We see much less information about the distribution, but the boxplots are much more compact so we can more easily compare them (and fit more on one plot). It supports the counterintuitive finding that better quality diamonds are cheaper on average! In the exercises, you’ll be challenged to figure out why. cut is an ordered factor: fair is worse than good, which is worse than very good and so on. Many categorical variables don’t have such an intrinsic order, so you might want to reorder them to make a more informative display. One way to do that is within the alt.X() or alt.Y() functions with the sort argument that accepts a string of the encoding channel used for the sort (e.g. 'x' or 'y') with an optional minus prefix for descending order. Another options that will work with all Altair mark types is to use pandas to define the ordering of the categorical levels to use in alt.Scale() with the domain argument. For example, take the class variable in the mpg dataset. You might be interested to know how highway mileage varies across classes: mpg = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/mpg/mpg.csv&quot;) chart = (alt.Chart(mpg). encode( x = &#39;class&#39;, y = &#39;hwy&#39;). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_boxplot_3.png&quot;) To make the trend easier to see, we can reorder class based on the median value of hwy: index = list(mpg.groupby(&#39;class&#39;).median().sort_values(&#39;hwy&#39;).index) chart = (alt.Chart(mpg). encode( x = alt.X(&#39;class&#39;, scale = alt.Scale(domain = index)), y = &#39;hwy&#39;). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_boxplot_4.png&quot;) If you have long variable names, mark_boxplot() will work better if you flip it 90°. You can do that with changing the x and y encodings. chart = (alt.Chart(mpg). encode( y = alt.Y(&#39;class&#39;, scale = alt.Scale(domain = index)), x = &#39;hwy&#39;). mark_boxplot(size = 25). properties(height = 300) ) chart.save(&quot;screenshots/altair_boxplot_5.png&quot;) 7.5.1.1 Exercises Use what you’ve learned to improve the visualisation of the departure times of cancelled vs. non-cancelled flights. What variable in the diamonds dataset is most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive? 7.5.2 Two categorical variables To visualise the covariation between categorical variables, you’ll need to count the number of observations for each combination. One way to do that is to compute the count with pandas: chart_dat = (diamonds. groupby([&#39;color&#39;, &#39;cut&#39;]). size(). reset_index(name = &#39;n&#39;) ) chart_dat.head() #&gt; color cut n #&gt; 0 D Fair 163 #&gt; 1 D Good 662 #&gt; 2 D Very Good 1513 #&gt; 3 D Premium 1603 #&gt; 4 D Ideal 2834 Then visualise with geom_tile() and the fill aesthetic: chart = (alt.Chart(chart_dat). encode( x = &#39;color&#39;, y = &#39;cut&#39;, color = &#39;n&#39;, stroke = alt.value(&#39;grey&#39;)). mark_rect() ) chart.save(&quot;screenshots/altair_heatmap.png&quot;) If the categorical variables are unordered, you might want to simultaneously reorder the rows and columns in order to more clearly reveal interesting patterns. 7.5.2.1 Exercises How could you rescale the count dataset above to more clearly show the distribution of cut within colour, or colour within cut? Use mark_rect() together with pandas to explore how average flight delays vary by destination and month of year. What makes the plot difficult to read? How could you improve it? 7.5.3 Two continuous variables You’ve already seen one great way to visualise the covariation between two continuous variables: draw a scatterplot with mark_point(). You can see covariation as a pattern in the points. For example, you can see an exponential relationship between the carat size and price of a diamond. chart = (alt.Chart(diamonds). encode( x = &#39;carat&#39;, y = &#39;price&#39;). mark_circle() ) chart.save(&quot;screenshots/altair_diamonds_scatter_eda.png&quot;) Scatterplots become less useful as the size of your dataset grows, because points begin to overplot, and pile up into areas of uniform black (as above). You’ve already seen one way to fix the problem: using the opacity argument to add transparency. chart = (alt.Chart(diamonds). encode( x = &#39;carat&#39;, y = &#39;price&#39;). mark_circle(opacity = 1/100) ) chart.save(&quot;screenshots/altair_diamonds_scatter_eda_2.png&quot;) But using transparency can be challenging for very large datasets. Another solution is to use bins. Previously you used mark_histogram() to bin in one dimension. We will need to use pandas to bin in two directions. First create two new columns that bin price and carat. chart_dat = (smaller. assign( price_cut = lambda x: pd.cut(x.price, bins = np.arange(0, 20000, step = 1000), labels = np.arange(0, 19000, step = 1000)), carat_cut = lambda x: pd.cut(x.carat, bins = np.arange(0, 5.4, step = .2), labels = np.arange(0, 5.2, step = .2)) )) chart_dat.head() #&gt; carat cut color clarity depth ... x y z price_cut carat_cut #&gt; 0 0.23 Ideal E SI2 61.5 ... 3.95 3.98 2.43 0 0.2 #&gt; 1 0.21 Premium E SI1 59.8 ... 3.89 3.84 2.31 0 0.2 #&gt; 2 0.23 Good E VS1 56.9 ... 4.05 4.07 2.31 0 0.2 #&gt; 3 0.29 Premium I VS2 62.4 ... 4.20 4.23 2.63 0 0.2 #&gt; 4 0.31 Good J SI2 63.3 ... 4.34 4.35 2.75 0 0.2 #&gt; #&gt; [5 rows x 12 columns] Then create the binned counts and replace all zero counts with np.nan. chart_dat_binned = (chart_dat. groupby([&#39;carat_cut&#39;, &#39;price_cut&#39;]). size(). reset_index(name = &#39;n&#39;)) chart_dat_binned[&#39;n&#39;].replace(to_replace = 0, value = np.nan, inplace = True) chart_dat_binned.head() #&gt; carat_cut price_cut n #&gt; 0 0.0 0 12.0 #&gt; 1 0.0 1000 NaN #&gt; 2 0.0 2000 NaN #&gt; 3 0.0 3000 NaN #&gt; 4 0.0 4000 NaN mark_rect can then use the divided coordinate plane (2d bins) and then use a fill color to display how many points fall into each bin. Notice the use of `sort = ‘y’’ to reorient the y-axis. chart = (alt.Chart(chart_dat_binned). encode( x = &#39;carat_cut&#39;, y = alt.Y(&#39;price_cut&#39;, sort = &#39;-y&#39;), color = &#39;n:Q&#39;). mark_rect() ) chart.save(&quot;screenshots/altair_diamonds_scatter_binned.png&quot;) Another option is to bin one continuous variable so it acts like a categorical variable. Then you can use one of the techniques for visualising the combination of a categorical and a continuous variable that you learned about. For example, you could bin carat and then for each group, display a boxplot: chart = (alt.Chart(chart_dat). encode( x = &#39;carat_cut:O&#39;, y = &#39;price&#39;). mark_boxplot(). properties(width = 300) ) chart.save(&quot;screenshots/altair_diamonds_scatter_binned_boxplot.png&quot;) pd.cut(), as used above, divides x into bins of equal widths. By default, boxplots look roughly the same (apart from number of outliers) regardless of how many observations there are, so it’s difficult to tell that each boxplot summarises a different number of points. One approach is to display approximately the same number of points in each bin. That’s the job of pd.qcut(): chart_dat = smaller.assign(carat_cut = lambda x: pd.qcut(x.carat,10, labels = False)) chart = (alt.Chart(chart_dat). encode( x = &#39;carat_cut:O&#39;, y = &#39;price&#39;). mark_boxplot(). properties(width = 300) ) chart.save(&quot;screenshots/altair_diamonds_scatter_binned_boxplot_quantiles.png&quot;) 7.5.3.1 Exercises Visualise the distribution of carat, partitioned by price. How does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you? Combine two of the techniques you’ve learned to visualise the combined distribution of cut, carat, and price. Two dimensional plots reveal outliers that are not visible in one dimensional plots. For example, some points in the plot below have an unusual combination of x and y values, which makes the points outliers even though their x and y values appear normal when examined separately. chart = (alt.Chart(diamonds). encode( x = alt.X(&#39;x&#39;, scale = alt.Scale(domain = (4, 11))), y = alt.Y(&#39;y&#39;, scale = alt.Scale(domain = (4, 11)))). mark_circle(clip = True)) chart.save(&quot;screenshots/altair_diamonds_scatter_clip.png&quot;) Why is a scatterplot a better display than a binned plot for this case? What does clamp = True in alt.Scale() do due your chart? 7.6 Patterns and models Patterns in your data provide clues about relationships. If a systematic relationship exists between two variables it will appear as a pattern in the data. If you spot a pattern, ask yourself: Could this pattern be due to coincidence (i.e. random chance)? How can you describe the relationship implied by the pattern? How strong is the relationship implied by the pattern? What other variables might affect the relationship? Does the relationship change if you look at individual subgroups of the data? A scatterplot of Old Faithful eruption lengths versus the wait time between eruptions shows a pattern: longer wait times are associated with longer eruptions. The scatterplot also displays the two clusters that we noticed above. chart = (alt.Chart(faithful). encode( x = alt.X(&#39;eruptions&#39;, scale=alt.Scale(zero=False)), y = alt.Y(&#39;waiting&#39;, scale=alt.Scale(zero=False))). mark_circle() ) chart.save(&quot;screenshots/altair_faithful_scatter_clip.png&quot;) Patterns provide one of the most useful tools for data scientists because they reveal covariation. If you think of variation as a phenomenon that creates uncertainty, covariation is a phenomenon that reduces it. If two variables covary, you can use the values of one variable to make better predictions about the values of the second. If the covariation is due to a causal relationship (a special case), then you can use the value of one variable to control the value of the second. Models are a tool for extracting patterns out of data. For example, consider the diamonds data. It’s hard to understand the relationship between cut and price, because cut and carat, and carat and price are tightly related. It’s possible to use a model to remove the very strong relationship between price and carat so we can explore the subtleties that remain. The following code fits a model that predicts price from carat and then computes the residuals (the difference between the predicted value and the actual value). The residuals give us a view of the price of the diamond, once the effect of carat has been removed. import statsmodels.formula.api as smf mod = smf.ols(&#39;np.log(price) ~ np.log(carat)&#39;, data = diamonds).fit() diamonds2 = diamonds.assign(resid = np.exp(mod.resid)) chart = (alt.Chart(diamonds2). encode( x = &#39;carat&#39;, y = &#39;resid&#39;). mark_circle() ) chart.save(&quot;screenshots/altair_diamonds_model_scatter.png&quot;) Once you’ve removed the strong relationship between carat and price, you can see what you expect in the relationship between cut and price: relative to their size, better quality diamonds are more expensive. chart = (alt.Chart(diamonds2). encode( x = &#39;cut&#39;, y = &#39;resid&#39;). mark_boxplot(size = 25). properties(width = 300) ) chart.save(&quot;screenshots/altair_diamonds_model_boxplot.png&quot;) You’ll learn how models, and the modelr package, work in the final part of the book, model. We’re saving modelling for later because understanding what models are and how they work is easiest once you have tools of data wrangling and programming in hand. 7.7 Altair calls As we move on from these introductory chapters, we’ll transition to a more concise expression of Altair code. So far we’ve been very explicit, which is helpful when you are learning: (alt.Chart(faithful). encode( x = alt.X(&#39;eruptions&#39;), y = alt.Y(&#39;waiting&#39;)). mark_circle() ) In the remainder of the book, we won’t supply the x and y names if we are using alt.X() and alt.Y(). That saves typing, and, by reducing the amount of boilerplate, makes it easier to see what’s different between plots. That’s a really important programming concern that we’ll come back in [functions]. In addition, for less complicated charts, we will not use () with each element on it’s own line. Altair guidance also often places the mark_ before the encode(). Rewriting the previous plot more concisely yields: alt.Chart(faithful).encode( alt.X(&#39;eruptions&#39;), alt.Y(&#39;waiting&#39;)).mark_circle() Altair guidance also often places the mark_ before the encode() to allow the final element to be the encoding which can easily be placed on multiple lines without the need of () around the entire command. alt.Chart(faithful).mark_circle().encode( alt.X(&#39;eruptions&#39;), alt.Y(&#39;waiting&#39;)) 7.8 Learning more If you want to learn more about the mechanics of Altair, I’d highly recommend reveiwing the Altair website: https://altair-viz.github.io/getting_started/overview.html. Another useful resource is the Vega-Lite website. "],
["workflow-projects.html", "8 Workflow: projects 8.1 What is real? 8.2 Where does your analysis live? 8.3 Paths and directories 8.4 VS Code workspaces 8.5 Summary", " 8 Workflow: projects One day you will need to quit Python, go do something else and return to your analysis the next day. One day you will be working on multiple analyses simultaneously that all use Python and you want to keep them separate. One day you will need to bring data from the outside world into Python and send numerical results and figures from Python back out into the world. To handle these real life situations, you need to make two decisions: What about your analysis is “real”, i.e. what will you save as your lasting record of what happened? Where does your analysis “live”? 8.1 What is real? As a beginning Python user, it’s OK to consider your environment (i.e. the objects listed in the variables pane) “real”. However, in the long run, you’ll be much better off if you consider your Python scripts as “real”. With your Python scripts (and your data files), you can recreate the environment. It’s much harder to recreate your Python scripts from your environment! You’ll either have to retype a lot of code from memory (making mistakes all the way) or you’ll have to carefully mine your Python history. There’s nothing worse than discovering three months after the fact that you’ve only stored the results of an important calculation in your workspace, not the calculation itself in your code. 8.2 Where does your analysis live? VS Code’s interactive Python has a powerful notion of the working directory. This is where Python looks for files that you ask it to load, and where it will put any files that you ask it to save. The interactive console will default the working directory to the location of your .py script. And you can print this out in Python code by running os.getcwd(): import os os.getcwd() #&gt; &#39;/Users/hathawayj/git/byuidatascience/python4ds&#39; As a beginning Python user, it’s OK to let your home directory, documents directory, or any other weird directory on your computer be Python’s working directory. But you’re six chapters into this book, and you’re no longer a rank beginner. Very soon now you should evolve to organising your analytical projects into directories and, when working on a project, setting Python’s working directory to the associated directory. I do not recommend it, but you can also set the working directory from within Python: os.chdir(&quot;/path/to/my/CoolProject&quot;) But you should never do this because there’s a better way; a way that also puts you on the path to managing your Python data science work like an expert. 8.3 Paths and directories Paths and directories are a little complicated because there are two basic styles of paths: Mac/Linux and Windows. There are three chief ways in which they differ: The most important difference is how you separate the components of the path. Mac and Linux uses slashes (e.g. plots/diamonds.pdf) and Windows uses backslashes (e.g. plots\\diamonds.pdf). Python can work with either type (no matter what platform you’re currently using), but unfortunately, backslashes mean something special to Python, and to get a single backslash in the path, you need to type two backslashes! That makes life frustrating, so I recommend always using the Linux/Mac style with forward slashes. Absolute paths (i.e. paths that point to the same place regardless of your working directory) look different. In Windows they start with a drive letter (e.g. C:) or two backslashes (e.g. \\\\servername) and in Mac/Linux they start with a slash “/” (e.g. /users/hadley). You should never use absolute paths in your scripts, because they hinder sharing: no one else will have exactly the same directory configuration as you. The last minor difference is the place that ~ points to. ~ is a convenient shortcut to your home directory. Windows doesn’t really have the notion of a home directory, so it instead points to your documents directory. 8.4 VS Code workspaces Python experts keep all the files associated with a project together — input data, R scripts, analytical results, figures. This is such a wise and common practice that VS Code has built-in support for this via workspaces. Let’s make a workspace for you to use while you’re working through the rest of this book. Click File &gt; Open and select a newly created folder for your work. New Project, then: Name your folder python4ds and think carefully about which subdirectory you put the folder in. If you don’t store it somewhere sensible, it will be hard to find it in the future! Once this process is complete, you’ll get a new VS Code workspace just for this book. Under the Welcome screen select ‘New File’ and save the file, calling it “diamonds.py”. Upon saving the file as a Python file VS Code will make sure your workspace is setup to work with Python with a few prompts. Check that the “home” directory of your workspace is the current working directory: import os os.getcwd() #&gt; &#39;/Users/hathawayj/Downloads/python4ds&#39; Whenever you refer to a file with a relative path it will look for it here. Now enter the following commands in the script editor, and save the file, calling it “diamonds.py”. Next, run the complete script which will save a PNG, CSV, and JSON file into your project directory. Don’t worry about the details, you’ll learn them later in the book. import pandas as pd import altair as alt alt.data_transformers.enable(&#39;json&#39;) url_path = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot; diamonds = pd.read_csv(url_path) chart = (alt.Chart(diamonds). mark_circle(). encode( x = alt.X(&quot;carat&quot;, bin=True), y = alt.Y(&quot;price&quot;, bin=True), size = &quot;count()&quot; ) ) chart.save(&quot;diamonds.png&quot;) diamonds.to_csv(&quot;diamonds.csv&quot;) Inspect the folder associated with your project — notice the .vscode folder. Double-click that folder to see the default workspace settings for your project. You can read more about VS code workspaces to understand the other available options. In your favorite OS-specific way, search your computer for diamonds.png and you will find the PNG (no surprise) but also the script that created it (diamonds.py). This is huge win! One day you will want to remake a figure or just understand where it came from. If you rigorously save figures to files with Python code and never with the mouse or the clipboard, you will be able to reproduce old work with ease! 8.5 Summary In summary, VS Code workspaces give you a solid workflow that will serve you well in the future: Create an workspace for each data analysis project. Keep data files there; we’ll talk about loading them into Python in data import. Keep scripts there; edit them, run them in bits or as a whole. Save your outputs (plots and cleaned data) there. Only ever use relative paths, not absolute paths. Everything you need is in one place, and cleanly separated from all the other projects that you are working on. "],
["wrangle-intro.html", "9 Introduction", " 9 Introduction In this part of the book, you’ll learn about data wrangling, the art of getting your data into R in a useful form for visualisation and modelling. Data wrangling is very important: without it you can’t work with your own data! There are three main parts to data wrangling: This part of the book proceeds as follows: In [tibbles], you’ll learn about the variant of the data frame that we use in this book: the tibble. You’ll learn what makes them different from regular data frames, and how you can construct them “by hand”. In data import, you’ll learn how to get your data from disk and into R. We’ll focus on plain-text rectangular formats, but will give you pointers to packages that help with other types of data. In tidy data, you’ll learn about tidy data, a consistent way of storing your data that makes transformation, visualisation, and modelling easier. You’ll learn the underlying principles, and how to get your data into a tidy form. Data wrangling also encompasses data transformation, which you’ve already learned a little about. Now we’ll focus on new skills for three specific types of data you will frequently encounter in practice: Relational data will give you tools for working with multiple interrelated datasets. Strings will introduce regular expressions, a powerful tool for manipulating strings. Factors are how R stores categorical data. They are used when a variable has a fixed set of possible values, or when you want to use a non-alphabetical ordering of a string. Dates and times will give you the key tools for working with dates and date-times. "],
["dataframe.html", "10 DataFrame 10.1 Introduction 10.2 Creating tibbles 10.3 Printing 10.4 Exercises", " 10 DataFrame 10.1 Introduction Throughout this book we work with pandas _DataFrame__. Python is an old language, and the tools for data in Python that were useful 10 or 20 years ago now get in your way. Here we will describe the DataFrame data structure, which provides opinionated data frames that make working in with data easier. In most places, I’ll use the term data frame. If this chapter leaves you wanting to learn more about data frames, you might enjoy reading the documentation. 10.1.1 Prerequisites In this chapter we’ll explore the DataFrame, a foundational element of pandas. import pandas as pd import pandas as pd import altair as alt import numpy as np 10.2 Creating tibbles Almost all of the functions that you’ll use in this book produce data frames, as data frames are one of the unifying features of pandas. You can create a new data frames from individual vectors with pd.DataFrame(). pd.DataFrame() will automatically recycle inputs of length 1 but does not allow you to refer to variables that you just created. pd.DataFrame({ &#39;x&#39;: [1, 2, 3, 4, 5], &#39;y&#39;: 1} ).assign(z = lambda x: x.x**2 + x.y) #&gt; x y z #&gt; 0 1 1 2 #&gt; 1 2 1 5 #&gt; 2 3 1 10 #&gt; 3 4 1 17 #&gt; 4 5 1 26 Unlike R data frames, pandas data frames can have column names that are not valid R variable names, aka non-syntactic names. For example, they might not start with a letter, or they might contain unusual characters like a space. Notice the use of index as we are passing all scalar values: tb = pd.DataFrame({ &#39;:)&#39;: &#39;smile&#39;, &#39; &#39; : &#39;space&#39;, &#39;2000&#39;: &#39;number&#39;},index=[0]) tb #&gt; :) 2000 #&gt; 0 smile space number Another way to create a tibble is with np.arrray(). Sometimes np.array() makes it possible to lay out small amounts of data in easy to read form. pd.DataFrame(np.array( [[&quot;a&quot;, 2, 3.6], [&quot;b&quot;, 1, 8.5]]), columns = [&#39;x&#39;, &#39;y&#39;, &#39;z&#39;]) #&gt; x y z #&gt; 0 a 2 3.6 #&gt; 1 b 1 8.5 10.3 Printing Data frames have a refined print method that shows only the first and last 5 rows, and all the columns that fit on screen. This makes it much easier to work with large data. tibble( a = lubridate::now() + runif(1e3) * 86400, b = lubridate::today() + runif(1e3) * 30, c = 1:1e3, d = runif(1e3), e = sample(letters, 1e3, replace = TRUE) ) #&gt; # A tibble: 1,000 x 5 #&gt; a b c d e #&gt; &lt;dttm&gt; &lt;date&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 2020-06-25 19:21:49 2020-07-02 1 0.368 n #&gt; 2 2020-06-26 13:26:59 2020-07-07 2 0.612 l #&gt; 3 2020-06-26 07:50:38 2020-07-17 3 0.415 p #&gt; 4 2020-06-25 21:11:55 2020-07-16 4 0.212 m #&gt; 5 2020-06-25 17:36:12 2020-07-13 5 0.733 i #&gt; 6 2020-06-26 04:37:09 2020-07-09 6 0.460 n #&gt; # … with 994 more rows Data frames are designed so that you don’t accidentally overwhelm your console when you print large data frames. But sometimes you need more output than the default display. There are a few options that can help. First, you can return the data frame using .head() on the data frame and control the number of rows (n) of the display. In the interactive Python viewer in VS Code you can scroll to see the other columns. flights.head(20) You can also control the default print behaviour by setting options: pd.set_option(\"display.max_rows\", 101): if more than 101 rows, print only n rows. pd.set_option('precision', 5) will set the number of decimals that are shown. You can see a complete list of options by looking at the pandas help. 10.3.1 Subsetting So far all the tools you’ve learned have worked with complete data frames. If you want to pull out a single variable, you need some new tools, [. [ can extract by name or position. df = pd.DataFrame({ &#39;x&#39;: np.random.uniform(size = 5), &#39;y&#39;: np.random.normal(size = 5)}) # Extract by name as pandas data frame df[[&quot;x&quot;]] # Extract by name as array #&gt; x #&gt; 0 0.024984 #&gt; 1 0.033701 #&gt; 2 0.275419 #&gt; 3 0.523895 #&gt; 4 0.305597 df[&quot;x&quot;] # Extract by position #&gt; 0 0.024984 #&gt; 1 0.033701 #&gt; 2 0.275419 #&gt; 3 0.523895 #&gt; 4 0.305597 #&gt; Name: x, dtype: float64 df.iloc[:, 1] #&gt; 0 -0.357907 #&gt; 1 0.558153 #&gt; 2 -1.630738 #&gt; 3 -0.203377 #&gt; 4 0.414008 #&gt; Name: y, dtype: float64 df[df.columns[1]] #&gt; 0 -0.357907 #&gt; 1 0.558153 #&gt; 2 -1.630738 #&gt; 3 -0.203377 #&gt; 4 0.414008 #&gt; Name: y, dtype: float64 10.4 Exercises If you have the name of a variable stored in an object, e.g. var &lt;- \"mpg\", how can you extract the reference variable from a tibble? Practice referring to non-syntactic names in the following data frame by: Extracting the variable called 1. Plotting a scatterplot of 1 vs 2. Creating a new column called 3 which is 2 divided by 1. "],
["data-import.html", "11 Data import 11.1 Introduction 11.2 Getting started 11.3 Parsing a vector 11.4 Parsing a file 11.5 Writing to a file 11.6 Other types of data", " 11 Data import 11.1 Introduction In this chapter, you’ll learn how to read plain-text rectangular files into Python. Here, we’ll only scratch the surface of data import, but many of the principles will translate to other forms of data. We’ll finish with a few pointers to packages that are useful for other types of data. 11.1.1 Prerequisites In this chapter, you’ll learn how to load flat files in R with the pandas package. import pandas as pd import altair as alt import numpy as np 11.2 Getting started The pandas input/output functions are concerned with turning varied files into pandas data frames for use in Python: pd.read_csv() reads comma delimited files, pd.read_table() reads general delimited files with any delimiter. pd.read_fwf() reads fixed width files. You can specify fields either by their widths with widths or their position with colspecs arguments. These functions all have similar syntax: once you’ve mastered one, you can use the others with ease. For the rest of this chapter we’ll focus on pd.read_csv(). Not only are csv files one of the most common forms of data storage, but once you understand pd.read_csv(), you can easily apply your knowledge to all the other input/output functions in pandas. You can see the full set of functions by reading the pandas documentation. The first argument to pd.read_csv() is the most important: it’s the path to the file to read. heights = pd.read_csv(&quot;data/heights.csv&quot;) You can also supply an inline csv file with the use of StringIO in the io package. StringIO creates a buffer from a string. This is useful for experimenting with pandas and for creating reproducible examples to share with others: from io import StringIO data = StringIO(&quot;&quot;&quot;a,b,c 1,2,3 4,5,6&quot;&quot;&quot;) pd.read_csv(data) #&gt; a b c #&gt; 0 1 2 3 #&gt; 1 4 5 6 In both cases pd.read_csv() uses the first line of the data for the column names, which is a very common convention. There are two cases where you might want to tweak this behaviour: Sometimes there are a few lines of metadata at the top of the file. You can use skiprows = n to skip the first n lines; or use comment = \"#\" to drop all lines that start with (e.g.) #. Both arguments have other methods that can be used - see the pandas documentation. data_metada = StringIO(&quot;&quot;&quot;The first line of metadata The second line of metadata x,y,z 1,2,3&quot;&quot;&quot;) pd.read_csv(data_metada, skiprows = 2) data_comment = StringIO(&quot;&quot;&quot;# A comment I want to skip x,y,z 1,2,3 &quot;&quot;&quot;) pd.read_csv(data_comment, comment = &quot;#&quot;) The data might not have column names. You can use header = None to tell read_csv() not to treat the first row as headings, and instead label them sequentially from 1 to n: data_nonames = StringIO(&quot;&quot;&quot;1,2,3\\n4,5,6&quot;&quot;&quot;) pd.read_csv(data_nonames, header = None) #&gt; 0 1 2 #&gt; 0 1 2 3 #&gt; 1 4 5 6 (\"\\n\" is a convenient shortcut for adding a new line. You’ll learn more about it and other types of string escape in string basics.) Alternatively you can pass names an list of names which will be used as the column names: data_nonames = StringIO(&quot;&quot;&quot;1,2,3\\n4,5,6&quot;&quot;&quot;) pd.read_csv(data_nonames, names = [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;], header = None) #&gt; x y z #&gt; 0 1 2 3 #&gt; 1 4 5 6 Another option that commonly needs tweaking is na: this specifies the value (or values) that are used to represent missing values in your file: data_missing = StringIO(&quot;&quot;&quot;a,b,c\\n1,2,.&quot;&quot;&quot;) pd.read_csv(data_missing, na_values = &quot;.&quot;) #&gt; a b c #&gt; 0 1 2 NaN This is all you need to know to read ~75% of CSV files that you’ll encounter in practice. You can also easily adapt what you’ve learned to read fixed width files with pd.read_fwf(). To read in more challenging files, you’ll need to learn more about how pandas parses each column, turning them into DataFrame objects. 11.2.1 Exercises What function would you use to read a file where fields were separated with “|”? Apart from hte file path, skiprows, and comment, what other arguments does pd.read_csv() have? What are the most important arguments to pd.read_fwf()? Sometimes strings in a CSV file contain commas. To prevent them from causing problems they need to be surrounded by a quoting character, like \" or '. By default, pd.read_csv() assumes that the quoting character will be \". What argument to pd.read_csv() do you need to specify to read the following text into a data frame? &quot;x,y\\n1,&#39;a,b&#39;&quot; 11.3 Parsing a vector Before we get into the details of how pandas reads files from disk, we need to take a little detour to talk about the astype() function. This function casts a pandas object to a more specialised data type like a logical or integer. The pandas package has a special .to_datetime() function to convert dates: pd.Series([&quot;TRUE&quot;, &quot;FALSE&quot;, np.nan]).astype(&#39;bool&#39;) #&gt; 0 True #&gt; 1 True #&gt; 2 True #&gt; dtype: bool pd.Series([&quot;TRUE&quot;, &quot;FALSE&quot;, &quot;FALSE&quot;]).astype(&#39;bool&#39;) #&gt; 0 True #&gt; 1 True #&gt; 2 True #&gt; dtype: bool pd.Series([&quot;1&quot;, &quot;2&quot;, &quot;3&quot;]).astype(&quot;int&quot;) #&gt; 0 1 #&gt; 1 2 #&gt; 2 3 #&gt; dtype: int64 pd.to_datetime(pd.Series([&#39;2010-01-01&#39;, &#39;1970-10-14&#39;]), infer_datetime_format=True) #&gt; 0 2010-01-01 #&gt; 1 1970-10-14 #&gt; dtype: datetime64[ns] These functions are useful in their own right, but are also an important building block for pandas. Once you’ve learned how the individual parsers work in this section, we’ll circle back and see how they fit together to parse a complete file in the next section. Using arsers is mostly a matter of understanding what’s available and how they deal with different types of input. There are eight particularly important dtypes that you can use within astype(): bool, int, and float parse logicals integers, and floating point numbers respectively. The bool dtype has trouble with nan values and may not perform as expected. object handles string and mixed values. category create factors, the data structure that pandas uses to represent categorical variables with fixed and known values. to_datetime() allows you to parse various date &amp; time specifications. These are the most complicated because there are so many different ways of writing dates. to_numeric() allows you to coerce mixed value columns to numeric using errors = coerce pd.to_numeric(pd.Series([&quot;bb&quot;, &quot;1&quot;, &quot;True&quot;, &quot;9&quot;, np.nan]), errors = &#39;coerce&#39;) #&gt; 0 NaN #&gt; 1 1.0 #&gt; 2 NaN #&gt; 3 9.0 #&gt; 4 NaN #&gt; dtype: float64 The following sections describe these parsers in more detail. 11.3.1 Numbers It seems like it should be straightforward to parse a number, but three problems make it tricky: People write numbers differently in different parts of the world. For example, some countries use . in between the integer and fractional parts of a real number, while others use ,. Numbers are often surrounded by other characters that provide some context, like “$1000” or “10%”. Numbers often contain “grouping” characters to make them easier to read, like “1,000,000”, and these grouping characters vary around the world. You will need to use regex expressions to remove the unwanted characters before converting the value to a float. 11.3.2 Strings It seems like strings should be really simple. Unfortunately life isn’t so simple, as there are multiple ways to represent the same string. To understand what’s going on, we need to dive into the details of how computers represent strings. In Python, we can get at the underlying representation of a string using hex() on a binary object: b&#39;Hathaway&#39;.hex() # and convert it back #&gt; &#39;4861746861776179&#39; bytes.fromhex(&#39;4861746861776179&#39;).decode() #&gt; &#39;Hathaway&#39; Each hexadecimal number represents a byte of information: 48 is H, 61 is a, and so on. The mapping from hexadecimal number to character is called the encoding, and in this case the encoding is called ASCII. ASCII does a great job of representing English characters, because it’s the American Standard Code for Information Interchange. Things get more complicated for languages other than English. In the early days of computing there were many competing standards for encoding non-English characters, and to correctly interpret a string you needed to know both the values and the encoding. For example, two common encodings are Latin1 (aka ISO-8859-1, used for Western European languages) and Latin2 (aka ISO-8859-2, used for Eastern European languages). In Latin1, the byte b1 is “±”, but in Latin2, it’s “ą”! Fortunately, today there is one standard that is supported almost everywhere: UTF-8. UTF-8 can encode just about every character used by humans today, as well as many extra symbols (like emoji!). Python often defaults to UTF-8: pandas assumes your data is UTF-8 encoded when you read it, and always uses it when writing. This is a good default, but will fail for data produced by older systems that don’t understand UTF-8. If this happens to you, your strings will look weird when you print them. Sometimes just one or two characters might be messed up; other times you’ll get complete gibberish. For example: Encodings are a rich and complex topic, and I’ve only scratched the surface here. If you’d like to learn more I’d recommend reading the detailed explanation at http://kunststube.net/encoding/. 11.3.3 Category (Factors) Pandas uses category to represent categorical variables that have a known set of possible values. Give pd.Categorical() a Series of known levels to convert other unexpected value that are present to nan: fruit = [&quot;apple&quot;, &quot;banana&quot;] pd.Categorical([&quot;apple&quot;, &quot;banana&quot;, &quot;bananana&quot;], categories = fruit) #&gt; [apple, banana, NaN] #&gt; Categories (2, object): [apple, banana] But if you have many problematic entries, it’s often easier to leave as character vectors and then use the tools you’ll learn about in strings and factors to clean them up. 11.3.4 Dates and times Pandas provides an extensive set of capabilities for working with time using NumPy’s datetime64 and timedelta64 dtypes. You primarily use pd.to_datetime() whether you want a date or a date and time. When called without any additional arguments: pd.to_datetime() expects an ISO8601 date-time. ISO8601 is an international standard in which the components of a date are organised from biggest to smallest: year, month, day, hour, minute, second. pd.to_datetime([&quot;2010-10-01T2010&quot;]) # If time is omitted, it will be set to midnight and only print date. #&gt; DatetimeIndex([&#39;2010-10-01 20:10:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) pd.to_datetime([&quot;2010-10-01&quot;]) #&gt; DatetimeIndex([&#39;2010-10-01&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) This is the most important date/time standard, and if you work with dates and times frequently, I recommend reading https://en.wikipedia.org/wiki/ISO_8601 If these defaults don’t work for your data you can supply your own date-time format, built up of the following pieces: Year %Y (4 digits). %y (2 digits); 00-69 -&gt; 2000-2069, 70-99 -&gt; 1970-1999. Month %m (2 digits). %b (abbreviated name, like “Jan”). %B (full name, “January”). Day %d (2 digits). %e (optional leading space). Time %H 0-23 hour. %I 0-12, must be used with %p. %p AM/PM indicator. %M minutes. %S integer seconds. %OS real seconds. %Z Time zone (as name, e.g. America/Chicago). Beware of abbreviations: if you’re American, note that “EST” is a Canadian time zone that does not have daylight savings time. It is not Eastern Standard Time! We’ll come back to this in time zones. %z (as offset from UTC, e.g. +0800). Non-digits %. skips one non-digit character. %* skips any number of non-digits. The best way to figure out the correct format is to create a few examples in a character vector, and test with one of the parsing functions. For example: pd.to_datetime([&quot;01/02/15&quot;], format = &quot;%m/%d/%y&quot;) #&gt; DatetimeIndex([&#39;2015-01-02&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) pd.to_datetime([&quot;01/02/15&quot;], format = &quot;%d/%m/%y&quot;) #&gt; DatetimeIndex([&#39;2015-02-01&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) pd.to_datetime([&quot;01/02/15&quot;], format = &quot;%y/%m/%d&quot;) #&gt; DatetimeIndex([&#39;2001-02-15&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) The pd.to_datetime() function has a handy argument infer_datetime_format which can often allow you to avoid entring format strings. pd.to_datetime([&quot;01/02/15&quot;], infer_datetime_format = True) #&gt; DatetimeIndex([&#39;2015-01-02&#39;], dtype=&#39;datetime64[ns]&#39;, freq=None) 11.3.5 Exercises What are the most common encodings used in Europe? What are the most common encodings used in Asia? Do some googling to find out. Generate the correct format string to parse each of the following dates and times: d1 = [&quot;January 1, 2010&quot;] d2 = [&quot;2015-Mar-07&quot;] d3 = [&quot;06-Jun-2017&quot;] d4 = [&quot;August 19 (2015)&quot;, &quot;July 1 (2015)&quot;] d5 = [&quot;12/30/14&quot;] # Dec 30, 2014 11.4 Parsing a file Now that you’ve learned how to parse an Series, it’s time to return to the beginning and explore how pandas parses a file. There are two new things that you’ll learn about in this section: How pandas automatically guesses the type of each column. How to override the default specification. 11.4.1 Strategy Pandas uses a heuristic to figure out the type of each column: at first it tries to convert all values to an integer. If there is an error then it moves to the next data type. The last data type is an object which a Series of strings. The heuristic tries each of the following types in the listed order, stopping when it finds a match: integer: contains only numeric characters (and -). float: contains only valid doubles (including numbers like 4.5e-5). bool: contains only “F”, “T”, “FALSE”, “TRUE”, “False” or “True”. In addition, you can use the parse_dates argument to to parse datetime columns during parsing. See the read_csv documentation for guidance. If none of these rules apply, then the column will stay as a Series of strings. When using pd.read_csv(), I highly recommend always supplying dtypes. This ensures that you have a consistent, reproducible, and fast data import script. If you rely on the default guesses and your data changes, pandas will continue to read it in. If you’re having major parsing problems, sometimes it’s easier to just read into a character vector of lines with readlines().Then you can use the string parsing skills you’ll learn later to parse more exotic formats. You can read more about Python’s Input/Output understand readlines(). 11.5 Writing to a file pandas also comes with a useful function for writing data back to disk: to_csv(). to_csv() increase the chances of the output file being read back in correctly by: Always encoding strings in UTF-8. Saving dates and date-times in ISO8601 format so they are easily parsed elsewhere. If you want to export a csv file to Excel, use to_excel(). The most important argument is path (the location to save it). You can also specify how missing values are written with na_rep and if the index is included in the export index = False. df = pd.DataFrame({&#39;name&#39;: [&#39;Raphael&#39;, &#39;Donatello&#39;], &#39;mask&#39;: [&#39;red&#39;, &#39;purple&#39;], &#39;weapon&#39;: [&#39;sai&#39;, &#39;bo staff&#39;]}) df.to_csv(&quot;my_file.csv&quot;, index=False) Note that the type information is lost when you save to csv: This makes CSVs a little unreliable for caching interim results—you need to recreate the column specification every time you load in. We recommend the feather format: The feather format implements a fast binary file format that can be shared across programming languages: df = pd.DataFrame({&#39;name&#39;: [&#39;Raphael&#39;, &#39;Donatello&#39;], &#39;mask&#39;: [&#39;red&#39;, &#39;purple&#39;], &#39;weapon&#39;: [&#39;sai&#39;, &#39;bo staff&#39;]}) df.to_feather(&quot;my_file.ftr&quot;) There is a feather package in R to share files quickly between the two tools. library(feather) write_feather(challenge, &quot;challenge.feather&quot;) read_feather(&quot;challenge.feather&quot;) 11.6 Other types of data To get other types of data into Python, we recommend you review pandas IO tools. "],
["tidy-data.html", "12 Tidy data 12.1 Introduction 12.2 Tidy data 12.3 Pivoting 12.4 Separating and uniting 12.5 Missing values 12.6 Case Study 12.7 Non-tidy data", " 12 Tidy data 12.1 Introduction “Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy “Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham In this chapter, you will learn a consistent way to organise your data in Python, an organisation called tidy data. Getting your data into this format requires some upfront work, but that work pays off in the long term. Once you have tidy data and pandas data frames, you will spend much less time munging data from one representation to another, allowing you to spend more time on the analytic questions at hand. This chapter will give you a practical introduction to tidy data and the accompanying tools in the pandas package. If you’d like to learn more about the underlying theory, you might enjoy the Tidy Data paper published in the Journal of Statistical Software, http://www.jstatsoft.org/v59/i10/paper. 12.1.1 Prerequisites In this chapter we’ll focus on pandas, a package that provides a bunch of tools to help tidy up your messy datasets. import pandas as pd import altair as alt import numpy as np 12.2 Tidy data You can represent the same underlying data in multiple ways. The example below shows the same data organised in four different ways. Each dataset shows the same values of four variables country, year, population, and cases, but each dataset organises the values in a different way. base_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/&quot; table1 = pd.read_csv(&quot;{}table1/table1.csv&quot;.format(base_url)) table2 = pd.read_csv(&quot;{}table2/table2.csv&quot;.format(base_url)) table3 = pd.read_csv(&quot;{}table3/table3.csv&quot;.format(base_url)) table4a = pd.read_csv(&quot;{}table4a/table4a.csv&quot;.format(base_url)) table4b = pd.read_csv(&quot;{}table4b/table4b.csv&quot;.format(base_url)) table5 = pd.read_csv(&quot;{}table5/table5.csv&quot;.format(base_url), dtype = &#39;object&#39;) table1 #&gt; country year cases population #&gt; 0 Afghanistan 1999 745 19987071 #&gt; 1 Afghanistan 2000 2666 20595360 #&gt; 2 Brazil 1999 37737 172006362 #&gt; 3 Brazil 2000 80488 174504898 #&gt; 4 China 1999 212258 1272915272 #&gt; 5 China 2000 213766 1280428583 table2 #&gt; country year type count #&gt; 0 Afghanistan 1999 cases 745 #&gt; 1 Afghanistan 1999 population 19987071 #&gt; 2 Afghanistan 2000 cases 2666 #&gt; 3 Afghanistan 2000 population 20595360 #&gt; 4 Brazil 1999 cases 37737 #&gt; 5 Brazil 1999 population 172006362 #&gt; 6 Brazil 2000 cases 80488 #&gt; 7 Brazil 2000 population 174504898 #&gt; 8 China 1999 cases 212258 #&gt; 9 China 1999 population 1272915272 #&gt; 10 China 2000 cases 213766 #&gt; 11 China 2000 population 1280428583 table3 # Spread across two tibbles #&gt; country year rate #&gt; 0 Afghanistan 1999 745/19987071 #&gt; 1 Afghanistan 2000 2666/20595360 #&gt; 2 Brazil 1999 37737/172006362 #&gt; 3 Brazil 2000 80488/174504898 #&gt; 4 China 1999 212258/1272915272 #&gt; 5 China 2000 213766/1280428583 table4a # cases #&gt; country 1999 2000 #&gt; 0 Afghanistan 745 2666 #&gt; 1 Brazil 37737 80488 #&gt; 2 China 212258 213766 table4b # population #&gt; country 1999 2000 #&gt; 0 Afghanistan 19987071 20595360 #&gt; 1 Brazil 172006362 174504898 #&gt; 2 China 1272915272 1280428583 These are all representations of the same underlying data, but they are not equally easy to use. One dataset, the tidy dataset, will be much easier to work with inside the tidyverse. There are three interrelated rules which make a dataset tidy: Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. Figure 12.1 shows the rules visually. Figure 12.1: Following three rules makes a dataset tidy: variables are in columns, observations are in rows, and values are in cells. These three rules are interrelated because it’s impossible to only satisfy two of the three. That interrelationship leads to an even simpler set of practical instructions: Put each dataset in a tibble. Put each variable in a column. In this example, only table1 is tidy. It’s the only representation where each column is a variable. Why ensure that your data is tidy? There are two main advantages: There’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity. There’s a specific advantage to placing variables in columns because it allows pandas’ and NumPy’s vectorised nature to shine. As you learned in assign and aggregate functions, most built-in R functions work with vectors of values. That makes transforming tidy data feel particularly natural. Altair and pandas work well with tidy data. Here are a couple of small examples showing how you might work with table1. # Compute rate per 10,000 table1.assign( rate = lambda x: x.cases / x.population * 1000 ) # Compute cases per year #&gt; country year cases population rate #&gt; 0 Afghanistan 1999 745 19987071 0.037274 #&gt; 1 Afghanistan 2000 2666 20595360 0.129447 #&gt; 2 Brazil 1999 37737 172006362 0.219393 #&gt; 3 Brazil 2000 80488 174504898 0.461236 #&gt; 4 China 1999 212258 1272915272 0.166750 #&gt; 5 China 2000 213766 1280428583 0.166949 (table1. groupby(&#39;year&#39;). agg(n = (&#39;cases&#39;, &#39;sum&#39;)). reset_index()) # Visualise changes over time # import altair as alt #&gt; year n #&gt; 0 1999 250740 #&gt; 1 2000 296920 base_chart = (alt.Chart(table1). encode(alt.X(&#39;year&#39;), alt.Y(&#39;cases&#39;), detail = &#39;country&#39;)) chart = base_chart.mark_line() + base_chart.encode(color = &#39;country&#39;).mark_circle() chart.save(&quot;screenshots/altair_table1.png&quot;) 12.2.1 Exercises Using prose, describe how the variables and observations are organised in each of the sample tables. Compute the rate for table2, and table4a + table4b. You will need to perform four operations: Extract the number of TB cases per country per year. Extract the matching population per country per year. Divide cases by population, and multiply by 10000. Store back in the appropriate place. Which representation is easiest to work with? Which is hardest? Why? Recreate the plot showing change in cases over time using table2 instead of table1. What do you need to do first? 12.3 Pivoting The principles of tidy data seem so obvious that you might wonder if you’ll ever encounter a dataset that isn’t tidy. Unfortunately, however, most data that you will encounter will be untidy. There are two main reasons: Most people aren’t familiar with the principles of tidy data, and it’s hard to derive them yourself unless you spend a lot of time working with data. Data is often organised to facilitate some use other than analysis. For example, data is often organised to make entry as easy as possible. This means for most real analyses, you’ll need to do some tidying. The first step is always to figure out what the variables and observations are. Sometimes this is easy; other times you’ll need to consult with the people who originally generated the data. The second step is to resolve one of two common problems: One variable might be spread across multiple columns. One observation might be scattered across multiple rows. Typically a dataset will only suffer from one of these problems; it’ll only suffer from both if you’re really unlucky! To fix these problems, you’ll need two functions in pandas: melt(), pivot(), and pivot_table(). There are two additional functions called stack() and unstack() that use multi-index columns and rows. Pandas provides a guide to reshaping and pivot tables in their user guide. 12.3.1 Longer (melt()) A common problem is a dataset where some of the column names are not names of variables, but values of a variable. Take table4a: the column names 1999 and 2000 represent values of the year variable, the values in the 1999 and 2000 columns represent values of the cases variable, and each row represents two observations, not one. table4a #&gt; country 1999 2000 #&gt; 0 Afghanistan 745 2666 #&gt; 1 Brazil 37737 80488 #&gt; 2 China 212258 213766 To tidy a dataset like this, we need to stack the offending columns into a new pair of variables. To describe that operation we need three parameters: The set of columns whose names are identifier variables, not values. In this example, country is the identifier column and the value columns are 1999 and 2000. The name of the variable to move the column names to. Here it is year. The name of the variable to move the column values to. Here it’s cases. Together those parameters generate the call to melt(): table4a.melt([&#39;country&#39;], var_name = &quot;year&quot;, value_name = &quot;cases&quot;) #&gt; country year cases #&gt; 0 Afghanistan 1999 745 #&gt; 1 Brazil 1999 37737 #&gt; 2 China 1999 212258 #&gt; 3 Afghanistan 2000 2666 #&gt; 4 Brazil 2000 80488 #&gt; 5 China 2000 213766 year and cases do not exist in table4a so we put their names in quotes. Figure 12.2: Pivoting table4 into a longer, tidy form. In the final result, the pivoted columns are dropped, and we get new year and cases columns. Otherwise, the relationships between the original variables are preserved. Visually, this is shown in Figure 12.2. melt() makes datasets longer (imagine melting icecream down the cone) by increasing the number of rows and decreasing the number of columns. I don’t believe it makes sense to describe a dataset as being in “long form”. Length is a relative term, and you can only say (e.g.) that dataset A is longer than dataset B. We can use melt() to tidy table4b in a similar fashion. The only difference is the variable stored in the cell values: table4b.melt([&#39;country&#39;], var_name = &#39;year&#39;, value_name = &#39;population&#39;) #&gt; country year population #&gt; 0 Afghanistan 1999 19987071 #&gt; 1 Brazil 1999 172006362 #&gt; 2 China 1999 1272915272 #&gt; 3 Afghanistan 2000 20595360 #&gt; 4 Brazil 2000 174504898 #&gt; 5 China 2000 1280428583 To combine the tidied versions of table4a and table4b into a single tibble, we need to use merge(), which you’ll learn about in relational data. tidy4a = table4a.melt([&#39;country&#39;], var_name = &quot;year&quot;, value_name = &quot;cases&quot;) tidy4b = table4b.melt([&#39;country&#39;], var_name = &#39;year&#39;, value_name = &#39;population&#39;) pd.merge(tidy4a, tidy4b, on = [&#39;country&#39;, &#39;year&#39;]) #&gt; country year cases population #&gt; 0 Afghanistan 1999 745 19987071 #&gt; 1 Brazil 1999 37737 172006362 #&gt; 2 China 1999 212258 1272915272 #&gt; 3 Afghanistan 2000 2666 20595360 #&gt; 4 Brazil 2000 80488 174504898 #&gt; 5 China 2000 213766 1280428583 12.3.2 Wider pivot() is the opposite of melt(). You use it when an observation is scattered across multiple rows. For example, take table2: an observation is a country in a year, but each observation is spread across two rows. table2 #&gt; country year type count #&gt; 0 Afghanistan 1999 cases 745 #&gt; 1 Afghanistan 1999 population 19987071 #&gt; 2 Afghanistan 2000 cases 2666 #&gt; 3 Afghanistan 2000 population 20595360 #&gt; 4 Brazil 1999 cases 37737 #&gt; 5 Brazil 1999 population 172006362 #&gt; 6 Brazil 2000 cases 80488 #&gt; 7 Brazil 2000 population 174504898 #&gt; 8 China 1999 cases 212258 #&gt; 9 China 1999 population 1272915272 #&gt; 10 China 2000 cases 213766 #&gt; 11 China 2000 population 1280428583 To tidy this up, we first analyse the representation in similar way to melt(). This time, however, we only need two parameters: The column to take variable names from. Here, it’s type. The column to take values from. Here it’s count. Once we’ve figured that out, we can use pivot(), as shown programmatically below, and visually in Figure 12.3. In this example, we have a multi-column index argument and will need to use pivot_table(). With a single column index pivot() can be used. table2.pivot_table( index = [&#39;country&#39;, &#39;year&#39;], columns = &#39;type&#39;, values = &#39;count&#39;).reset_index() #&gt; type country year cases population #&gt; 0 Afghanistan 1999 745 19987071 #&gt; 1 Afghanistan 2000 2666 20595360 #&gt; 2 Brazil 1999 37737 172006362 #&gt; 3 Brazil 2000 80488 174504898 #&gt; 4 China 1999 212258 1272915272 #&gt; 5 China 2000 213766 1280428583 Figure 12.3: Pivoting table2 into a “wider”, tidy form. As you might have guessed from their names, pivot() and pivot_table() are complements to melt(). melt() makes wide tables narrower and longer; pivot() and pivot_table() makes long tables shorter and wider. 12.3.3 Exercises Why are melt() and pivot() not perfectly symmetrical? Carefully consider the following example: stocks = pd.DataFrame({ &#39;year&#39;: [2015, 2015, 2016, 2016], &#39;half&#39;: [1, 2, 1, 2], &#39;return&#39;: [1.88, 0.59, 0.92, 0.17] }) (stocks. pivot( index = &#39;half&#39;, columns = &#39;year&#39;, values = &#39;return&#39;). melt( var_name = &#39;year&#39;, value_name = &#39;return&#39;) ) (Hint: look at the variable types and think about column names.) 12.4 Separating and uniting So far you’ve learned how to tidy table2 and table4, but not table3. table3 has a different problem: we have one column (rate) that contains two variables (cases and population). To fix this problem, we’ll need the pandas str.split() function. You’ll also learn about the complement of str.split(): str.join(), which you use if a single variable is spread across multiple columns. 12.4.1 Split (Separate) str.split() pulls apart one column into multiple columns, by splitting wherever a separator character appears. Take table3: table3 #&gt; country year rate #&gt; 0 Afghanistan 1999 745/19987071 #&gt; 1 Afghanistan 2000 2666/20595360 #&gt; 2 Brazil 1999 37737/172006362 #&gt; 3 Brazil 2000 80488/174504898 #&gt; 4 China 1999 212258/1272915272 #&gt; 5 China 2000 213766/1280428583 The rate column contains both cases and population variables, and we need to split it into two variables. str.split() takes the name of the column to split. The names of the columns to separate into can be names using rename(), as shown in Figure 12.4 and the code below. By default, str.split() will split values on white spaces. If you wish to use a specific character to separate a column, you can pass the character to the pat or first argument of str.split(). Unlike tidyr::separate() in R, you will need to append the new columns back onto your data set with a pd.concat() with the argument axis = 1 new_columns = (table3. rate.str.split(&quot;/&quot;, expand = True). rename(columns = {0: &quot;cases&quot;, 1: &quot;population&quot;}) ) pd.concat([table3.drop(columns = &#39;rate&#39;), new_columns], axis = 1) #&gt; country year cases population #&gt; 0 Afghanistan 1999 745 19987071 #&gt; 1 Afghanistan 2000 2666 20595360 #&gt; 2 Brazil 1999 37737 172006362 #&gt; 3 Brazil 2000 80488 174504898 #&gt; 4 China 1999 212258 1272915272 #&gt; 5 China 2000 213766 1280428583 Figure 12.4: Separating table3 makes it tidy (Formally, pat is a regular expression, which you’ll learn more about in strings.) Look carefully at the column types: you’ll notice that cases and population are objects or strings. This is the default behaviour in str.split(): it only works on object types and returns objects. Here, however, it’s not very useful as those really are numbers. We can ask use astype() to convert to better types: pd.concat([ table3.drop(columns = &#39;rate&#39;), new_columns.astype(&#39;float&#39;)], axis = 1) #&gt; country year cases population #&gt; 0 Afghanistan 1999 745.0 1.998707e+07 #&gt; 1 Afghanistan 2000 2666.0 2.059536e+07 #&gt; 2 Brazil 1999 37737.0 1.720064e+08 #&gt; 3 Brazil 2000 80488.0 1.745049e+08 #&gt; 4 China 1999 212258.0 1.272915e+09 #&gt; 5 China 2000 213766.0 1.280429e+09 To split on integers you would use str[]. str[] will interpret the integers as positions to split at. Positive values start at 1 on the far-left of the strings and are entered :2; negative value start at -1 on the far-right of the strings are entered -2:. You can use this arrangement to separate the last two digits of each year. This make this data less tidy, but is useful in other cases, as you’ll see in a little bit. cent_year = pd.DataFrame({ &#39;century&#39;: table3.year.astype(str).str[:2], &#39;year&#39;: table3.year.astype(str).str[-2:] }) pd.concat([table3.drop(columns = &#39;year&#39;), cent_year], axis = 1) #&gt; country rate century year #&gt; 0 Afghanistan 745/19987071 19 99 #&gt; 1 Afghanistan 2666/20595360 20 00 #&gt; 2 Brazil 37737/172006362 19 99 #&gt; 3 Brazil 80488/174504898 20 00 #&gt; 4 China 212258/1272915272 19 99 #&gt; 5 China 213766/1280428583 20 00 12.4.2 Unite For two string series the inverse of str.split() can be done with +: it combines multiple columns into a single column. You’ll need it much less frequently than str.split(), but it’s still a useful tool to have in your back pocket. Figure 12.5: Uniting table5 makes it tidy We can use + to rejoin the century and year string columns that we created in the last example. That data is saved as table5. + takes a two series and can be assigned to the name of the new variable to create: table5.assign(new = table5[&#39;century&#39;] + table5[&#39;year&#39;]) #&gt; country century year rate new #&gt; 0 Afghanistan 19 99 745/19987071 118 #&gt; 1 Afghanistan 20 0 2666/20595360 20 #&gt; 2 Brazil 19 99 37737/172006362 118 #&gt; 3 Brazil 20 0 80488/174504898 20 #&gt; 4 China 19 99 212258/1272915272 118 #&gt; 5 China 20 0 213766/1280428583 20 If you want join the strings with a specified string or more than two columns you can use agg with axis = 1. table5.assign(new = table5[[&#39;century&#39;, &#39;year&#39;]].agg(&quot;_&quot;.join, axis = 1)) #&gt; country century year rate new #&gt; 0 Afghanistan 19 99 745/19987071 19_99 #&gt; 1 Afghanistan 20 00 2666/20595360 20_00 #&gt; 2 Brazil 19 99 37737/172006362 19_99 #&gt; 3 Brazil 20 00 80488/174504898 20_00 #&gt; 4 China 19 99 212258/1272915272 19_99 #&gt; 5 China 20 00 213766/1280428583 20_00 12.4.3 Exercises Compare and contrast str.split() and str.extract(). Why are there three variations of separation (by position, by separator, and with groups? 12.5 Missing values Changing the representation of a dataset brings up an important subtlety of missing values. Surprisingly, a value can be missing in one of two possible ways: Explicitly, i.e. flagged with nan. Implicitly, i.e. simply not present in the data. Let’s illustrate this idea with a very simple data set: stocks = pd.DataFrame({ &#39;year&#39;: [2015, 2015, 2015, 2015, 2016, 2016, 2016], &#39;qtr&#39;: [ 1, 2, 3, 4, 2, 3, 4], &#39;return&#39;: [1.88, 0.59, 0.35, np.nan, 0.92, 0.17, 2.66] }) There are two missing values in this dataset: The return for the fourth quarter of 2015 is explicitly missing, because the cell where its value should be instead contains nan. The return for the first quarter of 2016 is implicitly missing, because it simply does not appear in the dataset. One way to think about the difference is with this Zen-like koan: An explicit missing value is the presence of an absence; an implicit missing value is the absence of a presence. The way that a dataset is represented can make implicit values explicit. For example, we can make the implicit missing value explicit by putting years in the columns: stocks.pivot( index = &#39;qtr&#39;, columns = &#39;year&#39;, values = &#39;return&#39;).reset_index() #&gt; year qtr 2015 2016 #&gt; 0 1 1.88 NaN #&gt; 1 2 0.59 0.92 #&gt; 2 3 0.35 0.17 #&gt; 3 4 NaN 2.66 Because these explicit missing values may not be important in other representations of the data, you turn explicit missing values implicit with .dropna(): stocks.pivot( index = &#39;qtr&#39;, columns = &#39;year&#39;, values = &#39;return&#39;).reset_index().melt(id_vars = [&#39;qtr&#39;]).dropna() #&gt; qtr year value #&gt; 0 1 2015 1.88 #&gt; 1 2 2015 0.59 #&gt; 2 3 2015 0.35 #&gt; 5 2 2016 0.92 #&gt; 6 3 2016 0.17 #&gt; 7 4 2016 2.66 You can use the pandas functions stack() and unstack() with set_index() to take a set of columns and find all unique combinations. This ensures the original dataset contains all values by filling in explicit missingness where necessary. See this stackoverflow for an example. tidyr::complete() in R performs a similar function. 12.6 Case Study To finish off the chapter, let’s pull together everything you’ve learned to tackle a realistic data tidying problem. The tidyr::who dataset contains tuberculosis (TB) cases broken down by year, country, age, gender, and diagnosis method. The data comes from the 2014 World Health Organization Global Tuberculosis Report, available at http://www.who.int/tb/country/data/download/en/. There’s a wealth of epidemiological information in this dataset, but it’s challenging to work with the data in the form that it’s provided: who = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/who/who.csv&quot;) # Fix the Namibia NA iso code. who.loc[who.iso2.isna(),&#39;iso2&#39;] = &quot;NA&quot; This is a very typical real-life example dataset. It contains redundant columns, odd variable codes, and many missing values. In short, who is messy, and we’ll need multiple steps to tidy it. Pandas is designed so that each function does one thing well. That means in real-life situations you’ll usually need to string together multiple verbs into a pipeline. The best place to start is almost always to gather together the columns that are not variables. Let’s have a look at what we’ve got: It looks like country, iso2, and iso3 are three variables that redundantly specify the country. year is clearly also a variable. We don’t know what all the other columns are yet, but given the structure in the variable names (e.g. new_sp_m014, new_ep_m014, new_ep_f014) these are likely to be values, not variables. So we need to gather together all the columns from new_sp_m014 to newrel_f65. We don’t know what those values represent yet, so we’ll give them the generic name \"key\". We know the cells represent the count of cases, so we’ll use the variable cases. There are a lot of missing values in the current representation, so for now we’ll use na.rm just so we can focus on the values that are present. who1 = who.melt( id_vars = [&#39;country&#39;, &#39;iso2&#39;, &#39;iso3&#39;, &#39;year&#39;], var_name = &#39;key&#39;, value_name = &#39;cases&#39;).dropna() We can get some hint of the structure of the values in the new key column by counting them: who1.key.value_counts().head(25) #&gt; new_sp_m4554 3223 #&gt; new_sp_m3544 3219 #&gt; new_sp_m5564 3218 #&gt; new_sp_m1524 3209 #&gt; new_sp_m65 3209 #&gt; new_sp_m2534 3206 #&gt; new_sp_f4554 3204 #&gt; new_sp_f2534 3200 #&gt; new_sp_f3544 3199 #&gt; new_sp_f65 3197 #&gt; new_sp_f5564 3195 #&gt; new_sp_f1524 3194 #&gt; new_sp_f014 3174 #&gt; new_sp_m014 3173 #&gt; new_sn_m014 1045 #&gt; new_sn_f014 1040 #&gt; new_ep_m014 1038 #&gt; new_ep_f014 1032 #&gt; new_sn_m1524 1030 #&gt; new_sn_m4554 1027 #&gt; new_ep_m1524 1026 #&gt; new_sn_m3544 1025 #&gt; new_ep_m3544 1024 #&gt; new_sn_f1524 1022 #&gt; new_sn_m2534 1022 #&gt; Name: key, dtype: int64 You might be able to parse this out by yourself with a little thought and some experimentation, but luckily we have the data dictionary handy. It tells us: The first three letters of each column denote whether the column contains new or old cases of TB. In this dataset, each column contains new cases. The next two letters describe the type of TB: rel stands for cases of relapse ep stands for cases of extrapulmonary TB sn stands for cases of pulmonary TB that could not be diagnosed by a pulmonary smear (smear negative) sp stands for cases of pulmonary TB that could be diagnosed be a pulmonary smear (smear positive) The sixth letter gives the sex of TB patients. The dataset groups cases by males (m) and females (f). The remaining numbers gives the age group. The dataset groups cases into seven age groups: 014 = 0 – 14 years old 1524 = 15 – 24 years old 2534 = 25 – 34 years old 3544 = 35 – 44 years old 4554 = 45 – 54 years old 5564 = 55 – 64 years old 65 = 65 or older We need to make a minor fix to the format of the column names: unfortunately the names are slightly inconsistent because instead of new_rel we have newrel (it’s hard to spot this here but if you don’t fix it we’ll get errors in subsequent steps). You’ll learn about str.replace() in strings, but the basic idea is pretty simple: replace the characters “newrel” with “new_rel”. This makes all variable names consistent. who2 = who1.assign(names_from = lambda x: x.key.str.replace(&#39;newrel&#39;, &#39;new_rel&#39;)) who2 #&gt; country iso2 iso3 ... key cases names_from #&gt; 17 Afghanistan AF AFG ... new_sp_m014 0.0 new_sp_m014 #&gt; 18 Afghanistan AF AFG ... new_sp_m014 30.0 new_sp_m014 #&gt; 19 Afghanistan AF AFG ... new_sp_m014 8.0 new_sp_m014 #&gt; 20 Afghanistan AF AFG ... new_sp_m014 52.0 new_sp_m014 #&gt; 21 Afghanistan AF AFG ... new_sp_m014 129.0 new_sp_m014 #&gt; ... ... ... ... ... ... ... ... #&gt; 405269 Viet Nam VN VNM ... newrel_f65 3110.0 new_rel_f65 #&gt; 405303 Wallis and Futuna Islands WF WLF ... newrel_f65 2.0 new_rel_f65 #&gt; 405371 Yemen YE YEM ... newrel_f65 360.0 new_rel_f65 #&gt; 405405 Zambia ZM ZMB ... newrel_f65 669.0 new_rel_f65 #&gt; 405439 Zimbabwe ZW ZWE ... newrel_f65 725.0 new_rel_f65 #&gt; #&gt; [76046 rows x 7 columns] We can separate the values in each code with two passes of str.split(). The first pass will split the codes at each underscore. new_columns = (who2.names_from. str.split(&quot;_&quot;,expand = True). rename(columns = {0: &quot;new&quot;, 1: &quot;type&quot;, 2: &quot;sexage&quot;})) new_columns #&gt; new type sexage #&gt; 17 new sp m014 #&gt; 18 new sp m014 #&gt; 19 new sp m014 #&gt; 20 new sp m014 #&gt; 21 new sp m014 #&gt; ... ... ... ... #&gt; 405269 new rel f65 #&gt; 405303 new rel f65 #&gt; 405371 new rel f65 #&gt; 405405 new rel f65 #&gt; 405439 new rel f65 #&gt; #&gt; [76046 rows x 3 columns] Then we might as well drop the new column because it’s constant in this dataset. While we’re dropping columns, let’s also drop iso2 and iso3 since they’re redundant. # who3 is in R process. They also have an error and they are not using the names_from column. who4 = pd.concat([ who2.filter(items = [&#39;country&#39;, &#39;year&#39;]), new_columns.filter(items = [&#39;type&#39;])], axis = 1) Next we’ll separate sexage into sex and age by splitting after the first character: who5 = who4.assign( sex = new_columns.sexage.str[:1], age = new_columns.sexage.str.slice(1) ) who5 #&gt; country year type sex age #&gt; 17 Afghanistan 1997 sp m 014 #&gt; 18 Afghanistan 1998 sp m 014 #&gt; 19 Afghanistan 1999 sp m 014 #&gt; 20 Afghanistan 2000 sp m 014 #&gt; 21 Afghanistan 2001 sp m 014 #&gt; ... ... ... ... .. ... #&gt; 405269 Viet Nam 2013 rel f 65 #&gt; 405303 Wallis and Futuna Islands 2013 rel f 65 #&gt; 405371 Yemen 2013 rel f 65 #&gt; 405405 Zambia 2013 rel f 65 #&gt; 405439 Zimbabwe 2013 rel f 65 #&gt; #&gt; [76046 rows x 5 columns] The who dataset is now ‘kinda’ tidy! We have left the age range as age for simplicity. I’ve shown you the code a piece at a time, assigning each interim result to a new variable. This typically isn’t how you’d work interactively. Instead, you’d use as little assignment as possible: # assigned fixed values back into key this time. who_melt = who.melt( id_vars = [&#39;country&#39;, &#39;iso2&#39;, &#39;iso3&#39;, &#39;year&#39;], var_name = &#39;key&#39;, value_name = &#39;cases&#39;).dropna().assign( key = lambda x: x.key.str.replace(&#39;newrel&#39;, &#39;new_rel&#39;) ) new_columns = (who_melt.key. str.split(&quot;_&quot;,expand = True). rename(columns = {0: &quot;new&quot;, 1: &quot;type&quot;, 2: &quot;sexage&quot;})) # Chose to assing type instead of using pd.concat. who_melt.assign( type = new_columns.type, sex = new_columns.sexage.str[:1], age = new_columns.sexage.str.slice(1) ) 12.6.1 Exercises In this case study I set dropna() just to make it easier to check that we had the correct values. Is this reasonable? Think about how missing values are represented in this dataset. Are there implicit missing values? What’s the difference between an NA and zero? I claimed that iso2 and iso3 were redundant with country. Confirm this claim. For each country, year, and sex compute the total number of cases of TB. Make an informative visualisation of the data. 12.7 Non-tidy data Before we continue on to other topics, it’s worth talking briefly about non-tidy data. Earlier in the chapter, I used the pejorative term “messy” to refer to non-tidy data. That’s an oversimplification: there are lots of useful and well-founded data structures that are not tidy data. There are two main reasons to use other data structures: Alternative representations may have substantial performance or space advantages. Specialised fields have evolved their own conventions for storing data that may be quite different to the conventions of tidy data. Either of these reasons means you’ll need something other than a tibble (or data frame). If your data does fit naturally into a rectangular structure composed of observations and variables, I think tidy data should be your default choice. But there are good reasons to use other structures; tidy data is not the only way. If you’d like to learn more about non-tidy data, I’d highly recommend this thoughtful blog post by Jeff Leek: http://simplystatistics.org/2016/02/17/non-tidy-data/ "],
["relational-data.html", "13 Relational data 13.1 Introduction 13.2 nycflights13 13.3 Keys 13.4 Mutating joins 13.5 Filtering joins 13.6 Join problems", " 13 Relational data 13.1 Introduction It’s rare that a data analysis involves only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you’re interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important. Relations are always defined between a pair of tables. All other relations are built up from this simple idea: the relations of three or more tables are always a property of the relations between each pair. Sometimes both elements of a pair can be the same table! This is needed if, for example, you have a table of people, and each person has a reference to their parents. To work with relational data you need verbs that work with pairs of tables. There are three families of verbs designed to work with relational data: Mutating joins, which add new variables to one data frame from matching observations in another. Filtering joins, which filter observations from one data frame based on whether or not they match an observation in the other table. Set operations, which treat observations as if they were set elements. The most common place to find relational data is in a relational database management system (or RDBMS), a term that encompasses almost all modern databases. If you’ve used a database before, you’ve almost certainly used SQL. If so, you should find the concepts in this chapter familiar, although their expression in pandas is a little different. Generally, pandas is a little easier to use than SQL because pandas is specialised to do data analysis: it makes common data analysis operations easier, at the expense of making it more difficult to do other things that aren’t commonly needed for data analysis. 13.1.1 Prerequisites We will explore relational data from nycflights13 using the two-table verbs from dplyr. import pandas as pd import altair as alt import numpy as np 13.2 nycflights13 We will use the nycflights13 package to learn about relational data. nycflights13 contains four tibbles that are related to the flights table that you used in data transformation: base_url = &quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/&quot; flights = pd.read_csv(&quot;{}flights/flights.csv&quot;.format(base_url)) airlines = pd.read_csv(&quot;{}airlines/airlines.csv&quot;.format(base_url)) airports = pd.read_csv(&quot;{}airports/airports.csv&quot;.format(base_url)) planes = pd.read_csv(&quot;{}planes/planes.csv&quot;.format(base_url)) weather = pd.read_csv(&quot;{}weather/weather.csv&quot;.format(base_url)) airlines lets you look up the full carrier name from its abbreviated code: airlines.head() #&gt; carrier name #&gt; 0 9E Endeavor Air Inc. #&gt; 1 AA American Airlines Inc. #&gt; 2 AS Alaska Airlines Inc. #&gt; 3 B6 JetBlue Airways #&gt; 4 DL Delta Air Lines Inc. airports gives information about each airport, identified by the faa airport code: airports.head() #&gt; faa name lat ... tz dst tzone #&gt; 0 04G Lansdowne Airport 41.130472 ... -5 A America/New_York #&gt; 1 06A Moton Field Municipal Airport 32.460572 ... -6 A America/Chicago #&gt; 2 06C Schaumburg Regional 41.989341 ... -6 A America/Chicago #&gt; 3 06N Randall Airport 41.431912 ... -5 A America/New_York #&gt; 4 09J Jekyll Island Airport 31.074472 ... -5 A America/New_York #&gt; #&gt; [5 rows x 8 columns] planes gives information about each plane, identified by its tailnum: planes.head() #&gt; tailnum year type ... seats speed engine #&gt; 0 N10156 2004.0 Fixed wing multi engine ... 55 NaN Turbo-fan #&gt; 1 N102UW 1998.0 Fixed wing multi engine ... 182 NaN Turbo-fan #&gt; 2 N103US 1999.0 Fixed wing multi engine ... 182 NaN Turbo-fan #&gt; 3 N104UW 1999.0 Fixed wing multi engine ... 182 NaN Turbo-fan #&gt; 4 N10575 2002.0 Fixed wing multi engine ... 55 NaN Turbo-fan #&gt; #&gt; [5 rows x 9 columns] weather gives the weather at each NYC airport for each hour: weather.head() #&gt; origin year month day ... precip pressure visib time_hour #&gt; 0 EWR 2013 1 1 ... 0.0 1012.0 10.0 2013-01-01T06:00:00Z #&gt; 1 EWR 2013 1 1 ... 0.0 1012.3 10.0 2013-01-01T07:00:00Z #&gt; 2 EWR 2013 1 1 ... 0.0 1012.5 10.0 2013-01-01T08:00:00Z #&gt; 3 EWR 2013 1 1 ... 0.0 1012.2 10.0 2013-01-01T09:00:00Z #&gt; 4 EWR 2013 1 1 ... 0.0 1011.9 10.0 2013-01-01T10:00:00Z #&gt; #&gt; [5 rows x 15 columns] One way to show the relationships between the different tables is with a drawing: This diagram is a little overwhelming, but it’s simple compared to some you’ll see in the wild! The key to understanding diagrams like this is to remember each relation always concerns a pair of tables. You don’t need to understand the whole thing; you just need to understand the chain of relations between the tables that you are interested in. For nycflights13: flights connects to planes via a single variable, tailnum. flights connects to airlines through the carrier variable. flights connects to airports in two ways: via the origin and dest variables. flights connects to weather via origin (the location), and year, month, day and hour (the time). 13.2.1 Exercises Imagine you wanted to draw (approximately) the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine? I forgot to draw the relationship between weather and airports. What is the relationship and how should it appear in the diagram? weather only contains information for the origin (NYC) airports. If it contained weather records for all airports in the USA, what additional relation would it define with flights? We know that some days of the year are “special”, and fewer people than usual fly on them. How might you represent that data as a data frame? What would be the primary keys of that table? How would it connect to the existing tables? 13.3 Keys The variables used to connect each pair of tables are called keys. A key is a variable (or set of variables) that uniquely identifies an observation. In simple cases, a single variable is sufficient to identify an observation. For example, each plane is uniquely identified by its tailnum. In other cases, multiple variables may be needed. For example, to identify an observation in weather you need five variables: year, month, day, hour, and origin. There are two types of keys: A primary key uniquely identifies an observation in its own table. For example, planes.tailnum is a primary key because it uniquely identifies each plane in the planes table. A foreign key uniquely identifies an observation in another table. For example, flights.tailnum is a foreign key because it appears in the flights table where it matches each flight to a unique plane. A variable can be both a primary key and a foreign key. For example, origin is part of the weather primary key, and is also a foreign key for the airport table. Once you’ve identified the primary keys in your tables, it’s good practice to verify that they do indeed uniquely identify each observation. One way to do that is to value_counts() the primary keys and look for entries where the count is greater than one: planes.tailnum.value_counts().value_counts() #&gt; 1 3322 #&gt; Name: tailnum, dtype: int64 (weather. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;,&#39;hour&#39;, &#39;origin&#39;]). size().reset_index(name = &#39;n&#39;).n.value_counts()) #&gt; 1 26109 #&gt; 2 3 #&gt; Name: n, dtype: int64 Sometimes a table doesn’t have an explicit primary key: each row is an observation, but no combination of variables reliably identifies it. For example, what’s the primary key in the flights table? You might think it would be the date plus the flight or tail number, but neither of those are unique: (flights. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;, &#39;flight&#39;]). size().reset_index(name = &#39;n&#39;).n.value_counts()) #&gt; 1 274398 #&gt; 2 27029 #&gt; 3 2636 #&gt; 4 103 #&gt; Name: n, dtype: int64 (flights. groupby([&#39;year&#39;, &#39;month&#39;,&#39;day&#39;, &#39;tailnum&#39;]). size().reset_index(name = &#39;n&#39;).n.value_counts()) #&gt; 1 186729 #&gt; 2 49530 #&gt; 3 12211 #&gt; 4 2863 #&gt; 5 78 #&gt; Name: n, dtype: int64 When starting to work with this data, I had naively assumed that each flight number would be only used once per day: that would make it much easier to communicate problems with a specific flight. Unfortunately that is not the case! If a table lacks a primary key, it’s sometimes useful to add one with mutate() and row_number(). That makes it easier to match observations if you’ve done some filtering and want to check back in with the original data. This is called a surrogate key. A primary key and the corresponding foreign key in another table form a relation. Relations are typically one-to-many. For example, each flight has one plane, but each plane has many flights. In other data, you’ll occasionally see a 1-to-1 relationship. You can think of this as a special case of 1-to-many. You can model many-to-many relations with a many-to-1 relation plus a 1-to-many relation. For example, in this data there’s a many-to-many relationship between airlines and airports: each airline flies to many airports; each airport hosts many airlines. 13.3.1 Exercises Add a surrogate key to flights. 13.4 Mutating joins The first tool we’ll look at for combining a pair of tables is the mutating join. A mutating join allows you to combine variables from two tables. It first matches observations by their keys, then copies across variables from one table to the other. Like assign(), the join functions add variables to the right, so if you have a lot of variables already, the new variables won’t get printed out. For these examples, we’ll make it easier to see what’s going on in the examples by creating a narrower dataset: flights2 = flights.filter([&#39;year&#39;,&#39;month&#39;, &#39;day&#39;,&#39;hour&#39;, &#39;origin&#39;, &#39;dest&#39;, &#39;tailnum&#39;, &#39;carrier&#39;]) flights2 #&gt; year month day hour origin dest tailnum carrier #&gt; 0 2013 1 1 5 EWR IAH N14228 UA #&gt; 1 2013 1 1 5 LGA IAH N24211 UA #&gt; 2 2013 1 1 5 JFK MIA N619AA AA #&gt; 3 2013 1 1 5 JFK BQN N804JB B6 #&gt; 4 2013 1 1 6 LGA ATL N668DN DL #&gt; ... ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 14 JFK DCA NaN 9E #&gt; 336772 2013 9 30 22 LGA SYR NaN 9E #&gt; 336773 2013 9 30 12 LGA BNA N535MQ MQ #&gt; 336774 2013 9 30 11 LGA CLE N511MQ MQ #&gt; 336775 2013 9 30 8 LGA RDU N839MQ MQ #&gt; #&gt; [336776 rows x 8 columns] (Remember, when you’re in VS Code, you can also use the data viewer to avoid this problem.) Imagine you want to add the full airline name to the flights2 data. You can combine the airlines and flights2 data frames with merge() using how = 'left': (flights2. merge(airlines, on = &#39;carrier&#39;). drop(columns = [&#39;origin&#39;, &#39;dest&#39;])) #&gt; year month day hour tailnum carrier name #&gt; 0 2013 1 1 5 N14228 UA United Air Lines Inc. #&gt; 1 2013 1 1 5 N24211 UA United Air Lines Inc. #&gt; 2 2013 1 1 5 N39463 UA United Air Lines Inc. #&gt; 3 2013 1 1 6 N29129 UA United Air Lines Inc. #&gt; 4 2013 1 1 6 N53441 UA United Air Lines Inc. #&gt; ... ... ... ... ... ... ... ... #&gt; 336771 2013 9 19 18 N760SK OO SkyWest Airlines Inc. #&gt; 336772 2013 9 20 18 N766SK OO SkyWest Airlines Inc. #&gt; 336773 2013 9 22 18 N772SK OO SkyWest Airlines Inc. #&gt; 336774 2013 9 23 18 N776SK OO SkyWest Airlines Inc. #&gt; 336775 2013 9 24 18 N785SK OO SkyWest Airlines Inc. #&gt; #&gt; [336776 rows x 7 columns] The result of joining airlines to flights2 is an additional variable: name. The following sections explain, in detail, how mutating joins work. You’ll start by learning a useful visual representation of joins. We’ll then use that to explain the four mutating join functions: the inner join, and the three outer joins. When working with real data, keys don’t always uniquely identify observations, so next we’ll talk about what happens when there isn’t a unique match. Finally, you’ll learn how to tell dplyr which variables are the keys for a given join. 13.4.1 Understanding joins To help you learn how joins work, I’m going to use a visual representation: x = pd.DataFrame({ &#39;key&#39;: [1,2,3], &#39;val_x&#39;: [&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;]}) y = pd.DataFrame({ &#39;key&#39;: [1,2,3], &#39;val_y&#39;: [&#39;y1&#39;, &#39;y2&#39;, &#39;y3&#39;]}) The coloured column represents the “key” variable: these are used to match the rows between the tables. The grey column represents the “value” column that is carried along for the ride. In these examples I’ll show a single key variable, but the idea generalises in a straightforward way to multiple keys and multiple values. A join is a way of connecting each row in x to zero, one, or more rows in y. The following diagram shows each potential match as an intersection of a pair of lines. (If you look closely, you might notice that we’ve switched the order of the key and value columns in x. This is to emphasise that joins match based on the key; the value is just carried along for the ride.) In an actual join, matches will be indicated with dots. The number of dots = the number of matches = the number of rows in the output. 13.4.2 Inner join The simplest type of join is the inner join. An inner join matches pairs of observations whenever their keys are equal using the argument how = 'inner': (To be precise, this is an inner equijoin because the keys are matched using the equality operator. Since most joins are equijoins we usually drop that specification.) The output of an inner join is a new data frame that contains the key, the x values, and the y values. We use by to tell dplyr which variable is the key: x.merge(y, on = &#39;key&#39;, how = &#39;inner&#39;) #&gt; key val_x val_y #&gt; 0 1 x1 y1 #&gt; 1 2 x2 y2 #&gt; 2 3 x3 y3 The most important property of an inner join is that unmatched rows are not included in the result. This means that generally inner joins are usually not appropriate for use in analysis because it’s too easy to lose observations. 13.4.3 Outer joins An inner join keeps observations that appear in both tables. An outer join keeps observations that appear in at least one of the tables. There are three types of outer joins: A left join keeps all observations in x using how = 'left'. A right join keeps all observations in y using how = 'right'. A full join keeps all observations in x and y using how = 'full'. These joins work by adding an additional “virtual” observation to each table. This observation has a key that always matches (if no other key matches), and a value filled with NA. Graphically, that looks like: The most commonly used join is the left join: you use this whenever you look up additional data from another table, because it preserves the original observations even when there isn’t a match. The left join should be your default join: use it unless you have a strong reason to prefer one of the others. Another way to depict the different types of joins is with a Venn diagram: However, this is not a great representation. It might jog your memory about which join preserves the observations in which table, but it suffers from a major limitation: a Venn diagram can’t show what happens when keys don’t uniquely identify an observation. 13.4.4 Duplicate keys So far all the diagrams have assumed that the keys are unique. But that’s not always the case. This section explains what happens when the keys are not unique. There are two possibilities: One table has duplicate keys. This is useful when you want to add in additional information as there is typically a one-to-many relationship. Note that I’ve put the key column in a slightly different position in the output. This reflects that the key is a primary key in y and a foreign key in x. x = pd.DataFrame({ &#39;key&#39;: [1, 2, 2, 1], &#39;val_x&#39;: [&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;]}) y = pd.DataFrame({ &#39;key&#39;: [1,2], &#39;val_y&#39;: [&#39;y1&#39;, &#39;y2&#39;]}) x.merge(y, on = &#39;key&#39;, how = &#39;left&#39;) #&gt; key val_x val_y #&gt; 0 1 x1 y1 #&gt; 1 2 x2 y2 #&gt; 2 2 x3 y2 #&gt; 3 1 x4 y1 Both tables have duplicate keys. This is usually an error because in neither table do the keys uniquely identify an observation. When you join duplicated keys, you get all possible combinations, the Cartesian product: x = pd.DataFrame({ &#39;key&#39;: [1, 2, 2, 1], &#39;val_x&#39;: [&#39;x1&#39;, &#39;x2&#39;, &#39;x3&#39;, &#39;x4&#39;]}) y = pd.DataFrame({ &#39;key&#39;: [1,2], &#39;val_y&#39;: [&#39;y1&#39;, &#39;y2&#39;]}) x.merge(y, on = &#39;key&#39;, how = &#39;left&#39;) #&gt; key val_x val_y #&gt; 0 1 x1 y1 #&gt; 1 2 x2 y2 #&gt; 2 2 x3 y2 #&gt; 3 1 x4 y1 13.4.5 Defining the key columns So far, the pairs of tables have always been joined by a single variable, and that variable has the same name in both tables. That constraint was encoded by on = \"key\". You can use other values for on to connect the tables in other ways: The default, by = NULL, uses all variables that appear in both tables, the so called natural join. The default how is left join. For example, the flights and weather tables match on their common variables: year, month, day, hour and origin. flights2.merge(weather) #&gt; year month day hour ... precip pressure visib time_hour #&gt; 0 2013 1 1 5 ... 0.0 1011.9 10.0 2013-01-01T10:00:00Z #&gt; 1 2013 1 1 5 ... 0.0 1011.9 10.0 2013-01-01T10:00:00Z #&gt; 2 2013 1 1 5 ... 0.0 1011.4 10.0 2013-01-01T10:00:00Z #&gt; 3 2013 1 1 5 ... 0.0 1012.1 10.0 2013-01-01T10:00:00Z #&gt; 4 2013 1 1 5 ... 0.0 1012.1 10.0 2013-01-01T10:00:00Z #&gt; ... ... ... ... ... ... ... ... ... ... #&gt; 335215 2013 9 30 22 ... 0.0 1016.5 10.0 2013-10-01T02:00:00Z #&gt; 335216 2013 9 30 22 ... 0.0 1016.5 10.0 2013-10-01T02:00:00Z #&gt; 335217 2013 9 30 22 ... 0.0 1016.5 10.0 2013-10-01T02:00:00Z #&gt; 335218 2013 9 30 22 ... 0.0 1016.5 10.0 2013-10-01T02:00:00Z #&gt; 335219 2013 9 30 23 ... 0.0 1016.3 10.0 2013-10-01T03:00:00Z #&gt; #&gt; [335220 rows x 18 columns] A character vector, on = \"x\". This is like a natural join, but uses only some of the common variables. For example, flights and planes have year variables, but they mean different things so we only want to join by tailnum. flights2.merge(planes, on = &#39;tailnum&#39;) #&gt; year_x month day hour ... engines seats speed engine #&gt; 0 2013 1 1 5 ... 2 149 NaN Turbo-fan #&gt; 1 2013 1 8 14 ... 2 149 NaN Turbo-fan #&gt; 2 2013 1 9 7 ... 2 149 NaN Turbo-fan #&gt; 3 2013 1 9 11 ... 2 149 NaN Turbo-fan #&gt; 4 2013 1 13 8 ... 2 149 NaN Turbo-fan #&gt; ... ... ... ... ... ... ... ... ... ... #&gt; 284165 2013 9 20 18 ... 2 80 NaN Turbo-fan #&gt; 284166 2013 9 22 18 ... 2 80 NaN Turbo-fan #&gt; 284167 2013 9 23 18 ... 2 80 NaN Turbo-fan #&gt; 284168 2013 9 24 18 ... 2 80 NaN Turbo-fan #&gt; 284169 2013 9 28 7 ... 2 149 NaN Turbo-fan #&gt; #&gt; [284170 rows x 16 columns] Note that the year variables (which appear in both input data frames, but are not constrained to be equal) are disambiguated in the output with a suffix. A named character vector: by = c(\"a\" = \"b\"). This will match variable a in table x to variable b in table y. The variables from x will be used in the output. For example, if we want to draw a map we need to combine the flights data with the airports data which contains the location (lat and lon) of each airport. Each flight has an origin and destination airport, so we need to specify which one we want to join to: flights2.merge(airports, left_on = &#39;dest&#39;, right_on = &#39;faa&#39;) #&gt; year month day hour ... alt tz dst tzone #&gt; 0 2013 1 1 5 ... 97 -6 A America/Chicago #&gt; 1 2013 1 1 5 ... 97 -6 A America/Chicago #&gt; 2 2013 1 1 6 ... 97 -6 A America/Chicago #&gt; 3 2013 1 1 7 ... 97 -6 A America/Chicago #&gt; 4 2013 1 1 7 ... 97 -6 A America/Chicago #&gt; ... ... ... ... ... ... ... .. .. ... #&gt; 329169 2013 8 3 16 ... 152 -9 A America/Anchorage #&gt; 329170 2013 8 10 16 ... 152 -9 A America/Anchorage #&gt; 329171 2013 8 17 16 ... 152 -9 A America/Anchorage #&gt; 329172 2013 8 24 16 ... 152 -9 A America/Anchorage #&gt; 329173 2013 7 27 1 ... 22 -5 A America/New_York #&gt; #&gt; [329174 rows x 16 columns] flights2.merge(airports, left_on = &#39;origin&#39;, right_on = &#39;faa&#39;) #&gt; year month day hour origin ... lon alt tz dst tzone #&gt; 0 2013 1 1 5 EWR ... -74.168667 18 -5 A America/New_York #&gt; 1 2013 1 1 5 EWR ... -74.168667 18 -5 A America/New_York #&gt; 2 2013 1 1 6 EWR ... -74.168667 18 -5 A America/New_York #&gt; 3 2013 1 1 6 EWR ... -74.168667 18 -5 A America/New_York #&gt; 4 2013 1 1 6 EWR ... -74.168667 18 -5 A America/New_York #&gt; ... ... ... ... ... ... ... ... .. .. .. ... #&gt; 336771 2013 9 30 22 JFK ... -73.778925 13 -5 A America/New_York #&gt; 336772 2013 9 30 22 JFK ... -73.778925 13 -5 A America/New_York #&gt; 336773 2013 9 30 22 JFK ... -73.778925 13 -5 A America/New_York #&gt; 336774 2013 9 30 23 JFK ... -73.778925 13 -5 A America/New_York #&gt; 336775 2013 9 30 14 JFK ... -73.778925 13 -5 A America/New_York #&gt; #&gt; [336776 rows x 16 columns] 13.4.6 Exercises Add the location of the origin and destination (i.e. the lat and lon) to flights. Is there a relationship between the age of a plane and its delays? What weather conditions make it more likely to see a delay? 13.4.7 Other implementations The pandas user guide on Merge, join, and concatenate provides documentation on joining using and index. SQL is the inspiration for pandas merge() function, so the translation is straightforward: dplyr SQL x.merge(y, on = 'z', how = 'inner') SELECT * FROM x INNER JOIN y USING (z) x.merge(y, on = 'z', how = 'left') SELECT * FROM x LEFT OUTER JOIN y USING (z) x.merge(y, on = 'z', how = 'right') SELECT * FROM x RIGHT OUTER JOIN y USING (z) x.merge(y, on = 'z', how = 'outer') SELECT * FROM x FULL OUTER JOIN y USING (z) Note that “INNER” and “OUTER” are optional, and often omitted. Joining different variables between the tables, e.g. x.merge(y, how = 'inner', left_on = 'a', right_on = 'b') uses a slightly different syntax in SQL: SELECT * FROM x INNER JOIN y ON x.a = y.b. As this syntax suggests, SQL supports a wider range of join types than pandas because you can connect the tables using constraints other than equality (sometimes called non-equijoins). 13.5 Filtering joins Filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables. There are two types: semi joins keeps all observations in x that have a match in y. anti joins drops all observations in x that have a match in y. Semi-joins are useful for matching filtered summary tables back to the original rows. For example, imagine that you’d found the 10 days with highest average delays. How would you construct the filter statement that used year, month, and day to match it back to flights? Instead you can use a semi-joins, which connects the two tables like a mutating join, but instead of adding new columns, only keeps the rows in x that have a match in y: Graphically, a semi-join looks like this: Only the existence of a match is important; it doesn’t matter which observation is matched. This means that filtering joins never duplicate rows like mutating joins do: The inverse of a semi-join is an anti-join. An anti-join keeps the rows that don’t have a match: Anti-joins are useful for diagnosing join mismatches. For example, when connecting flights and planes, you might be interested to know that there are many flights that don’t have a match in planes: the pandas merge arguments don’t handle these two types of joins. However you can use pandas to get the same results. Anti-Join Pandas on stackoverflow provides a guide. 13.5.1 Exercises What does it mean for a flight to have a missing tailnum? What do the tail numbers that don’t have a matching record in planes have in common? (Hint: one variable explains ~90% of the problems.) Filter flights to only show flights with planes that have flown at least 100 flights. Find the 48 hours (over the course of the whole year) that have the worst delays. Cross-reference it with the weather data. Can you see any patterns? You might expect that there’s an implicit relationship between plane and airline, because each plane is flown by a single airline. Confirm or reject this hypothesis using the tools you’ve learned above. 13.6 Join problems The data you’ve been working with in this chapter has been cleaned up so that you’ll have as few problems as possible. Your own data is unlikely to be so nice, so there are a few things that you should do with your own data to make your joins go smoothly. Start by identifying the variables that form the primary key in each table. You should usually do this based on your understanding of the data, not empirically by looking for a combination of variables that give a unique identifier. If you just look for variables without thinking about what they mean, you might get (un)lucky and find a combination that’s unique in your current data but the relationship might not be true in general. For example, the altitude and longitude uniquely identify each airport, but they are not good identifiers! airports.groupby([&#39;alt&#39;, &#39;lon&#39;]).size().value_counts() #&gt; 1 1458 #&gt; dtype: int64 Check that none of the variables in the primary key are missing. If a value is missing then it can’t identify an observation! Check that your foreign keys match primary keys in another table. It’s common for keys not to match because of data entry errors. Fixing these is often a lot of work. If you do have missing keys, you’ll need to be thoughtful about your use of inner vs. outer joins, carefully considering whether or not you want to drop rows that don’t have a match. Be aware that simply checking the number of rows before and after the join is not sufficient to ensure that your join has gone smoothly. If you have an inner join with duplicate keys in both tables, you might get unlucky as the number of dropped rows might exactly equal the number of duplicated rows! "],
["strings.html", "14 Strings 14.1 Introduction 14.2 String basics 14.3 Matching patterns with regular expressions 14.4 Tools 14.5 Other types of pattern 14.6 stringi", " 14 Strings 14.1 Introduction This chapter introduces you to string manipulation in R. You’ll learn the basics of how strings work and how to create them by hand, but the focus of this chapter will be on regular expressions, or regexps for short. Regular expressions are useful because strings usually contain unstructured or semi-structured data, and regexps are a concise language for describing patterns in strings. When you first look at a regexp, you’ll think a cat walked across your keyboard, but as your understanding improves they will soon start to make sense. 14.1.1 Prerequisites This chapter will focus on the stringr package for string manipulation, which is part of the core tidyverse. import pandas as pd import altair as alt import numpy as np 14.2 String basics You can create strings with either single quotes or double quotes. You can read the pandas user guide on working with text data for more details. string1 = &quot;This is a string&quot; string2 = &#39;If I want to include a &quot;quote&quot; inside a string, I use single quotes&#39; To include a literal single or double quote in a string you can use \\ to “escape” it: double_quote = &quot;\\&quot;&quot; # or &#39;&quot;&#39; single_quote = &#39;\\&#39;&#39; # or &quot;&#39;&quot; That means if you want to include a literal backslash, you’ll need to double it up: \"\\\\\". Beware that the printed representation of a string is not the same as string itself, because the printed representation shows the escapes: x = &quot;\\&quot; \\\\&quot; x #&gt; &#39;&quot; \\\\&#39; print(x) #&gt; &quot; \\ There are a handful of other special characters. The most common are \"\\n\", newline, and \"\\t\", tab, but you can see the complete list in the Python reference manual. You’ll also sometimes see strings like \"\\u00b5\", this is a way of writing non-English characters that works on all platforms: x = &quot;\\u00b5&quot; x #&gt; &#39;µ&#39; Multiple strings are often stored in a object series, which you can create with []: pd.Series([&quot;one&quot;, &quot;two&quot;, &quot;three&quot;]) #&gt; 0 one #&gt; 1 two #&gt; 2 three #&gt; dtype: object 14.2.1 String length Python contains many functions to work with strings. We’ll use the functions from pandas for use on series. These all start with str. For example, str.length() tells you the number of characters in a string: pd.Series([&quot;a&quot;, &quot;R for data science&quot;, np.nan]).str.len() #&gt; 0 1.0 #&gt; 1 18.0 #&gt; 2 NaN #&gt; dtype: float64 14.2.2 Combining strings To combine two or more strings, use str_c(): pd.Series([&quot;x&quot;, &quot;y&quot;]).str.cat() #&gt; &#39;xy&#39; pd.Series([&quot;x&quot;, &quot;y&quot;, &quot;z&quot;]).str.cat() #&gt; &#39;xyz&#39; Use the sep argument to control how they’re separated: pd.Series([&quot;x&quot;, &quot;y&quot;]).str.cat(sep = &#39;_&#39;) #&gt; &#39;x_y&#39; Like most other functions in Python, missing values are contagious. If you want them to print as \"NA\", use fillna() or na_rep = 'NA': x = pd.Series([&quot;abc&quot;, np.nan]) x.str.cat() #&gt; &#39;abc&#39; x.str.cat(na_rep = &quot;NA&quot;) #&gt; &#39;abcNA&#39; x.fillna(&#39;NA&#39;).str.cat() #&gt; &#39;abcNA&#39; 14.2.3 Subsetting strings You can extract parts of a string using str[]. As well as the string, str[] takes start:end arguments which give the (inclusive) position of the substring: x = pd.Series([&quot;Apple&quot;, &quot;Banana&quot;, &quot;Pear&quot;]) x.str[0:3] # negative numbers count backwards from end #&gt; 0 App #&gt; 1 Ban #&gt; 2 Pea #&gt; dtype: object x.str[-3:] #&gt; 0 ple #&gt; 1 ana #&gt; 2 ear #&gt; dtype: object Note that str[] won’t fail if the string is too short: it will just return as much as possible: pd.Series([&quot;a&quot;]).str[0:5] #&gt; 0 a #&gt; dtype: object You can also use the assign strings using str.slice_replace() to modify strings: x.str.slice_replace(0,0, repl = &quot;5&quot;) #&gt; 0 5Apple #&gt; 1 5Banana #&gt; 2 5Pear #&gt; dtype: object 14.3 Matching patterns with regular expressions Regexps are a very terse language that allow you to describe patterns in strings. They take a little while to get your head around, but once you understand them, you’ll find them extremely useful. 14.3.1 Basic matches The simplest patterns match exact strings: x = pd.Series([&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;]) x.str.replace(&quot;an&quot;, &quot;-M-&quot;) #&gt; 0 apple #&gt; 1 b-M--M-a #&gt; 2 pear #&gt; dtype: object The next step up in complexity is ., which matches any character (except a newline): x.str.replace(&quot;.a.&quot;, &quot;-M-&quot;) #&gt; 0 apple #&gt; 1 -M-ana #&gt; 2 p-M- #&gt; dtype: object But if “.” matches any character, how do you match the character “.”? You need to use an “escape” to tell the regular expression you want to match it exactly, not use its special behaviour. Like strings, regexps use the backslash, \\, to escape special behaviour. So to match an ., you need the regexp \\.. Unfortunately this creates a problem. We use strings to represent regular expressions, and \\ is also used as an escape symbol in strings. So to create the regular expression \\. we need the string \"\\\\.\". # To create the regular expression, we need \\\\ dot = &#39;\\\\.&#39; # But the expression itself only contains one: print(dot) # And this tells pandas to look for an explicit . #&gt; \\. pd.Series([&quot;abc&quot;, &quot;a.c&quot;, &quot;bef&quot;]).replace(&#39;a\\\\.c&#39;, &#39;-M-&#39;, regex=True) #&gt; 0 abc #&gt; 1 -M- #&gt; 2 bef #&gt; dtype: object If \\ is used as an escape character in regular expressions, how do you match a literal \\? Well you need to escape it, creating the regular expression \\\\. To create that regular expression, you need to use a string, which also needs to escape \\. That means to match a literal \\ you need to write \"\\\\\\\\\" — you need four backslashes to match one! x = pd.Series([&quot;a\\\\b&quot;]) print(x) #&gt; 0 a\\b #&gt; dtype: object x.replace(&#39;\\\\\\\\&#39;, &#39;-M-&#39;, regex = True) #&gt; 0 a-M-b #&gt; dtype: object In this book, I’ll write regular expression as \\. and strings that represent the regular expression as \"\\\\.\". 14.3.1.1 Exercises Explain why each of these strings don’t match a \\: \"\\\", \"\\\\\", \"\\\\\\\". How would you match the sequence \"'\\? 14.3.2 Anchors By default, regular expressions will match any part of a string. It’s often useful to anchor the regular expression so that it matches from the start or end of the string. You can use: ^ to match the start of the string. $ to match the end of the string. You can also use pandas startwith() and endswith() that work similiar but return booleans. x = pd.Series([&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;]) x.str.replace(&quot;^a&quot;, &quot;-M-&quot;) #&gt; 0 -M-pple #&gt; 1 banana #&gt; 2 pear #&gt; dtype: object x.str.replace(&quot;a$&quot;, &quot;-M-&quot;) #&gt; 0 apple #&gt; 1 banan-M- #&gt; 2 pear #&gt; dtype: object To remember which is which, try this mnemonic which I learned from Evan Misshula: if you begin with power (^), you end up with money ($). To force a regular expression to only match a complete string, anchor it with both ^ and $: x = pd.Series([&quot;apple pie&quot;, &quot;apple&quot;, &quot;apple cake&quot;]) x.str.replace(&quot;apple&quot;, &quot;-M-&quot;) #&gt; 0 -M- pie #&gt; 1 -M- #&gt; 2 -M- cake #&gt; dtype: object x.str.replace(&quot;^apple$&quot;, &quot;-M-&quot;) #&gt; 0 apple pie #&gt; 1 -M- #&gt; 2 apple cake #&gt; dtype: object You can also match the boundary between words with \\b. I don’t often use this in Python, but I will sometimes use it when I’m doing a search to find the name of a function that’s a component of other functions. For example, I’ll search for \\bsum\\b to avoid matching summarise, summary, rowsum and so on. 14.3.2.1 Exercises How would you match the literal string \"$^$\"? 14.3.3 Character classes and alternatives There are a number of special patterns that match more than one character. You’ve already seen ., which matches any character apart from a newline. There are four other useful tools: \\d: matches any digit. \\s: matches any whitespace (e.g. space, tab, newline). [abc]: matches a, b, or c. [^abc]: matches anything except a, b, or c. Remember, to create a regular expression containing \\d or \\s, you’ll need to escape the \\ for the string, so you’ll type \"\\\\d\" or \"\\\\s\". A character class containing a single character is a nice alternative to backslash escapes when you want to include a single metacharacter in a regex. Many people find this more readable. # Look for a literal character that normally has special meaning in a regex x = pd.Series([&quot;abc&quot;, &quot;a.c&quot;, &quot;a*c&quot;, &quot;a c&quot;]) x.str.replace(&quot;a[.]c&quot;, &quot;-M-&quot;) #&gt; 0 abc #&gt; 1 -M- #&gt; 2 a*c #&gt; 3 a c #&gt; dtype: object x.str.replace(&quot;.[*]c&quot;, &quot;-M-&quot;) #&gt; 0 abc #&gt; 1 a.c #&gt; 2 -M- #&gt; 3 a c #&gt; dtype: object x.str.replace(&quot;a[ ]&quot;, &quot;-M-&quot;) #&gt; 0 abc #&gt; 1 a.c #&gt; 2 a*c #&gt; 3 -M-c #&gt; dtype: object This works for most (but not all) regex metacharacters: $ . | ? * + ( ) [ {. Unfortunately, a few characters have special meaning even inside a character class and must be handled with backslash escapes: ] \\ ^ and -. You can use alternation to pick between one or more alternative patterns. For example, abc|d..f will match either ‘“abc”’, or \"deaf\". Note that the precedence for | is low, so that abc|xyz matches abc or xyz not abcyz or abxyz. Like with mathematical expressions, if precedence ever gets confusing, use parentheses to make it clear what you want: pd.Series([&quot;grey&quot;, &quot;gray&quot;]).str.replace(&quot;r(e|a)&quot;, &quot;-M-&quot;) #&gt; 0 g-M-y #&gt; 1 g-M-y #&gt; dtype: object 14.3.3.1 Exercises Create regular expressions to find all words that: Start with a vowel. That only contain consonants. (Hint: thinking about matching “not”-vowels.) End with ed, but not with eed. End with ing or ise. Is “q” always followed by a “u”? Write a regular expression that matches a word if it’s probably written in British English, not American English. Create a regular expression that will match telephone numbers as commonly written in your country. 14.3.4 Repetition The next step up in power involves controlling how many times a pattern matches: ?: 0 or 1 +: 1 or more *: 0 or more Notice the use of n = 1 which functions like the R package stringr::str_&lt;TYPE&gt; the default argument for str.replace() of n = -1 functions like stringr::str_&lt;TYPE&gt;_all. x = pd.Series([&quot;1888 in Roman numerals: MDCCCLXXXVIII&quot;]) x.str.replace(&quot;CC?&quot;, &quot;-M-&quot;, n = 1) #&gt; 0 1888 in Roman numerals: MD-M-CLXXXVIII #&gt; dtype: object x.str.replace(&quot;CC+&quot;, &quot;-M-&quot;, n = 1) #&gt; 0 1888 in Roman numerals: MD-M-LXXXVIII #&gt; dtype: object x.str.replace(&#39;C[LX]+&#39;, &quot;-M-&quot;, n = 1) #&gt; 0 1888 in Roman numerals: MDCC-M-VIII #&gt; dtype: object Note that the precedence of these operators is high, so you can write: colou?r to match either American or British spellings. That means most uses will need parentheses, like bana(na)+. You can also specify the number of matches precisely: {n}: exactly n {n,}: n or more {,m}: at most m {n,m}: between n and m x.str.replace(&quot;C{2}&quot;, &quot;-M-&quot;, n = 1) #&gt; 0 1888 in Roman numerals: MD-M-CLXXXVIII #&gt; dtype: object x.str.replace(&quot;C{2,}&quot;, &quot;-M-&quot;, n = 1) #&gt; 0 1888 in Roman numerals: MD-M-LXXXVIII #&gt; dtype: object x.str.replace(&quot;C{2,3}&quot;, &quot;-M-&quot;, n = 1) #&gt; 0 1888 in Roman numerals: MD-M-LXXXVIII #&gt; dtype: object By default these matches are “greedy”: they will match the longest string possible. You can make them “lazy”, matching the shortest string possible by putting a ? after them. This is an advanced feature of regular expressions, but it’s useful to know that it exists: x.str.replace(&quot;C{2,3}?&quot;, &quot;-M-&quot;, n = 1) #&gt; 0 1888 in Roman numerals: MD-M-CLXXXVIII #&gt; dtype: object x.str.replace(&#39;C[LX]+?&#39;, &quot;-M-&quot;, n = 1) #&gt; 0 1888 in Roman numerals: MDCC-M-XXXVIII #&gt; dtype: object 14.3.4.1 Exercises Describe the equivalents of ?, +, * in {m,n} form. Describe in words what these regular expressions match: (read carefully to see if I’m using a regular expression or a string that defines a regular expression.) ^.*$ \"\\\\{.+\\\\}\" \\d{4}-\\d{2}-\\d{2} \"\\\\\\\\{4}\" Create regular expressions to find all words that: Start with three consonants. Have three or more vowels in a row. Have two or more vowel-consonant pairs in a row. Solve the beginner regexp crosswords at https://regexcrossword.com/challenges/beginner. 14.3.5 Grouping and backreferences We will use a few csv files that contain one column labeld name. The fruit names and words as taken from the stringr::fruit character vectors. fruit = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/fruit/fruit.csv&quot;) words = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/words/words.csv&quot;) fruit #&gt; name #&gt; 0 apple #&gt; 1 apricot #&gt; 2 avocado #&gt; 3 banana #&gt; 4 bell pepper #&gt; .. ... #&gt; 75 strawberry #&gt; 76 tamarillo #&gt; 77 tangerine #&gt; 78 ugli fruit #&gt; 79 watermelon #&gt; #&gt; [80 rows x 1 columns] words #&gt; name #&gt; 0 a #&gt; 1 able #&gt; 2 about #&gt; 3 absolute #&gt; 4 accept #&gt; .. ... #&gt; 975 yes #&gt; 976 yesterday #&gt; 977 yet #&gt; 978 you #&gt; 979 young #&gt; #&gt; [980 rows x 1 columns] Earlier, you learned about parentheses as a way to disambiguate complex expressions. Parentheses also create a numbered capturing group (number 1, 2 etc.). A capturing group stores the part of the string matched by the part of the regular expression inside the parentheses. You can refer to the same text as previously matched by a capturing group with backreferences, like \\1, \\2 etc. For example, the following regular expression finds all fruits that have a repeated pair of letters. fruit[fruit[&#39;name&#39;].str.contains(&#39;(..)\\\\1&#39;)] #&gt; name #&gt; 3 banana #&gt; 19 coconut #&gt; 21 cucumber #&gt; 40 jujube #&gt; 55 papaya #&gt; 72 salal berry #&gt; #&gt; /usr/local/lib/python3.7/site-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract. #&gt; return func(self, *args, **kwargs) 14.3.5.1 Exercises Describe, in words, what these expressions will match: (.)\\1\\1 \"(.)(.)\\\\2\\\\1\" (..)\\1 \"(.).\\\\1.\\\\1\" \"(.)(.)(.).*\\\\3\\\\2\\\\1\" Construct regular expressions to match words that: Start and end with the same character. Contain a repeated pair of letters (e.g. “church” contains “ch” repeated twice.) Contain one letter repeated in at least three places (e.g. “eleven” contains three “e”s.) 14.4 Tools Now that you’ve learned the basics of regular expressions, it’s time to learn how to apply them to real problems. In this section you’ll learn a wide array of string functions that let you: Determine which strings match a pattern. Find the positions of matches. Extract the content of matches. Replace matches with new values. Split a string based on a match. A word of caution before we continue: because regular expressions are so powerful, it’s easy to try and solve every problem with a single regular expression. In the words of Jamie Zawinski: Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. As a cautionary tale, check out this regular expression that checks if a email address is valid: (?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t] )+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?: \\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:( ?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\0 31]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\ ](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+ (?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?: (?:\\r\\n)?[ \\t])*))*|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z |(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n) ?[ \\t])*)*\\&lt;(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\ r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n) ?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t] )*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])* )(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t] )+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*) *:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+ |\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r \\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?: \\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t ]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031 ]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\]( ?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(? :(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(? :\\r\\n)?[ \\t])*))*\\&gt;(?:(?:\\r\\n)?[ \\t])*)|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(? :(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)? [ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*:(?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]| \\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt; @,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot; (?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t] )*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(? :[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[ \\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000- \\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|( ?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*\\&lt;(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()&lt;&gt;@,; :\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([ ^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot; .\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\ ]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\ [\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\ r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\] |\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\0 00-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\ .|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@, ;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(? :[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])* (?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;. \\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[ ^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\] ]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\&gt;(?:(?:\\r\\n)?[ \\t])*)(?:,\\s*( ?:(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:( ?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[ \\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t ])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t ])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(? :\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+| \\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?: [^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\ ]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)*\\&lt;(?:(?:\\r\\n) ?[ \\t])*(?:@(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot; ()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n) ?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt; @,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@, ;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t] )*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\ &quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)? (?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;. \\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?: \\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[ &quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|&quot;(?:[^\\&quot;\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*&quot;(?:(?:\\r\\n)?[ \\t]) *))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t]) +|\\Z|(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\ .(?:(?:\\r\\n)?[ \\t])*(?:[^()&lt;&gt;@,;:\\\\&quot;.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z |(?=[\\[&quot;()&lt;&gt;@,;:\\\\&quot;.\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\&gt;(?:( ?:\\r\\n)?[ \\t])*))*)?;\\s*) This is a somewhat pathological example (because email addresses are actually surprisingly complex), but is used in real code. See the stackoverflow discussion at http://stackoverflow.com/a/201378 for more details. Don’t forget that you’re in a programming language and you have other tools at your disposal. Instead of creating one complex regular expression, it’s often easier to write a series of simpler regexps. If you get stuck trying to create a single regexp that solves your problem, take a step back and think if you could break the problem down into smaller pieces, solving each challenge before moving onto the next one. 14.4.1 Detect matches To determine if a character vector matches a pattern, use str.contains(). It returns a logical vector the same length as the input: x = pd.Series([&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;]) x.str.contains(&quot;e&quot;) #&gt; 0 True #&gt; 1 False #&gt; 2 True #&gt; dtype: bool Remember that when you use a logical vector in a numeric context, FALSE becomes 0 and TRUE becomes 1. That makes sum() and mean() useful if you want to answer questions about matches across a larger vector: # How many common words start with t? words.name.str.contains(&quot;^t&quot;).sum() # What proportion of common words end with a vowel? #&gt; 65 words.name.str.contains(&quot;[aeiou]$&quot;).mean() #&gt; 0.27653061224489794 When you have complex logical conditions (e.g. match a or b but not c unless d) it’s often easier to combine multiple str.contains() calls with logical operators, rather than trying to create a single regular expression. For example, here are two ways to find all words that don’t contain any vowels: # Find all words containing at least one vowel, and negate no_vowels_1 = ~words.name.str.contains(&quot;[aeiou]&quot;) # Find all words consisting only of consonants (non-vowels) no_vowels_2 = words.name.str.contains(&quot;^[^aeiou]+$&quot;) no_vowels_1.tolist() == no_vowels_2.tolist() #&gt; True The results are identical, but I think the first approach is significantly easier to understand. If your regular expression gets overly complicated, try breaking it up into smaller pieces, giving each piece a name, and then combining the pieces with logical operations. As you saw above, a common use of str.containst() is to select the elements that match a pattern: words[words[&#39;name&#39;].str.contains(&quot;x$&quot;)] #&gt; name #&gt; 107 box #&gt; 746 sex #&gt; 771 six #&gt; 840 tax Typically, however, your strings will be one column of a data frame, and you’ll want to use filter instead: df = pd.DataFrame({ &#39;words&#39;: words.name, &#39;i&#39;: words.index}) df.query(&#39;words.str.contains(&quot;x$&quot;)&#39;) #&gt; words i #&gt; 107 box 107 #&gt; 746 sex 746 #&gt; 771 six 771 #&gt; 840 tax 840 A variation on str.contains() is str_count(): rather than a simple yes or no, it tells you how many matches there are in a string: x = pd.Series([&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;]) x.str.count(&quot;a&quot;) # On average, how many vowels per word? #&gt; 0 1 #&gt; 1 3 #&gt; 2 1 #&gt; dtype: int64 words.name.str.count(&quot;[aeiou]&quot;).mean() #&gt; 1.9918367346938775 You can use str.count() with assign(): df.assign( vowels = words.name.str.count(&quot;[aeiou]&quot;), consonants = words.name.str.count(&quot;[^aeiou]&quot;) ) #&gt; words i vowels consonants #&gt; 0 a 0 1 0 #&gt; 1 able 1 2 2 #&gt; 2 about 2 3 2 #&gt; 3 absolute 3 4 4 #&gt; 4 accept 4 2 4 #&gt; .. ... ... ... ... #&gt; 975 yes 975 1 2 #&gt; 976 yesterday 976 3 6 #&gt; 977 yet 977 1 2 #&gt; 978 you 978 2 1 #&gt; 979 young 979 2 3 #&gt; #&gt; [980 rows x 4 columns] Note that matches never overlap. For example, in \"abababa\", how many times will the pattern \"aba\" match? Regular expressions say two, not three: pd.Series([&quot;abababa&quot;]).str.count(&quot;aba&quot;) #&gt; 0 2 #&gt; dtype: int64 14.4.1.1 Exercises For each of the following challenges, try solving it by using both a single regular expression, and a combination of multiple str.contains() calls. Find all words that start or end with x. Find all words that start with a vowel and end with a consonant. Are there any words that contain at least one of each different vowel? What word has the highest number of vowels? What word has the highest proportion of vowels? (Hint: what is the denominator?) 14.4.2 Extract matches To extract the actual text of a match, use str.findall(). To show that off, we’re going to need a more complicated example. I’m going to use the Harvard sentences, which were designed to test VOIP systems, but are also useful for practicing regexps. These are provided in stringr::sentences: sentences = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/sentences/sentences.csv&quot;) sentences.count #&gt; &lt;bound method DataFrame.count of name #&gt; 0 The birch canoe slid on the smooth planks. #&gt; 1 Glue the sheet to the dark blue background. #&gt; 2 It&#39;s easy to tell the depth of a well. #&gt; 3 These days a chicken leg is a rare dish. #&gt; 4 Rice is often served in round bowls. #&gt; .. ... #&gt; 715 The grass and bushes were wet with dew. #&gt; 716 The blind man counted his old coins. #&gt; 717 A severe storm tore down the barn. #&gt; 718 She called his name many times. #&gt; 719 When you hear the bell, come quickly. #&gt; #&gt; [720 rows x 1 columns]&gt; Imagine we want to find all sentences that contain a colour. We first create a vector of colour names, and then turn it into a single regular expression: colours = pd.Series([&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;]) colour_match = colours.str.cat(sep = &quot;|&quot;) colour_match #&gt; &#39;red|orange|yellow|green|blue|purple&#39; Now we can select the sentences that contain a colour, and then extract the colour to figure out which one it is: has_colour = sentences.query(&#39;name.str.contains(@colour_match)&#39;) matches = has_colour.name.str.findall(colour_match) matches.head() #&gt; 1 [blue] #&gt; 25 [blue] #&gt; 27 [red] #&gt; 43 [red] #&gt; 81 [red] #&gt; Name: name, dtype: object Note that str.findall() extracts all matches. You’ll learn more about lists in lists and [iteration]. 14.4.2.1 Exercises In the previous example, you might have noticed that the regular expression matched “flickered”, which is not a colour. Modify the regex to fix the problem. From the Harvard sentences data, extract: The first word from each sentence. All words ending in ing. All plurals. 14.4.3 Grouped matches Earlier in this chapter we talked about the use of parentheses for clarifying precedence and for backreferences when matching. You can also use parentheses to extract parts of a complex match. For example, imagine we want to extract nouns from the sentences. As a heuristic, we’ll look for any word that comes after “a” or “the”. Defining a “word” in a regular expression is a little tricky, so here I use a simple approximation: a sequence of at least one character that isn’t a space. str.findall() gives us the complete match: noun = &#39;(a|the) ([^ ]+)&#39; has_noun = sentences.query(&#39;name.str.contains(@noun)&#39;) #&gt; /usr/local/lib/python3.7/site-packages/pandas/core/strings.py:1954: UserWarning: This pattern has match groups. To actually get the groups, use str.extract. #&gt; return func(self, *args, **kwargs) has_noun.name.str.findall(noun).head() #&gt; 0 [(the, smooth)] #&gt; 1 [(the, sheet), (the, dark)] #&gt; 2 [(the, depth), (a, well.)] #&gt; 3 [(a, chicken), (a, rare)] #&gt; 6 [(the, parked)] #&gt; Name: name, dtype: object (Unsurprisingly, our heuristic for detecting nouns is poor, and also picks up adjectives like smooth and parked.) 14.4.3.1 Exercises Find all words that come after a “number” like “one”, “two”, “three” etc. Pull out both the number and the word. Find all contractions. Separate out the pieces before and after the apostrophe. 14.4.4 Replacing matches str.replace() allows you to replace matches with new strings. The simplest use is to replace a pattern with a fixed string: x = pd.Series([&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;]) x.str.replace( &quot;[aeiou]&quot;, &quot;-&quot;, n = 1) #&gt; 0 -pple #&gt; 1 b-nana #&gt; 2 p-ar #&gt; dtype: object x.str.replace( &quot;[aeiou]&quot;, &quot;-&quot;) #&gt; 0 -ppl- #&gt; 1 b-n-n- #&gt; 2 p--r #&gt; dtype: object Instead of replacing with a fixed string you can use backreferences to insert components of the match. In the following code, I flip the order of the second and third words. sentences.name.str.replace(&quot;([^ ]+) ([^ ]+) ([^ ]+)&quot;, &quot;\\\\1 \\\\3 \\\\2&quot;).head() #&gt; 0 The canoe birch slid the on smooth planks. #&gt; 1 Glue sheet the to dark the blue background. #&gt; 2 It&#39;s to easy tell depth the of well. a #&gt; 3 These a days chicken is leg a dish. rare #&gt; 4 Rice often is served round in bowls. #&gt; Name: name, dtype: object 14.4.4.1 Exercises Replace all forward slashes in a string with backslashes. Implement a simple version of str_to_lower() using replace_all(). Switch the first and last letters in words. Which of those strings are still words? 14.4.5 Splitting Use str.split() to split a string up into pieces. For example, we could split sentences into words: sentences.name.str.split(&quot; &quot;).head() #&gt; 0 [The, birch, canoe, slid, on, the, smooth, pla... #&gt; 1 [Glue, the, sheet, to, the, dark, blue, backgr... #&gt; 2 [It&#39;s, easy, to, tell, the, depth, of, a, well.] #&gt; 3 [These, days, a, chicken, leg, is, a, rare, di... #&gt; 4 [Rice, is, often, served, in, round, bowls.] #&gt; Name: name, dtype: object Because each component might contain a different number of pieces, this returns an object with list elements. If you want all items in each list to a row you can use explode(): sentences.name.str.split(&quot; &quot;).explode() #&gt; 0 The #&gt; 0 birch #&gt; 0 canoe #&gt; 0 slid #&gt; 0 on #&gt; ... #&gt; 719 hear #&gt; 719 the #&gt; 719 bell, #&gt; 719 come #&gt; 719 quickly. #&gt; Name: name, Length: 5741, dtype: object You can also request a maximum number of pieces: fields = pd.Series([&quot;Name: Hadley&quot;, &quot;Country: NZ&quot;, &quot;Age: 35&quot;]) fields.str.split(&quot;: &quot;, n = 2) #&gt; 0 [Name, Hadley] #&gt; 1 [Country, NZ] #&gt; 2 [Age, 35] #&gt; dtype: object 14.4.5.1 Exercises Split up a string like \"apples, pears, and bananas\" into individual components. What does splitting with an empty string (\"\") do? Experiment, and then read the documentation. 14.4.6 Find matches str.find() gives you the starting positions of each match and a -1 for non-matches. This is particularly useful when none of the other functions does exactly what you want. 14.5 Other types of pattern When you use a pattern that’s a string, the default is for the str. functions to use the pattern as a regex. For some str. functions you can change this by using the regex = False argument: You can use the other arguments to control details of the match: case = False allows characters to match either their uppercase or lowercase forms. The default is case = True if the pattern is a string. bananas = pd.Series([&quot;banana&quot;, &quot;Banana&quot;, &quot;BANANA&quot;]) bananas.str.replace(&#39;banana&#39;, &quot;-R-&quot;) #&gt; 0 -R- #&gt; 1 Banana #&gt; 2 BANANA #&gt; dtype: object bananas.str.replace(&#39;banana&#39;, &quot;-R-&quot;, case = True) #&gt; 0 -R- #&gt; 1 Banana #&gt; 2 BANANA #&gt; dtype: object bananas.str.replace(&#39;banana&#39;, &quot;-R-&quot;, case = False) #&gt; 0 -R- #&gt; 1 -R- #&gt; 2 -R- #&gt; dtype: object 14.5.1 Exercises What are the five most common words in sentences? 14.6 stringi The pandas str. functions are built using the Python string functions. Jake VanderPlas’ Whirlwind Tour of Python provides examples of string manipulation using Python’s built in methods. If you find yourself struggling to do something with str. in pandas, it’s worth taking a look at the built in tools. "],
["factors.html", "15 Factors 15.1 Introduction 15.2 Creating categories 15.3 General Social Survey 15.4 Modifying factor order 15.5 Modifying factor levels", " 15 Factors 15.1 Introduction In pandas, categorical variables (factors in R) are variables that have a fixed and known set of possible values. They are also useful when you want to display character vectors in a non-alphabetical order. You can read the pandas documentation. 15.1.1 Prerequisites To work with categorical variables, we’ll use the category data type in pandas. It supports tools for dealing with categorical variables using a wide range of helper methods. import pandas as pd import altair as alt import numpy as np alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) 15.2 Creating categories Imagine that you have a variable that records month: x1 = pd.Series([&quot;Dec&quot;, &quot;Apr&quot;, &quot;Jan&quot;, &quot;Mar&quot;]) Using a string to record this variable has two problems: There are only twelve possible months, and there’s nothing saving you from typos: x2 = pd.Series([&quot;Dec&quot;, &quot;Apr&quot;, &quot;Jam&quot;, &quot;Mar&quot;]) It doesn’t sort in a useful way: x1.sort_values() #&gt; 1 Apr #&gt; 0 Dec #&gt; 2 Jan #&gt; 3 Mar #&gt; dtype: object You can fix both of these problems with a factor. To create a factor you must start by creating a list of the valid levels: month_levels = pd.Series([ &quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot; ]) Now you can create a factor: y1 = pd.Categorical(x1, categories=month_levels) y1 #&gt; [Dec, Apr, Jan, Mar] #&gt; Categories (12, object): [Jan, Feb, Mar, Apr, ..., Sep, Oct, Nov, Dec] y1.sort_values() #&gt; [Jan, Mar, Apr, Dec] #&gt; Categories (12, object): [Jan, Feb, Mar, Apr, ..., Sep, Oct, Nov, Dec] And any values not in the set will be silently converted to nan: y2 = pd.Categorical(x2, categories=month_levels) y2 #&gt; [Dec, Apr, NaN, Mar] #&gt; Categories (12, object): [Jan, Feb, Mar, Apr, ..., Sep, Oct, Nov, Dec] Sometimes you’d prefer that the order of the levels match the order of the first appearance in the data. You can do that when creating the factor by setting levels to pd.unique(x): f1 = pd.Categorical(x1, categories=pd.unique(x1)) f1 #&gt; [Dec, Apr, Jan, Mar] #&gt; Categories (4, object): [Dec, Apr, Jan, Mar] If you ever need to access the set of valid levels directly, you can do so with levels(): pd.Series(f1).cat.categories #&gt; Index([&#39;Dec&#39;, &#39;Apr&#39;, &#39;Jan&#39;, &#39;Mar&#39;], dtype=&#39;object&#39;) 15.3 General Social Survey For the rest of this chapter, we’re going to focus on gss_cat data found in the forcats R package. It’s a sample of data from the General Social Survey, which is a long-running US survey conducted by the independent research organization NORC at the University of Chicago. The survey has thousands of questions, so in gss_cat I’ve selected a handful that will illustrate some common challenges you’ll encounter when working with factors. gss_cat = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/gss_cat/gss_cat.csv&quot;) marital_levels = [&quot;No answer&quot;, &quot;Never married&quot;, &quot;Separated&quot;, &quot;Divorced&quot;, &quot;Widowed&quot;, &quot;Married&quot;] race_levels = [&quot;Other&quot;, &quot;Black&quot;, &quot;White&quot;, &quot;Not applicable&quot;] income_levels = [&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Refused&quot;, &quot;$25000 or more&quot;, &quot;$20000 - 24999&quot;, &quot;$15000 - 19999&quot;, &quot;$10000 - 14999&quot;, &quot;$8000 to 9999&quot;, &quot;$7000 to 7999&quot;, &quot;$6000 to 6999&quot;, &quot;$5000 to 5999&quot;, &quot;$4000 to 4999&quot;, &quot;$3000 to 3999&quot;, &quot;$1000 to 2999&quot;, &quot;Lt $1000&quot;, &quot;Not applicable&quot;] party_levels = [&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Other party&quot;, &quot;Strong republican&quot;, &quot;Not str republican&quot;, &quot;Ind,near rep&quot;, &quot;Independent&quot;, &quot;Ind,near dem&quot;, &quot;Not str democrat&quot;, &quot;Strong democrat&quot;] religion_levels = [&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;Inter-nondenominational&quot;, &quot;Native american&quot;, &quot;Christian&quot;, &quot;Orthodox-christian&quot;, &quot;Moslem/islam&quot;, &quot;Other eastern&quot;, &quot;Hinduism&quot;, &quot;Buddhism&quot;, &quot;Other&quot;, &quot;None&quot;, &quot;Jewish&quot;, &quot;Catholic&quot;, &quot;Protestant&quot;, &quot;Not applicable&quot;] denom_levels = [&quot;No answer&quot;, &quot;Don&#39;t know&quot;, &quot;No denomination&quot;, &quot;Other&quot;, &quot;Episcopal&quot;, &quot;Presbyterian-dk wh&quot;, &quot;Presbyterian, merged&quot;, &quot;Other presbyterian&quot;, &quot;United pres ch in us&quot;, &quot;Presbyterian c in us&quot;, &quot;Lutheran-dk which&quot;, &quot;Evangelical luth&quot;, &quot;Other lutheran&quot;, &quot;Wi evan luth synod&quot;, &quot;Lutheran-mo synod&quot;, &quot;Luth ch in america&quot;, &quot;Am lutheran&quot;, &quot;Methodist-dk which&quot;, &quot;Other methodist&quot;, &quot;United methodist&quot;, &quot;Afr meth ep zion&quot;, &quot;Afr meth episcopal&quot;, &quot;Baptist-dk which&quot;, &quot;Other baptists&quot;, &quot;Southern baptist&quot;, &quot;Nat bapt conv usa&quot;, &quot;Nat bapt conv of am&quot;, &quot;Am bapt ch in usa&quot;, &quot;Am baptist asso&quot;, &quot;Not applicable&quot;] gss_cat = gss_cat.assign( marital = lambda x: pd.Categorical(x.marital, categories=marital_levels), race = lambda x: pd.Categorical(x.race, categories=race_levels), rincome = lambda x: pd.Categorical(x.rincome, categories=income_levels), partyid = lambda x: pd.Categorical(x.partyid, categories=party_levels), relig = lambda x: pd.Categorical(x.relig, categories=religion_levels), denom = lambda x: pd.Categorical(x.denom, categories=denom_levels), ) (You can get more information about the variables using the data description sheet in data4python4ds.) When factors are stored in a tibble, you can’t see their levels so easily. One way to see them is with value_counts() or you can get a high level summary with describe(): gss_cat.race.value_counts() #&gt; White 16395 #&gt; Black 3129 #&gt; Other 1959 #&gt; Not applicable 0 #&gt; Name: race, dtype: int64 gss_cat.race.describe() #&gt; count 21483 #&gt; unique 3 #&gt; top White #&gt; freq 16395 #&gt; Name: race, dtype: object Or with a bar chart: chart = (alt.Chart(gss_cat). encode(alt.X(&#39;race&#39;), alt.Y(&#39;count()&#39;)). mark_bar(). properties(width = 400)) chart.save(&quot;screenshots/altair_cat_1.png&quot;) By default, ggplot2 will drop levels that don’t have any values. You can force them to display with: levels_use = gss_cat.race.cat.categories.to_list() chart = (alt.Chart(gss_cat). encode( x = alt.X(&#39;race&#39;, scale = alt.Scale(domain = levels_use)), y = alt.Y(&#39;count()&#39;)). mark_bar(). properties(width = 400)) chart.save(&quot;screenshots/altair_cat_2.png&quot;) These levels represent valid values that simply did not occur in this dataset. When working with factors, the two most common operations are changing the order of the levels, and changing the values of the levels. Those operations are described in the sections below. 15.3.1 Exercise Explore the distribution of rincome (reported income). What makes the default bar chart hard to understand? How could you improve the plot? What is the most common relig in this survey? What’s the most common partyid? Which relig does denom (denomination) apply to? How can you find out with a table? How can you find out with a visualisation? 15.4 Modifying factor order It’s often useful to change the order of the factor levels in a visualisation. For example, imagine you want to explore the average number of hours spent watching TV per day across religions: relig_summary = gss_cat.groupby(&#39;relig&#39;).agg( age = (&#39;age&#39;, np.mean), tvhours = (&#39;tvhours&#39;, np.mean), n = (&#39;tvhours&#39;, &#39;size&#39;) ).reset_index() chart = (alt.Chart(relig_summary). encode(alt.X(&#39;tvhours&#39;), alt.Y(&#39;relig&#39;)). mark_circle()) chart.save(&quot;screenshots/altair_cat_3.png&quot;) It is difficult to interpret this plot because there’s no overall pattern. We can improve it by reordering the levels of relig using the sort argument in alt.Y(). The sort argument uses -x to sort largest at the top and x to sort with the largest at the bottom of the y-axis. If you would like to implement more intricate sortings using alt.EncodingSortField() with the following arguments. field, the column to use for the sorting. op, the function you would like to use for the sort. Optionally, order, allows you to take the values from the op argument function and sort them as 'descending' or 'ascending'. Thus, if we were going to implement more detailed sorting we would use alt.EncodingSortField(field = 'tvhours', op = 'sum', order = 'ascending')). Note that sorting within Altair for boxplots is not very functional. You would need to use pd.Categorical() to put the categories in your prefered order. chart = (alt.Chart(relig_summary). encode(alt.X(&#39;tvhours&#39;), alt.Y(&#39;relig&#39;)). mark_circle()) chart.save(&quot;screenshots/altair_cat_4.png&quot;) Reordering religion makes it much easier to see that people in the “Don’t know” category watch much more TV, and Hinduism &amp; Other Eastern religions watch much less. As you start making more complicated transformations, I’d recommend moving them out of Altair and into a new variable using pandas. chart = (alt.Chart(relig_summary). encode(alt.X(&#39;tvhours&#39;), alt.Y(&#39;relig&#39;, sort = &#39;-x&#39;)). mark_circle()) As you start making more complicated transformations, I’d recommend moving them out of Altair and into a new variable using pandas. What if we create a similar plot looking at how average age varies across reported income level? rincome_summary = gss_cat.groupby(&#39;rincome&#39;).agg( age = (&#39;age&#39;, np.mean), tvhours = (&#39;tvhours&#39;, np.mean), n = (&#39;tvhours&#39;, &#39;size&#39;) ).reset_index() chart = (alt.Chart(rincome_summary). encode(alt.X(&#39;age&#39;), alt.Y(&#39;rincome&#39;, sort = &#39;-x&#39;)). mark_circle()) chart.save(&quot;screenshots/altair_cat_5.png&quot;) Here, arbitrarily reordering the levels isn’t a good idea! That’s because rincome already has a principled order that we shouldn’t mess with. Reserve sorting for factors whose levels are arbitrarily ordered. Why do you think the average age for “Not applicable” is so high? 15.4.1 Exercises There are some suspiciously high numbers in tvhours. Is the mean a good summary? For each factor in gss_cat identify whether the order of the levels is arbitrary or principled. 15.5 Modifying factor levels The pandas categorical methods for editing the categories are done using three primary methods: rename_categories(): simply pass a list of the new names. add_categories(): new list names are appended. remove_categories(): Values which are removed are replaced with np.nan. remove_unused_categories(): Drops categories with no values. You can read more about categories within pandas with the categorical data documentation. 15.5.1 Exercises How have the proportions of people identifying as Democrat, Republican, and Independent changed over time? How could you collapse rincome into a small set of categories? "],
["dates-and-times.html", "16 Dates and times 16.1 Introduction 16.2 Creating date/times 16.3 Date-time components 16.4 Time spans (Deltas) 16.5 Time zones", " 16 Dates and times 16.1 Introduction This chapter will show you how to work with dates and times in pandas. At first glance, dates and times seem simple. You use them all the time in your regular life, and they don’t seem to cause much confusion. However, the more you learn about dates and times, the more complicated they seem to get. To warm up, try these three seemingly simple questions: Does every year have 365 days? Does every day have 24 hours? Does every minute have 60 seconds? I’m sure you know that not every year has 365 days, but do you know the full rule for determining if a year is a leap year? (It has three parts.) You might have remembered that many parts of the world use daylight savings time (DST), so that some days have 23 hours, and others have 25. You might not have known that some minutes have 61 seconds because every now and then leap seconds are added because the Earth’s rotation is gradually slowing down. Dates and times are hard because they have to reconcile two physical phenomena (the rotation of the Earth and its orbit around the sun) with a whole raft of geopolitical phenomena including months, time zones, and DST. This chapter won’t teach you every last detail about dates and times, but it will give you a solid grounding of practical skills that will help you with common data analysis challenges. 16.1.1 Prerequisites This chapter will focus on the pandas time series functionality and the Timestamp methods in pandas, which makes it easier to work with dates and times in Python. We will also need nycflights13 for practice data. import pandas as pd import altair as alt import numpy as np import datetime alt.data_transformers.enable(&#39;json&#39;) #&gt; DataTransformerRegistry.enable(&#39;json&#39;) flights = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/flights/flights.csv&quot;) 16.2 Creating date/times pandas has two primary types of date/time data that refers to an instant in time: A datetime is a date plus a time: it uniquely identifies an instant in time (typically to the nearest second). pandas reports these types as datetime64. The NumPy team established this native time series data type. Being encoded as 64-bit integers allows arrays of dates to be represented very compactly ref. A Timestamp is a subclass of a datetime.datetime object. This pandas class provides the speed and ease of use in handling complex time handling. In this chapter we are only going to focus on dates and date-times. We will focus on the pandas tools for dates and times and use the datetime python module for some elements of handling time in Python. To get the current date or date-time you can use date.today() or datetime.now(): # using datetime datetime.datetime.today() #&gt; datetime.datetime(2020, 7, 16, 16, 4, 8, 879113) datetime.datetime.now() #&gt; datetime.datetime(2020, 7, 16, 16, 4, 8, 885752) datetime.datetime.utcnow() # using pandas #&gt; datetime.datetime(2020, 7, 16, 22, 4, 8, 890607) pd.to_datetime(&#39;today&#39;) #&gt; Timestamp(&#39;2020-07-16 16:04:08.896175&#39;) pd.to_datetime(&#39;now&#39;) #&gt; Timestamp(&#39;2020-07-16 22:04:08.902931&#39;) Otherwise, there are three ways you’re likely to create a date/time: From a string. From individual date-time components. From an existing date/time object. They work as follows. 16.2.1 From strings Date/time data often comes as strings. You’ve seen one approach to parsing strings into date-times in date-times. Another approach is to use the helpers provided by pandas. They can work out the format of unambiguous dates. For example: pd.to_datetime(&quot;2017-01-31&quot;) #&gt; Timestamp(&#39;2017-01-31 00:00:00&#39;) pd.to_datetime(&quot;January 31st, 2017&quot;) #&gt; Timestamp(&#39;2017-01-31 00:00:00&#39;) pd.to_datetime(&quot;31-Jan-2017&quot;) # using timestamp #&gt; Timestamp(&#39;2017-01-31 00:00:00&#39;) pd.Timestamp(&quot;2017-01-31&quot;) #&gt; Timestamp(&#39;2017-01-31 00:00:00&#39;) pd.Timestamp(&quot;January 31st, 2017&quot;) #&gt; Timestamp(&#39;2017-01-31 00:00:00&#39;) pd.Timestamp(&quot;31-Jan-2017&quot;) #&gt; Timestamp(&#39;2017-01-31 00:00:00&#39;) You can specify component order for ambiguous dates with pd.to_datetime(). For example: pd.to_datetime(&#39;10-11-09&#39;) #&gt; Timestamp(&#39;2009-10-11 00:00:00&#39;) pd.to_datetime(&#39;10-11-09&#39;, yearfirst=True) #&gt; Timestamp(&#39;2010-11-09 00:00:00&#39;) pd.to_datetime(&#39;10-11-09&#39;, dayfirst=True) #&gt; Timestamp(&#39;2009-11-10 00:00:00&#39;) pd.to_datetime(&#39;10-11-09&#39;, format = &#39;%d-%y-%m&#39;) #&gt; Timestamp(&#39;2011-09-10 00:00:00&#39;) If you use numbers in pd.to_datetime() it is interpreted as the number of nanoseconds from 1970-01-01. However, the unit can be changed. pd.to_datetime(100000000) #&gt; Timestamp(&#39;1970-01-01 00:00:00.100000&#39;) pd.to_datetime(100000000, unit = &#39;ns&#39;) #&gt; Timestamp(&#39;1970-01-01 00:00:00.100000&#39;) pd.to_datetime(1, unit = &#39;s&#39;) #&gt; Timestamp(&#39;1970-01-01 00:00:01&#39;) To input time into a datetime add the hh:mm:ss format into the string of the parsing function: pd.to_datetime(&quot;2017-01-31 20:11:59&quot;) #&gt; Timestamp(&#39;2017-01-31 20:11:59&#39;) pd.to_datetime(&quot;01/31/2017 08:01&quot;) #&gt; Timestamp(&#39;2017-01-31 08:01:00&#39;) pd.Timestamp(&quot;2017-01-31 20:11:59&quot;) #&gt; Timestamp(&#39;2017-01-31 20:11:59&#39;) pd.Timestamp(&quot;01/31/2017 08:01&quot;) #&gt; Timestamp(&#39;2017-01-31 08:01:00&#39;) 16.2.2 From individual components Instead of a single string, sometimes you’ll have the individual components of the date-time spread across multiple columns. This is what we have in the flights data: flights.filter([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;, &#39;minute&#39;]).head() #&gt; year month day hour minute #&gt; 0 2013 1 1 5 15 #&gt; 1 2013 1 1 5 29 #&gt; 2 2013 1 1 5 40 #&gt; 3 2013 1 1 5 45 #&gt; 4 2013 1 1 6 0 To create a date/time from this sort of input, use Timestamp(): (flights. filter([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;, &#39;minute&#39;]). assign(departure = pd.to_datetime(flights[[&#39;year&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;, &#39;minute&#39;]])) ) #&gt; year month day hour minute departure #&gt; 0 2013 1 1 5 15 2013-01-01 05:15:00 #&gt; 1 2013 1 1 5 29 2013-01-01 05:29:00 #&gt; 2 2013 1 1 5 40 2013-01-01 05:40:00 #&gt; 3 2013 1 1 5 45 2013-01-01 05:45:00 #&gt; 4 2013 1 1 6 0 2013-01-01 06:00:00 #&gt; ... ... ... ... ... ... ... #&gt; 336771 2013 9 30 14 55 2013-09-30 14:55:00 #&gt; 336772 2013 9 30 22 0 2013-09-30 22:00:00 #&gt; 336773 2013 9 30 12 10 2013-09-30 12:10:00 #&gt; 336774 2013 9 30 11 59 2013-09-30 11:59:00 #&gt; 336775 2013 9 30 8 40 2013-09-30 08:40:00 #&gt; #&gt; [336776 rows x 6 columns] Let’s do the same thing for each of the four time columns in flights. The times are represented in a slightly odd format, so we use modulus arithmetic to pull out the hour and minute components. Once I’ve created the date-time variables, I focus in on the variables we’ll explore in the rest of the chapter. # parsing function def make_datetime_100(data, time): date_data = (data[[&#39;year&#39;, &#39;month&#39;, &#39;day&#39;, time]]. assign( hour = lambda x: x[time] // 100, minute = lambda x: x[time] % 100). filter([&#39;year&#39;, &#39;month&#39;, &#39;day&#39;, &#39;hour&#39;, &#39;minute&#39;]) ) out = pd.to_datetime(date_data) return out # data table flights_dt = (flights. query(&#39;(dep_time.notna()) &amp; (arr_time.notna())&#39;). assign( dep_time = lambda x: make_datetime_100(x, &#39;dep_time&#39;), arr_time = lambda x: make_datetime_100(x, &#39;arr_time&#39;), sched_dep_time = lambda x: make_datetime_100(x, &#39;sched_dep_time&#39;), sched_arr_time = lambda x: make_datetime_100(x, &#39;sched_arr_time&#39;) ). filter( regex = &#39;origin|dest|delay$|time$&#39;) ) flights_dt #&gt; dep_time sched_dep_time dep_delay ... origin dest air_time #&gt; 0 2013-01-01 05:17:00 2013-01-01 05:15:00 2.0 ... EWR IAH 227.0 #&gt; 1 2013-01-01 05:33:00 2013-01-01 05:29:00 4.0 ... LGA IAH 227.0 #&gt; 2 2013-01-01 05:42:00 2013-01-01 05:40:00 2.0 ... JFK MIA 160.0 #&gt; 3 2013-01-01 05:44:00 2013-01-01 05:45:00 -1.0 ... JFK BQN 183.0 #&gt; 4 2013-01-01 05:54:00 2013-01-01 06:00:00 -6.0 ... LGA ATL 116.0 #&gt; ... ... ... ... ... ... ... ... #&gt; 336765 2013-09-30 22:40:00 2013-09-30 22:45:00 -5.0 ... JFK SYR 41.0 #&gt; 336766 2013-09-30 22:40:00 2013-09-30 22:50:00 -10.0 ... JFK BUF 52.0 #&gt; 336767 2013-09-30 22:41:00 2013-09-30 22:46:00 -5.0 ... JFK ROC 47.0 #&gt; 336768 2013-09-30 23:07:00 2013-09-30 22:55:00 12.0 ... JFK BOS 33.0 #&gt; 336769 2013-09-30 23:49:00 2013-09-30 23:59:00 -10.0 ... JFK PSE 196.0 #&gt; #&gt; [328063 rows x 9 columns] With this data, I can visualise the distribution of departure times across the year: dat_plot = (flights_dt. filter([&#39;dep_time&#39;]). assign(dpd = flights_dt.dep_time.dt.floor(&quot;D&quot;)). groupby([&#39;dpd&#39;], as_index = False). count()) chart = (alt.Chart(dat_plot). encode(x = &#39;dpd&#39;, y = &#39;dep_time&#39;). mark_line()) chart.save(&quot;screenshots/altair_datetimes_1.png&quot;) Or within a single day: dat_plot = (flights_dt.filter([&#39;dep_time&#39;]).query(&quot;dep_time &lt; 20130102&quot;). assign(dpd = flights_dt.dep_time.dt.floor(&quot;10min&quot;)). groupby([&#39;dpd&#39;], as_index = False). count()) chart = (alt.Chart(dat_plot). encode(x = &#39;dpd&#39;, y = &#39;dep_time&#39;). mark_line()) chart.save(&quot;screenshots/altair_datetimes_2.png&quot;) 16.2.3 From other types Sometimes you’ll get date/times as numeric offsets from the “Unix Epoch”, 1970-01-01. If the offset is in seconds, use as_datetime(); if it’s in days, use as_date(). pd.to_datetime(60 * 60 * 10, unit = &#39;s&#39;) #&gt; Timestamp(&#39;1970-01-01 10:00:00&#39;) pd.to_datetime(365 * 10 + 2, unit = &#39;d&#39;) #&gt; Timestamp(&#39;1980-01-01 00:00:00&#39;) 16.2.4 Exercises What happens if you parse a string that contains invalid dates? pd.to_datetime([&#39;2010-10-10&#39;, &#39;bananas&#39;]) pd.to_datetime([&#39;2010-10-10&#39;, &#39;bananas&#39;], errors = &#39;coerce&#39;) 16.3 Date-time components Now that you know how to get date-time data into Python’s datetime data structures, let’s explore what you can do with them. This section will focus on the Timestamp properites that let you get and set individual components. The next section will look at how arithmetic works with datetimes. 16.3.1 Getting components You can pull out individual parts of the date with the accessor properties .year, .month, .day() (day of the month), .dayofyear (day of the year), .dayofweek (day of the week), .hour, .minute, and .second. When using these properties on a pandas series you must include .dt. as in &lt;PANDAS SERIES&gt;.dt.month. datetime = pd.to_datetime(&quot;2016-07-08 12:34:56&quot;) datetime.year #&gt; 2016 datetime.month #&gt; 7 datetime.day #&gt; 8 datetime.dayofyear #&gt; 190 datetime.dayofweek #&gt; 4 To return the name of the month or day of the week use the two name methods .day_name() and month_name(). datetime.month_name() datetime.day_name() We can use .day_name() in pandas to create a new variable for your chart or use day() in Altair to build the date variable only for the chart using the TimeUnit transforms to see that more flights depart during the week than on the weekend: flights_dt.dep_time.dt.day_name() #&gt; 0 Tuesday #&gt; 1 Tuesday #&gt; 2 Tuesday #&gt; 3 Tuesday #&gt; 4 Tuesday #&gt; ... #&gt; 336765 Monday #&gt; 336766 Monday #&gt; 336767 Monday #&gt; 336768 Monday #&gt; 336769 Monday #&gt; Name: dep_time, Length: 328063, dtype: object chart = (alt.Chart(flights_dt). encode(alt.X(&#39;day(dep_time):O&#39;), alt.Y(&#39;count()&#39;)). mark_bar(). properties(width = 400)) chart.save(&quot;screenshots/altair_datetimes_3.png&quot;) There’s an interesting pattern if we look at the average departure delay by minute within the hour. It looks like flights leaving in minutes 20-30 and 50-60 have much lower delays than the rest of the hour! plot_dat = (flights_dt.assign( minute = lambda x: x.dep_time.dt.minute). groupby(&#39;minute&#39;). agg( avg_delay = (&#39;arr_delay&#39;, np.mean), n = (&#39;arr_delay&#39;, &#39;size&#39;) ).reset_index() ) chart = (alt.Chart(plot_dat). encode(alt.X(&#39;minute&#39;), alt.Y(&#39;avg_delay&#39;)). mark_line()) chart.save(&quot;screenshots/altair_datetimes_4.png&quot;) Interestingly, if we look at the scheduled departure time we don’t see such a strong pattern: sched_dep = (flights_dt.assign( minute = lambda x: x.sched_dep_time.dt.minute). groupby(&#39;minute&#39;). agg( avg_delay = (&#39;arr_delay&#39;, np.mean), n = (&#39;arr_delay&#39;, &#39;size&#39;) ).reset_index() ) chart = (alt.Chart(sched_dep). encode(alt.X(&#39;minute&#39;), alt.Y(&#39;avg_delay&#39;)). mark_line()) chart.save(&quot;screenshots/altair_datetimes_5.png&quot;) So why do we see that pattern with the actual departure times? Well, like much data collected by humans, there’s a strong bias towards flights leaving at “nice” departure times. Always be alert for this sort of pattern whenever you work with data that involves human judgement! chart = (alt.Chart(sched_dep). encode(alt.X(&#39;minute&#39;), alt.Y(&#39;n&#39;)). mark_line()) chart.save(&quot;screenshots/altair_datetimes_6.png&quot;) 16.3.2 Rounding An alternative approach to plotting individual components is to round the date to a nearby unit of time, with .dt.floor(), .dt.round(), and .dt.ceiling(). However, these methods don’t work well for coarser roundings. For example, finding the floor of week does not work using dt.floor() and the recommended approach is to use .dt.to_period() in tandem with .dt.to_timestep() as we do in the following example. Each method takes a Series of datetimes to adjust and then the name of the unit round down (floor), round up (ceiling), or round to. This, for example, allows us to plot the number of flights per week: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases plot_dat = flights_dt.assign( week = lambda x: x.dep_time.dt.to_period(&quot;W&quot;).dt.to_timestamp() ).groupby(&#39;week&#39;).agg( n = (&#39;dep_time&#39;, &#39;size&#39;) ).reset_index() chart = (alt.Chart(plot_dat). encode(alt.X(&#39;week&#39;), alt.Y(&#39;n&#39;)). mark_line()) chart.save(&quot;screenshots/altair_datetimes_7.png&quot;) Computing the difference between a rounded and unrounded date can be particularly useful. 16.3.3 Setting components You can also use each accessor function to set the components of a date/time: datetime = pd.Timestamp(&quot;2016-07-08 12:34:56&quot;) datetime.replace(year = 2020) #&gt; Timestamp(&#39;2020-07-08 12:34:56&#39;) datetime.replace(month = 1) #&gt; Timestamp(&#39;2016-01-08 12:34:56&#39;) datetime.replace(hour = datetime.hour + 1) #&gt; Timestamp(&#39;2016-07-08 13:34:56&#39;) Alternatively, Rather than modifying each separately, you can create a new date-time with replace() using all of the arguments. This allows you to set multiple values at once. datetime.replace(year = 2020, month = 2, day = 2, hour = 2) #&gt; Timestamp(&#39;2020-02-02 02:34:56&#39;) If values are too big, you will see an error: pd.Timestamp(&quot;2015-02-01&quot;).replace(day = 30) #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): ValueError: day is out of range for month #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; #&gt; File &quot;pandas/_libs/tslibs/timestamps.pyx&quot;, line 950, in pandas._libs.tslibs.timestamps.Timestamp.replace You can use replace() to show the distribution of flights across the course of the day for every day of the year: Note: The Timestamp.replace() method does not have an equivalent method that can be used on pandas Series so you will need to use an apply() method on the datetime series. We have avoided the use of apply() in this book up to this point. apply() is a powerful technique within pandas that is worth your time to master. You can read the pandas documentation to find additional help. plot_dat = flights_dt.assign( dep_hour = lambda x: x.dep_time.apply(lambda y: y.replace(month = 1, day = 1)) ) chart = (alt.Chart(plot_dat). encode(alt.X(&#39;dep_hour&#39;), alt.Y(&#39;count()&#39;)). mark_line()) chart.save(&quot;screenshots/altair_datetimes_8.png&quot;) Setting larger components of a date to a constant is a powerful technique that allows you to explore patterns in the smaller components. 16.3.4 Exercises How does the distribution of flight times within a day change over the course of the year? Compare dep_time, sched_dep_time and dep_delay. Are they consistent? Explain your findings. Compare air_time with the duration between the departure and arrival. Explain your findings. (Hint: consider the location of the airport.) How does the average delay time change over the course of a day? Should you use dep_time or sched_dep_time? Why? On what day of the week should you leave if you want to minimise the chance of a delay? What makes the distribution of diamonds.carat and flights.sched_dep_time similar? Confirm my hypothesis that the early departures of flights in minutes 20-30 and 50-60 are caused by scheduled flights that leave early. Hint: create a binary variable that tells you whether or not a flight was delayed. 16.4 Time spans (Deltas) Next you’ll learn about how arithmetic with dates work, including subtraction, addition, and division. Along the way, you’ll learn about pandas tools for representing time spans. In Python, when you subtract two dates, you get a Timedelta object: # How old is Hathaway? h_age = pd.Timestamp.now() - pd.Timestamp(&quot;19770907&quot;) h_age #&gt; Timedelta(&#39;15653 days 16:16:11.697669&#39;) A Timedelta object records a time span in nanoseconds using 64 bit integers which can make Timedeltas a little painful to work with, so Timedelta provides access to the number of days with .days and the number of seconds with .total_seconds(). h_age.days h_age.days // 365 # to get years h_age.total_seconds() Timedeltas come with a bunch of convenient constructors: pd.Timedelta(seconds = 15) #&gt; Timedelta(&#39;0 days 00:00:15&#39;) pd.Timedelta(minutes = 10) #&gt; Timedelta(&#39;0 days 00:10:00&#39;) pd.Timedelta(hours = 12) #&gt; Timedelta(&#39;0 days 12:00:00&#39;) pd.Timedelta(hours = 24) #&gt; Timedelta(&#39;1 days 00:00:00&#39;) pd.Timedelta(days = 5) #&gt; Timedelta(&#39;5 days 00:00:00&#39;) pd.Timedelta(weeks = 3) # to create a series of Timedeltas you can use a for loop. #&gt; Timedelta(&#39;21 days 00:00:00&#39;) pd.Series([pd.Timedelta(days=i) for i in range(5)]) #&gt; 0 0 days #&gt; 1 1 days #&gt; 2 2 days #&gt; 3 3 days #&gt; 4 4 days #&gt; dtype: timedelta64[ns] 2*pd.Timedelta(days = 365) #&gt; Timedelta(&#39;730 days 00:00:00&#39;) pd.Timedelta(days = 365) + pd.Timedelta(weeks = 12) + pd.Timedelta(hours = 15) #&gt; Timedelta(&#39;449 days 15:00:00&#39;) You can add and subtract Timedeltas to and from days: tomorrow = pd.Timestamp.today() + pd.Timedelta(days = 1) last_year = pd.Timestamp.today() - pd.Timedelta(days = 365) And of course, add them to dates. Timedeltas are strict durations based on the arguments. Those durations do work across the various novelties of daylight savings time and leaps in the calendar like February 29th. # A leap year pd.Timestamp(&quot;2020-02-01&quot;) + pd.Timedelta(days = 30) # Not a leap year #&gt; Timestamp(&#39;2020-03-02 00:00:00&#39;) pd.Timestamp(&quot;2019-02-01&quot;) + pd.Timedelta(days = 30) # Daylight Savings Time #&gt; Timestamp(&#39;2019-03-03 00:00:00&#39;) one_pm = pd.Timestamp(&#39;2016-03-12 13:00:00&#39;, tz = &#39;America/New_York&#39;) # Day two 1pm one_pm_d2 = pd.Timestamp(&#39;2016-03-13 13:00:00&#39;, tz = &#39;America/New_York&#39;) one_pm + pd.Timedelta(days = 1) #&gt; Timestamp(&#39;2016-03-13 14:00:00-0400&#39;, tz=&#39;America/New_York&#39;) one_pm + pd.Timedelta(hours = 24) # Notice that it correctly represents that only 23 hours had passed. #&gt; Timestamp(&#39;2016-03-13 14:00:00-0400&#39;, tz=&#39;America/New_York&#39;) one_pm_d2 - one_pm #&gt; Timedelta(&#39;0 days 23:00:00&#39;) Let’s use Timedeltas to fix an oddity related to our flight dates. Some planes appear to have arrived at their destination before they departed from New York City. flights_dt.query(&#39;arr_time &lt; dep_time&#39;) #&gt; dep_time sched_dep_time dep_delay ... origin dest air_time #&gt; 719 2013-01-01 19:29:00 2013-01-01 19:20:00 9.0 ... EWR BQN 192.0 #&gt; 725 2013-01-01 19:39:00 2013-01-01 18:40:00 59.0 ... JFK DFW NaN #&gt; 791 2013-01-01 20:58:00 2013-01-01 21:00:00 -2.0 ... EWR TPA 159.0 #&gt; 794 2013-01-01 21:02:00 2013-01-01 21:08:00 -6.0 ... EWR SJU 199.0 #&gt; 797 2013-01-01 21:08:00 2013-01-01 20:57:00 11.0 ... EWR SFO 354.0 #&gt; ... ... ... ... ... ... ... ... #&gt; 336753 2013-09-30 21:45:00 2013-09-30 21:45:00 0.0 ... JFK SJU 192.0 #&gt; 336754 2013-09-30 21:47:00 2013-09-30 21:37:00 10.0 ... LGA FLL 139.0 #&gt; 336762 2013-09-30 22:33:00 2013-09-30 21:13:00 80.0 ... EWR SFO 318.0 #&gt; 336763 2013-09-30 22:35:00 2013-09-30 20:01:00 154.0 ... JFK MCO 123.0 #&gt; 336769 2013-09-30 23:49:00 2013-09-30 23:59:00 -10.0 ... JFK PSE 196.0 #&gt; #&gt; [10633 rows x 9 columns] These are overnight flights. We used the same date information for both the departure and the arrival times, but these flights arrived on the following day. We can fix this by adding days(1) to the arrival time of each overnight flight. flights_dt = flights_dt.assign( overnight_TF = lambda x: x.arr_time &lt; x.dep_time, overnight = lambda x: x.overnight_TF.astype(&#39;int&#39;).apply(lambda y: pd.Timedelta(days = y)), arr_time = lambda x: x.arr_time + x.overnight, sched_arr_time = lambda x: x.sched_arr_time + x.overnight ) Now all of our flights obey the laws of physics. flights_dt.query(&#39;arr_time &lt; dep_time&#39;) #&gt; Empty DataFrame #&gt; Columns: [dep_time, sched_dep_time, dep_delay, arr_time, sched_arr_time, arr_delay, origin, dest, air_time, overnight_TF, overnight] #&gt; Index: [] 16.5 Time zones Time zones are an enormously complicated topic because of their interaction with geopolitical entities. Fortunately we don’t need to dig into all the details as they’re not all important for data analysis, but there are a few challenges we’ll need to tackle head on. The first challenge is that everyday names of time zones tend to be ambiguous. For example, if you’re American you’re probably familiar with EST, or Eastern Standard Time. However, both Australia and Canada also have EST! To avoid confusion, The Python package pytz uses the international standard IANA time zones. These use a consistent naming scheme “/”, typically in the form “&lt;continent&gt;/&lt;city&gt;” (there are a few exceptions because not every country lies on a continent). Examples include “America/New_York”, “Europe/Paris”, and “Pacific/Auckland”. Pandas leverages the pytz package to handle time zones. You might wonder why the time zone uses a city, when typically you think of time zones as associated with a country or region within a country. This is because the IANA database has to record decades worth of time zone rules. In the course of decades, countries change names (or break apart) fairly frequently, but city names tend to stay the same. Another problem is that name needs to reflect not only to the current behaviour, but also the complete history. For example, there are time zones for both “America/New_York” and “America/Detroit”. These cities both currently use Eastern Standard Time but in 1969-1972 Michigan (the state in which Detroit is located), did not follow DST, so it needs a different name. It’s worth reading the raw time zone database (available at http://www.iana.org/time-zones) just to read some of these stories! You can find out what Python thinks your current time zone is with Sys.timezone(): import time time.tzname #&gt; (&#39;MST&#39;, &#39;MDT&#39;) To see the complete list of all time zone Olson names for a country use pytz.country_timezones(): import pytz pytz.country_timezones(&#39;US&#39;) # pytz.all_timezones() #&gt; [&#39;America/New_York&#39;, &#39;America/Detroit&#39;, &#39;America/Kentucky/Louisville&#39;, &#39;America/Kentucky/Monticello&#39;, &#39;America/Indiana/Indianapolis&#39;, &#39;America/Indiana/Vincennes&#39;, &#39;America/Indiana/Winamac&#39;, &#39;America/Indiana/Marengo&#39;, &#39;America/Indiana/Petersburg&#39;, &#39;America/Indiana/Vevay&#39;, &#39;America/Chicago&#39;, &#39;America/Indiana/Tell_City&#39;, &#39;America/Indiana/Knox&#39;, &#39;America/Menominee&#39;, &#39;America/North_Dakota/Center&#39;, &#39;America/North_Dakota/New_Salem&#39;, &#39;America/North_Dakota/Beulah&#39;, &#39;America/Denver&#39;, &#39;America/Boise&#39;, &#39;America/Phoenix&#39;, &#39;America/Los_Angeles&#39;, &#39;America/Anchorage&#39;, &#39;America/Juneau&#39;, &#39;America/Sitka&#39;, &#39;America/Metlakatla&#39;, &#39;America/Yakutat&#39;, &#39;America/Nome&#39;, &#39;America/Adak&#39;, &#39;Pacific/Honolulu&#39;] In Python, the time zone is an attribute of the date-time that only controls printing. For example, these three objects represent the same instant in time: x1 = pd.Timestamp(&quot;2015-06-01 12:00:00&quot;, tz = &#39;America/New_York&#39;) x2 = pd.Timestamp(&quot;2015-06-01 18:00:00&quot;, tz = &#39;Europe/Copenhagen&#39;) x3 = pd.Timestamp(&quot;2015-06-01 04:00:00&quot;, tz = &#39;Pacific/Auckland&#39;) You can verify that they’re the same time by converting to UTC and comparing the offsets x1.tz_convert(&#39;utc&#39;) #&gt; Timestamp(&#39;2015-06-01 16:00:00+0000&#39;, tz=&#39;UTC&#39;) x2.tz_convert(&#39;utc&#39;) #&gt; Timestamp(&#39;2015-06-01 16:00:00+0000&#39;, tz=&#39;UTC&#39;) x3.tz_convert(&#39;utc&#39;) #&gt; Timestamp(&#39;2015-05-31 16:00:00+0000&#39;, tz=&#39;UTC&#39;) You can’t do subtraction with Timestamps of different time zones: x1 - x2 #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: Timestamp subtraction must have the same timezones or no timezones #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; #&gt; File &quot;pandas/_libs/tslibs/c_timestamp.pyx&quot;, line 300, in pandas._libs.tslibs.c_timestamp._Timestamp.__sub__ x1 - x3 #&gt; Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: Timestamp subtraction must have the same timezones or no timezones #&gt; #&gt; Detailed traceback: #&gt; File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; #&gt; File &quot;pandas/_libs/tslibs/c_timestamp.pyx&quot;, line 300, in pandas._libs.tslibs.c_timestamp._Timestamp.__sub__ But Timedeltas can be calculated after using .tz_convert(): x1.tz_convert(&#39;utc&#39;) - x2.tz_convert(&#39;utc&#39;) #&gt; Timedelta(&#39;0 days 00:00:00&#39;) Unless otherwise specified, pd.Timestampe() is not timezone aware. UTC (Coordinated Universal Time) is the standard time zone used by the scientific community and roughly equivalent to its predecessor GMT (Greenwich Mean Time). It does not have DST, which makes a convenient representation for computation. Operations that combine date-times into a Series will often convert the time zone to UTC and then return the Series as an object or character string. x4 = pd.Series([x1, x2, x3]) x4 #&gt; 0 2015-06-01 12:00:00-04:00 #&gt; 1 2015-06-01 18:00:00+02:00 #&gt; 2 2015-06-01 04:00:00+12:00 #&gt; dtype: object Combining elements that all have the same time zone will return a datetime object with that specified time zone: x5 = pd.Series([x1.tz_convert(&#39;utc&#39;), x2.tz_convert(&#39;utc&#39;), x3.tz_convert(&#39;utc&#39;)]) x5 #&gt; 0 2015-06-01 16:00:00+00:00 #&gt; 1 2015-06-01 16:00:00+00:00 #&gt; 2 2015-05-31 16:00:00+00:00 #&gt; dtype: datetime64[ns, UTC] Finally, Timestamp objects that don’t have a specified time zone will state ase datetime objects: pd.Series([pd.Timestamp(&quot;2015-06-01 04:00:00&quot;), pd.Timestamp(&quot;2015-06-01 04:00:00&quot;)]) #&gt; 0 2015-06-01 04:00:00 #&gt; 1 2015-06-01 04:00:00 #&gt; dtype: datetime64[ns] You can change the time zone in two ways: Keep the instant in time the same, and change how it’s displayed. Use this when the instant is correct, but you want a more natural display. x5a = x5.dt.tz_convert(&#39;Australia/Lord_Howe&#39;) x5a #&gt; 0 2015-06-02 02:30:00+10:30 #&gt; 1 2015-06-02 02:30:00+10:30 #&gt; 2 2015-06-01 02:30:00+10:30 #&gt; dtype: datetime64[ns, Australia/Lord_Howe] (This also illustrates another challenge of times zones: they’re not all integer hour offsets!) Change the underlying instant in time. Use None to strip the time zone when you have an instant that has been labelled with the incorrect time zone, and you need to fix it. x5b = x5.dt.tz_localize(None).dt.tz_localize(&#39;Australia/Lord_Howe&#39;) x5b #&gt; 0 2015-06-01 16:00:00+10:30 #&gt; 1 2015-06-01 16:00:00+10:30 #&gt; 2 2015-05-31 16:00:00+10:30 #&gt; dtype: datetime64[ns, Australia/Lord_Howe] "],
["program-intro.html", "17 Introduction 17.1 Learning more", " 17 Introduction In this part of the book, you’ll improve your programming skills. Programming is a cross-cutting skill needed for all data science work: you must use a computer to do data science; you cannot do it in your head, or with pencil and paper. Note: This section of the book may never be completed for the python4ds version. Programming produces code, and code is a tool of communication. Obviously code tells the computer what you want it to do. But it also communicates meaning to other humans. Thinking about code as a vehicle for communication is important because every project you do is fundamentally collaborative. Even if you’re not working with other people, you’ll definitely be working with future-you! Writing clear code is important so that others (like future-you) can understand why you tackled an analysis in the way you did. That means getting better at programming also involves getting better at communicating. Over time, you want your code to become not just easier to write, but easier for others to read. Writing code is similar in many ways to writing prose. One parallel which I find particularly useful is that in both cases rewriting is the key to clarity. The first expression of your ideas is unlikely to be particularly clear, and you may need to rewrite multiple times. After solving a data analysis challenge, it’s often worth looking at your code and thinking about whether or not it’s obvious what you’ve done. If you spend a little time rewriting your code while the ideas are fresh, you can save a lot of time later trying to recreate what your code did. But this doesn’t mean you should rewrite every function: you need to balance what you need to achieve now with saving time in the long run. (But the more you rewrite your functions the more likely your first attempt will be clear.) In the following four chapters, you’ll learn skills that will allow you to both tackle new programs and to solve existing problems with greater clarity and ease: Copy-and-paste is a powerful tool, but you should avoid doing it more than twice. Repeating yourself in code is dangerous because it can easily lead to errors and inconsistencies. Instead, in [functions], you’ll learn how to write functions which let you extract out repeated code so that it can be easily reused. As you start to write more powerful functions, you’ll need a solid grounding in R’s data structures, provided by [vectors]. You must master the four common atomic vectors, the three important S3 classes built on top of them, and understand the mysteries of the list and data frame. Functions extract out repeated code, but you often need to repeat the same actions on different inputs. You need tools for iteration that let you do similar things again and again. These tools include for loops and functional programming, which you’ll learn about in [iteration]. 17.1 Learning more The goal of these chapters is to teach you the minimum about programming that you need to practice data science, which turns out to be a reasonable amount. Once you have mastered the material in this book, I strongly believe you should invest further in your programming skills. Learning more about programming is a long-term investment: it won’t pay off immediately, but in the long term it will allow you to solve new problems more quickly, and let you reuse your insights from previous problems in new scenarios. To learn more you need to study Python as a programming language, not just an interactive environment for data science. The Python Tutorial. This is an introduction to Python as a programming language and is a great place to start if Python is your first programming language. "],
["pipes-not-included.html", "18 Pipes (Not included) 18.1 Introduction", " 18 Pipes (Not included) 18.1 Introduction Not currently used in Python data science packages. "],
["functions-tbd.html", "19 Functions (TBD) 19.1 Introduction", " 19 Functions (TBD) 19.1 Introduction One of the best ways to improve your reach as a data scientist is to write functions. Functions allow you to automate common tasks in a more powerful and general way than copy-and-pasting. Writing a function has three big advantages over using copy-and-paste: You can give a function an evocative name that makes your code easier to understand. As requirements change, you only need to update code in one place, instead of many. You eliminate the chance of making incidental mistakes when you copy and paste (i.e. updating a variable name in one place, but not in another). Writing good functions is a lifetime journey. Even after using R for many years I still learn new techniques and better ways of approaching old problems. The goal of this chapter is not to teach you every esoteric detail of functions but to get you started with some pragmatic advice that you can apply immediately. As well as practical advice for writing functions, this chapter also gives you some suggestions for how to style your code. Good code style is like correct punctuation. Youcanmanagewithoutit, but it sure makes things easier to read! As with styles of punctuation, there are many possible variations. Here we present the style we use in our code, but the most important thing is to be consistent. "],
["vectors-tbd.html", "20 Vectors (TBD) 20.1 Introduction", " 20 Vectors (TBD) 20.1 Introduction So far this book has focussed on series or data frames and functions that work with them. But as you start to write your own functions, and dig deeper into Python, you need to learn about NumPy. The package that underlies pandas. If you’ve learned Python in a more traditional scientific computing manner, you’re probably already familiar with NumPy. "],
["iteration-tbd.html", "21 Iteration (TBD) 21.1 Introduction", " 21 Iteration (TBD) 21.1 Introduction In [functions], we talked about how important it is to reduce duplication in your code by creating functions instead of copying-and-pasting. Reducing code duplication has three main benefits: It’s easier to see the intent of your code, because your eyes are drawn to what’s different, not what stays the same. It’s easier to respond to changes in requirements. As your needs change, you only need to make changes in one place, rather than remembering to change every place that you copied-and-pasted the code. You’re likely to have fewer bugs because each line of code is used in more places. One tool for reducing duplication is functions, which reduce duplication by identifying repeated patterns of code and extract them out into independent pieces that can be easily reused and updated. Another tool for reducing duplication is iteration, which helps you when you need to do the same thing to multiple inputs: repeating the same operation on different columns, or on different datasets. In this chapter you’ll learn about two important iteration paradigms: imperative programming and functional programming. On the imperative side you have tools like for loops and while loops, which are a great place to start because they make iteration very explicit, so it’s obvious what’s happening. However, for loops are quite verbose, and require quite a bit of bookkeeping code that is duplicated for every for loop. Functional programming (FP) offers tools to extract out this duplicated code, so each common for loop pattern gets its own function. Once you master the vocabulary of FP, you can solve many common iteration problems with less code, more ease, and fewer errors. "],
["model-intro.html", "22 Introduction 22.1 Hypothesis generation vs. hypothesis confirmation", " 22 Introduction Now that you are equipped with powerful programming tools we can finally return to modelling. You’ll use your new tools of data wrangling and programming, to fit many models and understand how they work. The focus of this book is on exploration, not confirmation or formal inference. But you’ll learn a few basic tools that help you understand the variation within your models. The goal of a model is to provide a simple low-dimensional summary of a dataset. Ideally, the model will capture true “signals” (i.e. patterns generated by the phenomenon of interest), and ignore “noise” (i.e. random variation that you’re not interested in). Here we only cover “predictive” models, which, as the name suggests, generate predictions. There is another type of model that we’re not going to discuss: “data discovery” models. These models don’t make predictions, but instead help you discover interesting relationships within your data. (These two categories of models are sometimes called supervised and unsupervised, but I don’t think that terminology is particularly illuminating.) This book is not going to give you a deep understanding of the mathematical theory that underlies models. It will, however, build your intuition about how statistical models work, and give you a family of useful tools that allow you to use models to better understand your data: In [model basics], you’ll learn how models work mechanistically, focussing on the important family of linear models. You’ll learn general tools for gaining insight into what a predictive model tells you about your data, focussing on simple simulated datasets. In [model building], you’ll learn how to use models to pull out known patterns in real data. Once you have recognised an important pattern it’s useful to make it explicit in a model, because then you can more easily see the subtler signals that remain. In [many models], you’ll learn how to use many simple models to help understand complex datasets. This is a powerful technique, but to access it you’ll need to combine modelling and programming tools. These topics are notable because of what they don’t include: any tools for quantitatively assessing models. That is deliberate: precisely quantifying a model requires a couple of big ideas that we just don’t have the space to cover here. For now, you’ll rely on qualitative assessment and your natural scepticism. In [Learning more about models], we’ll point you to other resources where you can learn more. 22.1 Hypothesis generation vs. hypothesis confirmation In this book, we are going to use models as a tool for exploration, completing the trifecta of the tools for EDA that were introduced in Part 1. This is not how models are usually taught, but as you will see, models are an important tool for exploration. Traditionally, the focus of modelling is on inference, or for confirming that an hypothesis is true. Doing this correctly is not complicated, but it is hard. There is a pair of ideas that you must understand in order to do inference correctly: Each observation can either be used for exploration or confirmation, not both. You can use an observation as many times as you like for exploration, but you can only use it once for confirmation. As soon as you use an observation twice, you’ve switched from confirmation to exploration. This is necessary because to confirm a hypothesis you must use data independent of the data that you used to generate the hypothesis. Otherwise you will be over optimistic. There is absolutely nothing wrong with exploration, but you should never sell an exploratory analysis as a confirmatory analysis because it is fundamentally misleading. If you are serious about doing an confirmatory analysis, one approach is to split your data into three pieces before you begin the analysis: 60% of your data goes into a training (or exploration) set. You’re allowed to do anything you like with this data: visualise it and fit tons of models to it. 20% goes into a query set. You can use this data to compare models or visualisations by hand, but you’re not allowed to use it as part of an automated process. 20% is held back for a test set. You can only use this data ONCE, to test your final model. This partitioning allows you to explore the training data, occasionally generating candidate hypotheses that you check with the query set. When you are confident you have the right model, you can check it once with the test data. (Note that even when doing confirmatory modelling, you will still need to do EDA. If you don’t do any EDA you will remain blind to the quality problems with your data.) "],
["model-basics-tbd.html", "23 Model basics (TBD) 23.1 Introduction", " 23 Model basics (TBD) 23.1 Introduction The goal of a model is to provide a simple low-dimensional summary of a dataset. In the context of this book we’re going to use models to partition data into patterns and residuals. Strong patterns will hide subtler trends, so we’ll use models to help peel back layers of structure as we explore a dataset. However, before we can start using models on interesting, real, datasets, you need to understand the basics of how models work. For that reason, this chapter of the book is unique because it uses only simulated datasets. These datasets are very simple, and not at all interesting, but they will help you understand the essence of modelling before you apply the same techniques to real data in the next chapter. There are two parts to a model: First, you define a family of models that express a precise, but generic, pattern that you want to capture. For example, the pattern might be a straight line, or a quadratic curve. You will express the model family as an equation like y = a_1 * x + a_2 or y = a_1 * x ^ a_2. Here, x and y are known variables from your data, and a_1 and a_2 are parameters that can vary to capture different patterns. Next, you generate a fitted model by finding the model from the family that is the closest to your data. This takes the generic model family and makes it specific, like y = 3 * x + 7 or y = 9 * x ^ 2. It’s important to understand that a fitted model is just the closest model from a family of models. That implies that you have the “best” model (according to some criteria); it doesn’t imply that you have a good model and it certainly doesn’t imply that the model is “true”. George Box puts this well in his famous aphorism: All models are wrong, but some are useful. It’s worth reading the fuller context of the quote: Now it would be very remarkable if any system existing in the real world could be exactly represented by any simple model. However, cunningly chosen parsimonious models often do provide remarkably useful approximations. For example, the law PV = RT relating pressure P, volume V and temperature T of an “ideal” gas via a constant R is not exactly true for any real gas, but it frequently provides a useful approximation and furthermore its structure is informative since it springs from a physical view of the behavior of gas molecules. For such a model there is no need to ask the question “Is the model true?”. If “truth” is to be the “whole truth” the answer must be “No”. The only question of interest is “Is the model illuminating and useful?”. The goal of a model is not to uncover truth, but to discover a simple approximation that is still useful. "],
["model-building-tbd.html", "24 Model building (TBD) 24.1 Introduction", " 24 Model building (TBD) 24.1 Introduction In the previous chapter you learned how linear models work, and learned some basic tools for understanding what a model is telling you about your data. The previous chapter focussed on simulated datasets. This chapter will focus on real data, showing you how you can progressively build up a model to aid your understanding of the data. We will take advantage of the fact that you can think about a model partitioning your data into pattern and residuals. We’ll find patterns with visualisation, then make them concrete and precise with a model. We’ll then repeat the process, but replace the old response variable with the residuals from the model. The goal is to transition from implicit knowledge in the data and your head to explicit knowledge in a quantitative model. This makes it easier to apply to new domains, and easier for others to use. For very large and complex datasets this will be a lot of work. There are certainly alternative approaches - a more machine learning approach is simply to focus on the predictive ability of the model. These approaches tend to produce black boxes: the model does a really good job at generating predictions, but you don’t know why. This is a totally reasonable approach, but it does make it hard to apply your real world knowledge to the model. That, in turn, makes it difficult to assess whether or not the model will continue to work in the long-term, as fundamentals change. For most real models, I’d expect you to use some combination of this approach and a more classic automated approach. It’s a challenge to know when to stop. You need to figure out when your model is good enough, and when additional investment is unlikely to pay off. I particularly like this quote from reddit user Broseidon241: A long time ago in art class, my teacher told me “An artist needs to know when a piece is done. You can’t tweak something into perfection - wrap it up. If you don’t like it, do it over again. Otherwise begin something new”. Later in life, I heard “A poor seamstress makes many mistakes. A good seamstress works hard to correct those mistakes. A great seamstress isn’t afraid to throw out the garment and start over.” – Broseidon241, https://www.reddit.com/r/datascience/comments/4irajq "],
["many-models-tbd.html", "25 Many models (TBD) 25.1 Introduction", " 25 Many models (TBD) 25.1 Introduction In this chapter you’re going to learn three powerful ideas that help you to work with large numbers of models with ease: Using many simple models to better understand complex datasets. Using list-columns to store arbitrary data structures in a data frame. For example, this will allow you to have a column that contains linear models. How to turn models into tidy data. This is a powerful technique for working with large numbers of models because once you have tidy data, you can apply all of the techniques that you’ve learned about earlier in the book. We’ll start by diving into a motivating example using data about life expectancy around the world. It’s a small dataset but it illustrates how important modelling can be for improving your visualisations. We’ll use a large number of simple models to partition out some of the strongest signals so we can see the subtler signals that remain. We’ll also see how model summaries can help us pick out outliers and unusual trends. The following sections will dive into more detail about the individual techniques: In [list-columns], you’ll learn more about the list-column data structure, and why it’s valid to put lists in data frames. In [creating list-columns], you’ll learn the three main ways in which you’ll create list-columns. In [simplifying list-columns] you’ll learn how to convert list-columns back to regular atomic vectors (or sets of atomic vectors) so you can work with them more easily. In [making tidy data with broom], you’ll learn about the full set of tools provided by broom, and see how they can be applied to other types of data structure. This chapter is somewhat aspirational: if this book is your first introduction to R, this chapter is likely to be a struggle. It requires you have to deeply internalised ideas about modelling, data structures, and iteration. So don’t worry if you don’t get it — just put this chapter aside for a few months, and come back when you want to stretch your brain. "],
["communicate-intro.html", "26 Introduction", " 26 Introduction So far, you’ve learned the tools to get your data into Python, tidy it into a form convenient for analysis, and then understand your data through transformation, visualisation and modelling. However, it doesn’t matter how great your analysis is unless you can explain it to others: you need to communicate your results. Communication is the theme of the following four chapters: In Markdown, you will learn about Markdown, a simple tool for prose and results. You can use Markdown in VS Code to cumminicate. In [Graphics for communication], you will learn how to take your exploratory graphics and turn them into expository graphics, graphics that help the newcomer to your analysis understand what’s going on as quickly and easily as possible. In [Markdown formats], you’ll learn a little about the many other varieties of outputs you can produce using Markdown, including dashboards, websites, and books. We’ll finish up with [Markdown workflow], where you’ll learn about the “analysis notebook” and how to systematically record your successes and failures so that you can learn from them. Unfortunately, these chapters focus mostly on the technical mechanics of communication, not the really hard problems of communicating your thoughts to other humans. However, there are lot of other great books about communication, which we’ll point you to at the end of each chapter. "],
["markdown.html", "27 Markdown 27.1 Introduction 27.2 Markdown basics 27.3 Text formatting with Markdown 27.4 Code chunks 27.5 Learning more", " 27 Markdown 27.1 Introduction Markdown provides a simple to use plain text syntax to make writing easier. It is especially useful for data scientists. Markdown documents support dozens of output formats, like PDFs, Word files, slideshows, and more. 27.2 Markdown basics This is a Markdown file, a plain text file that has the extension .md or .MD: # Diamond Sizes ```python # setup import altair as alt import pandas as pd import numpy as np alt.data_transformers.enable(&#39;json&#39;) ``` ```python # load and format data diamonds = pd.read_csv(&quot;https://github.com/byuidatascience/data4python4ds/raw/master/data-raw/diamonds/diamonds.csv&quot;) smaller = diamonds.query(&#39;carat &lt;= 2.5&#39;) ``` We have data about diamonds. Only those larger than 2.5 carats. The distribution of the remainder is shown below: ```python # plot data chart = (alt.Chart(smaller). encode(alt.X(&#39;carat&#39;), alt.Y(&#39;count()&#39;)). mark_line()) chart.save(&#39;markdown/diamonds_25.png&#39;) ``` ![](diamonds_25.png) It contains two important types of content: Chunks of code surrounded by ```. Text mixed with simple text formatting like # heading and _italics_. When you open a .md in VS code, you see a text file where code and output can be interleaved. To produce a complete pdf report containing the text, formatted code chunks, and charts you can install the Markdown PDF extension for VS Code (note their instructions about installation if you run into any errors). Once installed, you can create a pdf, html or png report by using the VS code command pallete hotkey Ctrl + Shift + P (Cmd + Shift + P for Mac) and then search using the keyword export. This will create a report in the same folder of the .md file with the same file name and the chosen export extension. When you export the document, the Markdown PDF extension sends the .md file to chromium, which then builds the report. To get started with your own .md file, simply create a new file and use the extension .md. The following sections dive into the two components of a Markdown document in more details: the markdown text, the code chunks. 27.3 Text formatting with Markdown Prose in .md files is written in Markdown, a lightweight set of conventions for formatting plain text files. Markdown is designed to be easy to read and easy to write. It is also very easy to learn. The guide below shows how to use Markdown. Text formatting ------------------------------------------------------------ *italic* or _italic_ **bold** __bold__ `code` superscript^2^ and subscript~2~ Headings ------------------------------------------------------------ # 1st Level Header ## 2nd Level Header ### 3rd Level Header Lists ------------------------------------------------------------ * Bulleted list item 1 * Item 2 * Item 2a * Item 2b 1. Numbered list item 1 1. Item 2. The numbers are incremented automatically in the output. Links and images ------------------------------------------------------------ &lt;http://example.com&gt; [linked phrase](http://example.com) ![optional caption text](path/to/img.png) Tables ------------------------------------------------------------ First Header | Second Header ------------- | ------------- Content Cell | Content Cell Content Cell | Content Cell The best way to learn these is simply to try them out. It will take a few days, but soon they will become second nature, and you won’t need to think about them. If you forget, you can use the Markdown snippets that are built into VS code. Use ^Space (ctrl+Space on a mac) to get a context specific list of suggestions. 27.3.1 Exercises Practice what you’ve learned by creating a brief CV. The title should be your name, and you should include headings for (at least) education or employment. Each of the sections should include a bulleted list of jobs/degrees. Highlight the year in bold. Using the VS code snippet suggestions, figure out how to: Add a footnote. Add a horizontal rule. Add a block quote. 27.4 Code chunks To display code inside a Markdown document, you need to insert a chunk. There are two ways to do so: The keyboard shortcut ^Space (ctrl+Space on a mac) then typing code and selecting fenced code block By manually typing the chunk delimiters ```python and ```. Obviously, I’d recommend you learn the keyboard shortcut. It will save you time in the long run! 27.4.1 Tables Markdown has functionality for displaying tables using their table format. Notice the readability and how the colons are used to set the text alignment in the column. To left-align the column, replace the leftmost dash with a colon, :---. To right-align the column, replace the rightmost dash with a colon, ---:. To center-align the column, both the leftmost and rightmost dashes with a colon, :---: | | carat | cut | color | clarity | |---:|--------:|:--------|:--------|:----------| | 0 | 0.23 | Ideal | E | SI2 | | 1 | 0.21 | Premium | E | SI1 | | 2 | 0.23 | Good | E | VS1 | | 3 | 0.29 | Premium | I | VS2 | | 4 | 0.31 | Good | J | SI2 | When displaying tables from your pandas dataFrames, you can use .to_markdown(). The above Markdown table was generated using the following: print(smaller. filter([&#39;carat&#39;, &#39;cut&#39;, &#39;color&#39;, &#39;clarity&#39;]). rename_axis(None). head(). to_markdown()) There is one other way to display Python code in a Markdown document: directly into the text, with: x = 5. 27.5 Learning more There are two important topics that we haven’t covered here: collaboration, and the details of accurately communicating your ideas to other humans. Collaboration is a vital part of modern data science, and you can make your life much easier by using version control tools, like Git and GitHub. We recommend two free resources that will teach you about Git: Microsoft provides a webpage titled “Working with GitHub in VS Code” to help you with GitHub integrations: https://code.visualstudio.com/docs/editor/github Microsoft’s learning documents provides further training on Git: https://docs.microsoft.com/en-us/learn/modules/use-git-from-vs-code/. I have also not touched on what you should actually write in order to clearly communicate the results of your analysis. To improve your writing, I highly recommend reading either Style: Lessons in Clarity and Grace by Joseph M. Williams &amp; Joseph Bizup, or The Sense of Structure: Writing from the Reader’s Perspective by George Gopen. Both books will help you understand the structure of sentences and paragraphs, and give you the tools to make your writing more clear. (These books are rather expensive if purchased new, but they’re used by many English classes so there are plenty of cheap second-hand copies). George Gopen also has a number of short articles on writing at https://www.georgegopen.com/the-litigation-articles.html. They are aimed at lawyers, but almost everything applies to data scientists too. "]
]
